# -*- coding: utf-8 -*-
"""
EPIS êµìœ¡/í–‰ì‚¬ ê²Œì‹œíŒ ìŠ¤í¬ë˜í¼
ë†ë¦¼ìˆ˜ì‚°ì‹í’ˆêµìœ¡ë¬¸í™”ì •ë³´ì› êµìœ¡/í–‰ì‚¬ ê²Œì‹œíŒ ëŒ€ìƒ
"""

import re
import time
import os
from bs4 import BeautifulSoup
from urllib.parse import urljoin, parse_qs, urlparse
from enhanced_base_scraper import EnhancedBaseScraper
import logging

logger = logging.getLogger(__name__)

class EnhancedEpis4Scraper(EnhancedBaseScraper):
    """EPIS êµìœ¡/í–‰ì‚¬ ê²Œì‹œíŒ ìŠ¤í¬ë˜í¼"""
    
    def __init__(self):
        super().__init__()
        self.base_url = "https://www.epis.or.kr"
        self.list_url = "https://www.epis.or.kr/home/kor/M943502192/board.do"
        self.site_code = "epis4"
        
        # ì„¸ì…˜ ê´€ë ¨ ì„¤ì •
        self.session_initialized = False
        
        # ì¤‘ë³µ ë°©ì§€ ê¸°ëŠ¥ ë¹„í™œì„±í™” (ìƒˆë¡œìš´ ìˆ˜ì§‘ì„ ìœ„í•´)
        self.enable_duplicate_check = False
        
        # ì¬ì‹œë„ ì„¤ì • ê°•í™”
        self.max_retries = 5
        self.retry_delay = 3
        
        # í—¤ë” ì„¤ì •
        self.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        
        # ì„¸ì…˜ ì—…ë°ì´íŠ¸
        self.session.headers.update(self.headers)
    
    def get_list_url(self, page_num: int) -> str:
        """í˜ì´ì§€ë³„ ëª©ë¡ URL ìƒì„±"""
        if page_num == 1:
            return self.list_url
        else:
            # í˜ì´ì§€ ë²ˆí˜¸ëŠ” pageIndex íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬
            return f"{self.list_url}?pageIndex={page_num}"
    
    def initialize_session(self):
        """ì„¸ì…˜ ì´ˆê¸°í™”"""
        if self.session_initialized:
            return True
        
        try:
            # ë¨¼ì € ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸
            main_response = self.get_page(self.base_url)
            if not main_response:
                logger.error("ë©”ì¸ í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨")
                return False
            
            # êµìœ¡/í–‰ì‚¬ ê²Œì‹œíŒ ì´ˆê¸° ë°©ë¬¸
            response = self.get_page(self.list_url)
            if not response:
                logger.error("êµìœ¡/í–‰ì‚¬ ê²Œì‹œíŒ ì ‘ê·¼ ì‹¤íŒ¨")
                return False
            
            self.session_initialized = True
            logger.info("EPIS ì„¸ì…˜ ì´ˆê¸°í™” ì„±ê³µ")
            return True
        
        except Exception as e:
            logger.error(f"ì„¸ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def parse_list_page(self, html_content: str) -> list:
        """ëª©ë¡ í˜ì´ì§€ íŒŒì‹±"""
        soup = BeautifulSoup(html_content, 'html.parser')
        announcements = []
        
        logger.info(f"í˜ì´ì§€ íŒŒì‹± ì‹œì‘ - í˜„ì¬ í˜ì´ì§€: {self.current_page_num}")
        
        # í…Œì´ë¸” ì°¾ê¸°
        table = soup.find('table')
        if not table:
            logger.warning("í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return announcements
        
        # ë°ì´í„° í–‰ë“¤ ì°¾ê¸° (í—¤ë” í–‰ ì œì™¸)
        rows = table.find_all('tr')[1:]  # ì²« ë²ˆì§¸ í–‰ì€ í—¤ë”
        
        if not rows:
            logger.warning("í…Œì´ë¸” í–‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return announcements
        
        for row_index, row in enumerate(rows):
            try:
                cells = row.find_all(['td', 'th'])
                if len(cells) < 3:
                    continue
                
                # ë²ˆí˜¸ (No)
                number_cell = cells[0]
                number = number_cell.get_text(strip=True)
                
                # êµ¬ë¶„ (ì¹´í…Œê³ ë¦¬)
                category_cell = cells[1]
                category = category_cell.get_text(strip=True)
                
                # ì œëª© (Title)
                title_cell = cells[2]
                title_link = title_cell.find('a')
                if not title_link:
                    continue
                
                title = title_link.get_text(strip=True)
                if not title:
                    continue
                
                # ìƒì„¸ í˜ì´ì§€ URL ì¶”ì¶œ
                href = title_link.get('href', '')
                if not href or href == 'javascript:void(0);':
                    # JavaScript ë§í¬ì¸ ê²½ìš° onclick ì´ë²¤íŠ¸ì—ì„œ URL ì¶”ì¶œ
                    onclick = title_link.get('onclick', '')
                    if onclick:
                        # fn_edit í•¨ìˆ˜ í˜¸ì¶œì—ì„œ ì„¸ ë²ˆì§¸ íŒŒë¼ë¯¸í„°(idx) ì¶”ì¶œ
                        match = re.search(r'fn_edit\(["\']([^"\']+)["\'][^,]*,[^,]*["\']([^"\']+)["\'][^,]*,[^,]*["\']([^"\']+)["\']', onclick)
                        if match:
                            action = match.group(1)  # 'detail'  
                            idx = match.group(2)     # ì‹¤ì œ idx ê°’
                            delete_at = match.group(3)  # 'N'
                            detail_url = f"{self.list_url}?deleteAt={delete_at}&act={action}&idx={idx}&pageIndex={self.current_page_num}"
                        else:
                            logger.warning(f"onclickì—ì„œ URL ì¶”ì¶œ ì‹¤íŒ¨: {onclick}")
                            continue
                    else:
                        logger.warning(f"hrefì™€ onclick ëª¨ë‘ ì—†ìŒ: {title}")
                        continue
                else:
                    detail_url = urljoin(self.base_url, href)
                
                # ë“±ë¡ì¼ (4ë²ˆì§¸ ì—´)
                date = ""
                if len(cells) >= 4:
                    date_cell = cells[3]
                    date = date_cell.get_text(strip=True)
                
                # ì¡°íšŒìˆ˜ (5ë²ˆì§¸ ì—´)
                views = ""
                if len(cells) >= 5:
                    views_cell = cells[4]
                    views = views_cell.get_text(strip=True)
                
                announcement = {
                    'title': title,
                    'url': detail_url,
                    'date': date,
                    'views': views,
                    'category': category,
                    'number': number
                }
                
                announcements.append(announcement)
                logger.debug(f"ê³µê³  ì¶”ê°€: {title} (ë²ˆí˜¸: {number})")
                
            except Exception as e:
                logger.error(f"í–‰ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ (í–‰ {row_index}): {e}")
                continue
        
        logger.info(f"ì´ {len(announcements)}ê°œ ê³µê³  íŒŒì‹± ì™„ë£Œ")
        return announcements
    
    def parse_detail_page(self, html_content: str) -> dict:
        """ìƒì„¸ í˜ì´ì§€ íŒŒì‹± - ê°œì„ ëœ ë²„ì „"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ì œëª© ì°¾ê¸° - ë” ì •í™•í•œ ë°©ë²•
        title = "ì œëª© ì—†ìŒ"
        title_elem = soup.find('strong')
        if title_elem:
            title = title_elem.get_text(strip=True)
        
        # ë©”íƒ€ ì •ë³´ ì°¾ê¸° - ë” ì •í™•í•œ ë°©ë²•
        meta_info = {}
        
        # ë©”íƒ€ ì •ë³´ê°€ í¬í•¨ëœ div ì°¾ê¸°
        meta_divs = soup.find_all('div')
        for div in meta_divs:
            text = div.get_text(strip=True)
            if 'ì‘ì„±ì' in text and 'ë“±ë¡ì¼' in text:
                # ì‘ì„±ì, ë“±ë¡ì¼, ì¡°íšŒìˆ˜ ì •ë³´ ì¶”ì¶œ
                parts = text.split('|')
                for part in parts:
                    part = part.strip()
                    if 'ì‘ì„±ì' in part and ':' in part:
                        meta_info['writer'] = part.split(':')[1].strip()
                    elif 'ë“±ë¡ì¼' in part and ':' in part:
                        meta_info['date'] = part.split(':')[1].strip()
                    elif 'ì¡°íšŒ' in part and ':' in part:
                        meta_info['views'] = part.split(':')[1].strip()
                break
        
        # ë³¸ë¬¸ ë‚´ìš© ì°¾ê¸° - EPIS íŠ¹í™” ë°©ë²•
        content = ""
        
        # EPIS íŠ¹í™” ë³¸ë¬¸ ì„ íƒì
        content_div = soup.select_one('div.board_view_con div.editor_view')
        
        if content_div:
            # ë³¸ë¬¸ ë‚´ìš© ì •ë¦¬
            content_parts = []
            for element in content_div.find_all(['p', 'div', 'span', 'br']):
                if element.name == 'br':
                    content_parts.append('\n')
                else:
                    element_text = element.get_text(strip=True)
                    if element_text and len(element_text) > 2:  # ì˜ë¯¸ìˆëŠ” ë‚´ìš©ë§Œ
                        content_parts.append(element_text)
            
            if content_parts:
                content = '\n\n'.join(content_parts)
            else:
                content = content_div.get_text(separator='\n', strip=True)
        else:
            # ëŒ€ì²´ ë°©ë²• - ì „ì²´ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ
            logger.warning("editor_viewë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ, ëŒ€ì²´ ë°©ë²• ì‚¬ìš©")
            content_div = soup.find('div', class_='board_view_con')
            if content_div:
                content = content_div.get_text(separator='\n', strip=True)
            else:
                content = "ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ì²¨ë¶€íŒŒì¼ ì°¾ê¸° - EPIS íŠ¹í™” ë°©ë²•
        attachments = []
        
        logger.info(f"ì²¨ë¶€íŒŒì¼ ê²€ìƒ‰ ì‹œì‘...")
        
        # EPIS íŠ¹í™” ì²¨ë¶€íŒŒì¼ ì„¹ì…˜ ì°¾ê¸°
        file_section = soup.select_one('div.board_view_file div.file_box')
        
        if file_section:
            logger.info(f"ì²¨ë¶€íŒŒì¼ ì„¹ì…˜ ë°œê²¬")
            
            # ê° íŒŒì¼ ë§í¬ ì°¾ê¸°
            file_links = file_section.select('p.file_each a')
            
            for link in file_links:
                onclick = link.get('onclick', '')
                link_text = link.get_text(strip=True)
                
                if onclick and link_text:
                    # onclickì—ì„œ ê³ ìœ í‚¤ ì¶”ì¶œ: kssFileDownloadForKeyAct('ê³ ìœ í‚¤')
                    match = re.search(r'kssFileDownloadForKeyAct\(["\']([^"\']+)["\']', onclick)
                    if match:
                        unique_key = match.group(1)
                        logger.info(f"ì²¨ë¶€íŒŒì¼ ê³ ìœ í‚¤ ë°œê²¬: {unique_key} ({link_text})")
                        
                        attachments.append({
                            'filename': link_text,
                            'url': '/fileDownload.do',
                            'unique_key': unique_key,
                            'method': 'POST'
                        })
                        logger.info(f"ì²¨ë¶€íŒŒì¼ ì¶”ê°€: {link_text}")
        else:
            logger.info("ì²¨ë¶€íŒŒì¼ ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        logger.info(f"ì´ {len(attachments)}ê°œ ì²¨ë¶€íŒŒì¼ ë°œê²¬")
        
        return {
            'title': title,
            'content': content,
            'attachments': attachments,
            'meta_info': meta_info
        }
    
    def download_epis_file(self, unique_key: str, filename: str, save_path: str) -> bool:
        """EPIS íŠ¹í™” íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        try:
            # POST ìš”ì²­ìœ¼ë¡œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            download_url = urljoin(self.base_url, '/fileDownload.do')
            
            data = {
                'uniqueKey': unique_key
            }
            
            headers = self.headers.copy()
            headers['Referer'] = self.list_url
            
            logger.info(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {filename} (ê³ ìœ í‚¤: {unique_key})")
            
            response = self.session.post(
                download_url,
                data=data,
                headers=headers,
                stream=True,
                timeout=self.timeout * 2
            )
            
            response.raise_for_status()
            
            # íŒŒì¼ ì €ì¥
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            total_size = 0
            with open(save_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        total_size += len(chunk)
            
            file_size = os.path.getsize(save_path)
            logger.info(f"ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filename} ({file_size:,} bytes)")
            
            # íŒŒì¼ í¬ê¸° ê²€ì¦ (1KB ë¯¸ë§Œì´ë©´ ì˜¤ë¥˜ í˜ì´ì§€ì¼ ê°€ëŠ¥ì„±)
            if file_size < 1024:
                logger.warning(f"íŒŒì¼ í¬ê¸°ê°€ ì‘ìŒ: {filename} ({file_size} bytes)")
                # íŒŒì¼ ë‚´ìš© í™•ì¸
                with open(save_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read(500)
                    if '<html' in content.lower() or 'error' in content.lower():
                        logger.error(f"HTML ì˜¤ë¥˜ í˜ì´ì§€ ë‹¤ìš´ë¡œë“œë¨: {filename}")
                        os.remove(save_path)
                        return False
            
            return True
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {filename} - {e}")
            return False
    
    def _download_attachments(self, attachments: list, folder_path: str):
        """EPIS íŠ¹í™” ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        if not attachments:
            logger.info("ì²¨ë¶€íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return
        
        logger.info(f"{len(attachments)}ê°œ ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
        attachments_folder = os.path.join(folder_path, 'attachments')
        os.makedirs(attachments_folder, exist_ok=True)
        
        for i, attachment in enumerate(attachments):
            try:
                filename = attachment.get('filename', f"attachment_{i+1}")
                unique_key = attachment.get('unique_key')
                
                if not unique_key:
                    logger.warning(f"ê³ ìœ í‚¤ê°€ ì—†ëŠ” ì²¨ë¶€íŒŒì¼: {filename}")
                    continue
                
                # íŒŒì¼ëª… ì •ë¦¬
                clean_filename = self.sanitize_filename(filename)
                if not clean_filename or clean_filename.isspace():
                    clean_filename = f"attachment_{i+1}"
                
                file_path = os.path.join(attachments_folder, clean_filename)
                
                # EPIS íŠ¹í™” ë‹¤ìš´ë¡œë“œ
                success = self.download_epis_file(unique_key, filename, file_path)
                if not success:
                    logger.warning(f"ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {filename}")
                
            except Exception as e:
                logger.error(f"ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _get_page_announcements(self, page_num: int) -> list:
        """ì„¸ì…˜ í™•ì¸ í›„ ê³µê³  ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        if not self.initialize_session():
            logger.error("ì„¸ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨")
            return []
        
        return super()._get_page_announcements(page_num)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('epis4_scraper.log', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    # ìŠ¤í¬ë˜í¼ ì‹¤í–‰
    scraper = EnhancedEpis4Scraper()
    
    try:
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        output_dir = f'output/{scraper.site_code}'
        
        # ì¤‘ë³µ ë°©ì§€ ê¸°ëŠ¥ ì™„ì „ ë¹„í™œì„±í™”
        scraper.enable_duplicate_check = False
        
        # 3í˜ì´ì§€ ìˆ˜ì§‘
        logger.info("="*60)
        logger.info(f"ğŸš€ EPIS êµìœ¡/í–‰ì‚¬ ê²Œì‹œíŒ ìŠ¤í¬ë˜í•‘ ì‹œì‘ (ìƒˆë¡œìš´ ìˆ˜ì§‘)")
        logger.info(f"ğŸ“‚ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
        logger.info(f"ğŸ”„ ì¤‘ë³µ ë°©ì§€ ê¸°ëŠ¥: {'í™œì„±í™”' if scraper.enable_duplicate_check else 'ë¹„í™œì„±í™”'}")
        logger.info("="*60)
        
        success = scraper.scrape_pages(max_pages=3, output_base=output_dir)
        
        if success:
            logger.info("âœ… ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
            
            # ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½
            content_files = []
            import os
            for root, dirs, files in os.walk(output_dir):
                for file in files:
                    if file == 'content.md':
                        content_files.append(os.path.join(root, file))
            
            logger.info(f"ğŸ“„ ìˆ˜ì§‘ëœ content.md íŒŒì¼: {len(content_files)}ê°œ")
            
            # ì²¨ë¶€íŒŒì¼ í™•ì¸
            attachment_files = []
            for root, dirs, files in os.walk(output_dir):
                if 'attachments' in root:
                    attachment_files.extend([os.path.join(root, f) for f in files])
            
            logger.info(f"ğŸ“ ë‹¤ìš´ë¡œë“œëœ ì²¨ë¶€íŒŒì¼: {len(attachment_files)}ê°œ")
            
        else:
            logger.error("âŒ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨")
            
    except KeyboardInterrupt:
        logger.info("â¹ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
    
    return success


if __name__ == "__main__":
    main()