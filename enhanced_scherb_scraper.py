# -*- coding: utf-8 -*-
"""
ì‚°ì²­í•œë°©ì•½ì´ˆì¶•ì œ(SCHERB) ê³µê³  ìŠ¤í¬ë˜í¼ - Enhanced ë²„ì „
URL: http://www.scherb.or.kr/bbs/board.php?bo_table=sub7_1&page=1
"""

import requests
import requests.adapters
from bs4 import BeautifulSoup
import os
import time
import re
from urllib.parse import urljoin, urlparse, unquote
import logging
from enhanced_base_scraper import EnhancedBaseScraper

logger = logging.getLogger(__name__)

class EnhancedScherbScraper(EnhancedBaseScraper):
    """ì‚°ì²­í•œë°©ì•½ì´ˆì¶•ì œ ì „ìš© ìŠ¤í¬ë˜í¼ - í–¥ìƒëœ ë²„ì „"""
    
    def __init__(self):
        super().__init__()
        # ê¸°ë³¸ ì„¤ì •
        self.base_url = "http://www.scherb.or.kr"
        self.list_url = "http://www.scherb.or.kr/bbs/board.php?bo_table=sub7_1&page=1"
        
        # ì‚¬ì´íŠ¸ë³„ íŠ¹í™” ì„¤ì •
        self.verify_ssl = False  # HTTP ì‚¬ì´íŠ¸
        self.default_encoding = 'utf-8'
        self.timeout = 45  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
        self.delay_between_requests = 1.5  # ìš”ì²­ ê°„ ì§€ì—° ë‹¨ì¶•
        self.delay_between_pages = 2  # í˜ì´ì§€ ê°„ ëŒ€ê¸° ì‹œê°„ ë‹¨ì¶•
        
        # ë‹¤ìš´ë¡œë“œ ì„±ëŠ¥ ì„¤ì •
        self.max_download_retries = 3
        self.download_timeout_multiplier = 2
        
        # SCHERB íŠ¹í™” ì„¤ì • - ì¼ë°˜ì ì¸ PHP ê²Œì‹œíŒ
        self.use_playwright = False
        
        # ì„¸ì…˜ ì´ˆê¸°í™” - SCHERB ì‚¬ì´íŠ¸ì— ë§ëŠ” í—¤ë” ì„¤ì •
        self._initialize_session()
    
    def _initialize_session(self):
        """SCHERB ì‚¬ì´íŠ¸ìš© ì„¸ì…˜ ì´ˆê¸°í™” - ê°œì„ ëœ ë²„ì „"""
        # ìµœì‹  Chrome ë¸Œë¼ìš°ì € í—¤ë”ë¡œ ì—…ë°ì´íŠ¸
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Cache-Control': 'max-age=0',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1'
        })
        
        # Keep-Alive ì—°ê²° ì„¤ì •
        adapter = requests.adapters.HTTPAdapter(
            max_retries=3,
            pool_connections=10,
            pool_maxsize=20
        )
        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)
        
        try:
            # ë‹¤ë‹¨ê³„ ì„¸ì…˜ ì´ˆê¸°í™”
            logger.info("SCHERB ì‚¬ì´íŠ¸ ì„¸ì…˜ ì´ˆê¸°í™” ì¤‘...")
            
            # 1. í™ˆí˜ì´ì§€ ë°©ë¬¸
            response = self.session.get(self.base_url, verify=self.verify_ssl, timeout=self.timeout)
            if response.status_code == 200:
                logger.info(f"í™ˆí˜ì´ì§€ ì ‘ì† ì„±ê³µ (ì¿ í‚¤ {len(self.session.cookies)}ê°œ ì„¤ì •)")
            else:
                logger.warning(f"í™ˆí˜ì´ì§€ ì ‘ì† ê²½ê³ : HTTP {response.status_code}")
            
            time.sleep(1)
            
            # 2. ê²Œì‹œíŒ ëª©ë¡ í˜ì´ì§€ ë°©ë¬¸ (ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ì„¤ì •)
            board_response = self.session.get(self.list_url, verify=self.verify_ssl, timeout=self.timeout)
            if board_response.status_code == 200:
                logger.info(f"ê²Œì‹œíŒ ì ‘ì† ì„±ê³µ (ì´ ì¿ í‚¤ {len(self.session.cookies)}ê°œ)")
            else:
                logger.warning(f"ê²Œì‹œíŒ ì ‘ì† ê²½ê³ : HTTP {board_response.status_code}")
                
            logger.info("ì„¸ì…˜ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"ì„¸ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            logger.info("ê¸°ë³¸ ì„¸ì…˜ìœ¼ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
    
    def get_list_url(self, page_num: int) -> str:
        """í˜ì´ì§€ë³„ ëª©ë¡ URL ìƒì„±"""
        return f"http://www.scherb.or.kr/bbs/board.php?bo_table=sub7_1&page={page_num}"
    
    def parse_list_page(self, html_content: str) -> list:
        """ëª©ë¡ í˜ì´ì§€ íŒŒì‹± - PHP ê²Œì‹œíŒ í…Œì´ë¸” ê¸°ë°˜ êµ¬ì¡°"""
        announcements = []
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ê²Œì‹œê¸€ ë§í¬ ì°¾ê¸° - board.php?bo_table=sub7_1&wr_id= íŒ¨í„´
        detail_links = soup.find_all('a', href=re.compile(r'board\.php\?bo_table=sub7_1.*wr_id=\d+'))
        
        logger.info(f"ê³µê³  ë§í¬ {len(detail_links)}ê°œ ë°œê²¬")
        
        for i, link in enumerate(detail_links):
            try:
                href = link.get('href', '')
                
                # ì ˆëŒ€ URL ìƒì„±
                if href.startswith('http'):
                    detail_url = href
                else:
                    detail_url = urljoin(self.base_url, href)
                
                # wr_id ì¶”ì¶œ
                wr_id_match = re.search(r'wr_id=(\d+)', href)
                if not wr_id_match:
                    continue
                    
                wr_id = wr_id_match.group(1)
                
                # ë§í¬ í…ìŠ¤íŠ¸ì—ì„œ ì œëª© ì¶”ì¶œ
                title = link.get_text(strip=True)
                
                # ì œëª©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ë¶€ëª¨ ìš”ì†Œì—ì„œ ì°¾ê¸°
                if len(title) < 5:
                    parent = link.parent
                    while parent and len(title) < 5:
                        parent_text = parent.get_text(strip=True)
                        if len(parent_text) > len(title) and len(parent_text) < 200:
                            title = parent_text
                            break
                        parent = parent.parent
                
                # í…Œì´ë¸” êµ¬ì¡°ì—ì„œ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
                table_row = link.find_parent('tr')
                category = "ê³µì§€"
                date = ""
                
                if table_row:
                    cells = table_row.find_all(['td', 'th'])
                    if len(cells) >= 4:
                        # ì¼ë°˜ì ì¸ ê²Œì‹œíŒ êµ¬ì¡°: ë²ˆí˜¸, ì œëª©, ì‘ì„±ì, ë‚ ì§œ
                        for cell in cells:
                            cell_text = cell.get_text(strip=True)
                            # ë‚ ì§œ íŒ¨í„´ ì°¾ê¸°
                            date_match = re.search(r'(\d{4}-\d{2}-\d{2})', cell_text)
                            if date_match:
                                date = date_match.group(1)
                                break
                
                # ì œëª© ì •ë¦¬
                if not title or title.strip() == "":
                    title = f"ê³µê³ _{wr_id}"
                
                # ì œëª©ì—ì„œ ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°
                title = re.sub(r'\s+', ' ', title).strip()
                title = title.replace('\n', ' ').replace('\t', ' ')
                
                announcement = {
                    'number': wr_id,
                    'category': category,
                    'title': title,
                    'url': detail_url,
                    'date': date,
                    'attachment_count': 0  # ìƒì„¸í˜ì´ì§€ì—ì„œ í™•ì¸
                }
                
                announcements.append(announcement)
                logger.info(f"ê³µê³  ì¶”ê°€: [{wr_id}] {title[:50]}...")
                
            except Exception as e:
                logger.error(f"ê³µê³  íŒŒì‹± ì¤‘ ì˜¤ë¥˜ (ë§í¬ {i}): {e}")
                continue
        
        logger.info(f"ì´ {len(announcements)}ê°œ ê³µê³  íŒŒì‹± ì™„ë£Œ")
        return announcements
    
    def parse_detail_page(self, html_content: str) -> dict:
        """ìƒì„¸ í˜ì´ì§€ íŒŒì‹±"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ì œëª© ì¶”ì¶œ
        title = ""
        
        # ê²Œì‹œê¸€ ì œëª© ì˜ì—­ ì°¾ê¸°
        title_selectors = [
            '.bo_v_title',
            '.view_title', 
            '.board_title',
            'h1',
            'h2'
        ]
        
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title = title_elem.get_text(strip=True)
                if len(title) > 5:
                    break
        
        # íƒ€ì´í‹€ íƒœê·¸ì—ì„œ ì¶”ì¶œ
        if not title:
            page_title = soup.find('title')
            if page_title:
                title_text = page_title.get_text().strip()
                # ì‚¬ì´íŠ¸ëª… ì œê±°
                if ' | ' in title_text:
                    title = title_text.split(' | ')[0].strip()
                else:
                    title = title_text
        
        if not title:
            title = "ì œëª© ì—†ìŒ"
        
        # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
        content = ""
        
        # ê²Œì‹œê¸€ ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸°
        content_selectors = [
            '.bo_v_con',
            '.view_content',
            '.board_content', 
            '.content',
            '#bo_v_con'
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem and len(content_elem.get_text(strip=True)) > 50:
                content = self.h.handle(str(content_elem))
                break
        
        # ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ ì˜ì—­ ì°¾ê¸°
        if len(content.strip()) < 50:
            all_divs = soup.find_all('div')
            max_text = ""
            for div in all_divs:
                div_text = div.get_text(strip=True)
                if len(div_text) > len(max_text) and len(div_text) > 100:
                    # í•˜ìœ„ divê°€ ë§ì§€ ì•Šì€ ì˜ì—­
                    sub_divs = div.find_all('div')
                    if len(sub_divs) < 5:
                        max_text = div_text
            
            if max_text:
                content = max_text
        
        # ë‚ ì§œ ì¶”ì¶œ
        date = ""
        date_patterns = [
            r'(\d{4}-\d{2}-\d{2})',
            r'(\d{4}\.\d{2}\.\d{2})',
            r'(\d{2}-\d{2}-\d{2})'
        ]
        
        page_text = soup.get_text()
        for pattern in date_patterns:
            date_match = re.search(pattern, page_text)
            if date_match:
                date = date_match.group(1)
                break
        
        # ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ
        attachments = self._extract_attachments(soup)
        
        return {
            'title': title,
            'content': content,
            'date': date,
            'author': "",
            'attachments': attachments
        }
    
    def _extract_attachments(self, soup: BeautifulSoup) -> list:
        """ì²¨ë¶€íŒŒì¼ ë§í¬ ì¶”ì¶œ - ì¤‘ë³µ ì œê±° ê°œì„ """
        attachments = []
        seen_urls = set()  # ì¤‘ë³µ URL ì œê±°ìš©
        
        # PHP ê²Œì‹œíŒ ì²¨ë¶€íŒŒì¼ íŒ¨í„´ë“¤ (SCHERB íŠ¹í™”)
        attachment_patterns = [
            'a[href*="download.php"]',
            'a[href*="file_download.php"]'
        ]
        
        for pattern in attachment_patterns:
            download_links = soup.select(pattern)
            
            for link in download_links:
                try:
                    href = link.get('href', '')
                    if not href:
                        continue
                    
                    # download.php ë§í¬ë§Œ ì²˜ë¦¬
                    if 'download.php' not in href:
                        continue
                    
                    # ì ˆëŒ€ URL ìƒì„±
                    if href.startswith('http'):
                        file_url = href
                    else:
                        file_url = urljoin(self.base_url, href)
                    
                    # ì¤‘ë³µ URL ì²´í¬
                    if file_url in seen_urls:
                        continue
                    seen_urls.add(file_url)
                    
                    # íŒŒì¼ëª… ì¶”ì¶œ (ë‹¤ë‹¨ê³„ ë°©ì‹)
                    filename = self._extract_filename_from_link(link, href)
                    
                    # íŒŒì¼ í¬ê¸° ì •ë³´ ì¶”ì¶œ (ê°€ëŠ¥í•œ ê²½ìš°)
                    file_size_info = self._extract_file_size_info(link)
                    
                    attachment = {
                        'filename': filename,
                        'url': file_url,
                        'size_info': file_size_info
                    }
                    
                    attachments.append(attachment)
                    size_display = f"({file_size_info})" if file_size_info else ""
                    logger.info(f"ì²¨ë¶€íŒŒì¼ ë°œê²¬: {filename}{size_display} - {file_url}")
                    
                except Exception as e:
                    logger.error(f"ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
        
        logger.info(f"ì´ {len(attachments)}ê°œ ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ ì™„ë£Œ")
        return attachments
    
    def _extract_filename_from_link(self, link, href: str) -> str:
        """ë§í¬ì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ (í•œê¸€ ë””ì½”ë”© ê°œì„ )"""
        filename = ""
        
        # 1. ë§í¬ í…ìŠ¤íŠ¸ì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ (ìš°ì„ ìˆœìœ„ 1)
        link_text = link.get_text(strip=True)
        if link_text and any(ext in link_text.lower() for ext in ['.pdf', '.hwp', '.doc', '.xls', '.ppt', '.zip']):
            # URL ì¸ì½”ë”©ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”© ì‹œë„
            try:
                if '%' in link_text:
                    decoded_text = unquote(link_text, encoding='utf-8')
                    filename = decoded_text
                else:
                    filename = link_text
            except:
                filename = link_text
        
        # 2. title ì†ì„±ì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ
        if not filename:
            title = link.get('title', '').strip()
            if title and any(ext in title.lower() for ext in ['.pdf', '.hwp', '.doc', '.xls', '.ppt', '.zip']):
                try:
                    if '%' in title:
                        decoded_title = unquote(title, encoding='utf-8')
                        filename = decoded_title
                    else:
                        filename = title
                except:
                    filename = title
        
        # 3. hrefì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ
        if not filename:
            # URL íŒŒë¼ë¯¸í„°ì—ì„œ íŒŒì¼ëª… ì°¾ê¸°
            filename_match = re.search(r'file[_=]([^&\s]+)', href)
            if filename_match:
                try:
                    raw_filename = filename_match.group(1)
                    filename = unquote(raw_filename, encoding='utf-8')
                except:
                    filename = filename_match.group(1)
        
        # 4. ë¶€ëª¨/í˜•ì œ ìš”ì†Œì—ì„œ íŒŒì¼ëª… ì°¾ê¸°
        if not filename:
            # ë¶€ëª¨ ìš”ì†Œ ê²€ìƒ‰
            parent = link.parent
            if parent:
                parent_text = parent.get_text()
                file_match = re.search(r'([^/\\:*?"<>|\n\t]+\.(?:pdf|hwp|doc|docx|xls|xlsx|ppt|pptx|zip|rar))', parent_text, re.IGNORECASE)
                if file_match:
                    candidate_filename = file_match.group(1).strip()
                    # URL ì¸ì½”ë”©ëœ íŒŒì¼ëª… ë””ì½”ë”© ì‹œë„
                    try:
                        if '%' in candidate_filename:
                            filename = unquote(candidate_filename, encoding='utf-8')
                        else:
                            filename = candidate_filename
                    except:
                        filename = candidate_filename
            
            # í˜•ì œ ìš”ì†Œ ê²€ìƒ‰
            if not filename:
                next_sibling = link.next_sibling
                if next_sibling and hasattr(next_sibling, 'get_text'):
                    sibling_text = next_sibling.get_text(strip=True)
                    if sibling_text and any(ext in sibling_text.lower() for ext in ['.pdf', '.hwp', '.doc', '.xls', '.ppt', '.zip']):
                        filename = sibling_text
        
        # 5. ê¸°ë³¸ íŒŒì¼ëª… ìƒì„±
        if not filename:
            # wr_idì—ì„œ íŒŒì¼ëª… ìƒì„±
            wr_id_match = re.search(r'wr_id=(\d+)', href)
            no_match = re.search(r'no=(\d+)', href)
            wr_id = wr_id_match.group(1) if wr_id_match else "unknown"
            no = no_match.group(1) if no_match else "0"
            filename = f"ì²¨ë¶€íŒŒì¼_{wr_id}_{no}.file"
        
        # íŒŒì¼ëª… ì •ë¦¬
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        filename = re.sub(r'\s+', ' ', filename).strip()
        
        # íŒŒì¼ëª…ì´ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
        if len(filename) > 100:
            name, ext = os.path.splitext(filename)
            filename = name[:90] + ext
        
        return filename
    
    def _extract_file_size_info(self, link) -> str:
        """ë§í¬ ì£¼ë³€ì—ì„œ íŒŒì¼ í¬ê¸° ì •ë³´ ì¶”ì¶œ"""
        try:
            # ë§í¬ í…ìŠ¤íŠ¸ì—ì„œ í¬ê¸° ì •ë³´ ì°¾ê¸°
            link_text = link.get_text()
            size_patterns = [
                r'(\d+(?:\.\d+)?\s*[KMGT]?B)',
                r'\((\d+(?:\.\d+)?\s*[KMGT]?B)\)',
                r'(\d+\.\d+[KMGT]?B)',
                r'(\d+[KMGT]?B)'
            ]
            
            for pattern in size_patterns:
                match = re.search(pattern, link_text, re.IGNORECASE)
                if match:
                    return match.group(1)
            
            # ë¶€ëª¨ ìš”ì†Œì—ì„œ í¬ê¸° ì •ë³´ ì°¾ê¸°
            parent = link.parent
            if parent:
                parent_text = parent.get_text()
                for pattern in size_patterns:
                    match = re.search(pattern, parent_text, re.IGNORECASE)
                    if match:
                        return match.group(1)
            
        except Exception:
            pass
        
        return ""
    
    def download_file(self, file_url: str, save_path: str) -> bool:
        """íŒŒì¼ ë‹¤ìš´ë¡œë“œ - SCHERB íŠ¹í™” ë²„ì „ (ê°œì„ ëœ ì„¸ì…˜ ê´€ë¦¬)"""
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                logger.info(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œë„ {attempt + 1}/{max_retries}: {file_url}")
                
                # 1. ì„¸ì…˜ ê°±ì‹ ì„ ìœ„í•œ ë‹¤ë‹¨ê³„ ì ‘ê·¼
                wr_id_match = re.search(r'wr_id=(\d+)', file_url)
                page_match = re.search(r'page=(\d+)', file_url)
                
                if wr_id_match:
                    wr_id = wr_id_match.group(1)
                    page_num = page_match.group(1) if page_match else "1"
                    
                    # A. ë¨¼ì € ëª©ë¡ í˜ì´ì§€ ë°©ë¬¸ (ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ì„¤ì •)
                    list_url = f"{self.base_url}/bbs/board.php?bo_table=sub7_1&page={page_num}"
                    logger.debug(f"ëª©ë¡ í˜ì´ì§€ ë°©ë¬¸: {list_url}")
                    list_response = self.session.get(list_url, verify=self.verify_ssl, timeout=self.timeout)
                    time.sleep(0.5)
                    
                    # B. ê·¸ ë‹¤ìŒ ìƒì„¸í˜ì´ì§€ ë°©ë¬¸ (ê²Œì‹œê¸€ ì½ê¸° ê¶Œí•œ í™•ë³´)
                    detail_url = f"{self.base_url}/bbs/board.php?bo_table=sub7_1&wr_id={wr_id}&page={page_num}"
                    logger.debug(f"ìƒì„¸í˜ì´ì§€ ë°©ë¬¸: {detail_url}")
                    detail_response = self.session.get(detail_url, verify=self.verify_ssl, timeout=self.timeout)
                    time.sleep(0.5)
                    
                    # C. ì„¸ì…˜ ì¿ í‚¤ í™•ì¸
                    session_cookies = self.session.cookies
                    logger.debug(f"í˜„ì¬ ì¿ í‚¤ ìˆ˜: {len(session_cookies)}")
                
                # 2. ë‹¤ìš´ë¡œë“œ ì‹œ ì™„ì „í•œ ë¸Œë¼ìš°ì € í—¤ë” ëª¨ë°©
                download_headers = {
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
                    'Accept-Encoding': 'gzip, deflate',
                    'Cache-Control': 'no-cache',
                    'Pragma': 'no-cache',
                    'Sec-Fetch-Dest': 'document',
                    'Sec-Fetch-Mode': 'navigate',
                    'Sec-Fetch-Site': 'same-origin',
                    'Sec-Fetch-User': '?1',
                    'Upgrade-Insecure-Requests': '1',
                    'Referer': detail_url if wr_id_match else f"{self.base_url}/bbs/board.php?bo_table=sub7_1"
                }
                
                # ê¸°ì¡´ í—¤ë” ë°±ì—… ë° ì ìš©
                original_headers = self.session.headers.copy()
                self.session.headers.update(download_headers)
                
                # 3. ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
                response = self.session.get(file_url, timeout=self.timeout * 2, verify=self.verify_ssl, stream=True)
                
                # í—¤ë” ë³µì›
                self.session.headers = original_headers
                
                # 4. ì‘ë‹µ ìƒíƒœ í™•ì¸
                response.raise_for_status()
                
                # 5. Content-Type 1ì°¨ ê²€ì¦
                content_type = response.headers.get('Content-Type', '').lower()
                content_length = response.headers.get('Content-Length')
                content_disposition = response.headers.get('Content-Disposition', '')
                
                logger.debug(f"Content-Type: {content_type}")
                logger.debug(f"Content-Length: {content_length}")
                logger.debug(f"Content-Disposition: {content_disposition}")
                
                # 6. HTML ì—ëŸ¬í˜ì´ì§€ ê°ì§€ (ë‹¤ì¤‘ ê²€ì¦)
                first_chunk = next(response.iter_content(chunk_size=2048), b'')
                
                # A. Content-Typeì´ text/htmlì¸ ê²½ìš°
                if 'text/html' in content_type:
                    # B. HTML ë¬¸ì„œ ì‹œì‘ íƒœê·¸ í™•ì¸
                    html_indicators = [b'<!doctype html', b'<html', b'<HTML', b'<!DOCTYPE html']
                    is_html = any(indicator in first_chunk.lower() for indicator in html_indicators)
                    
                    if is_html:
                        # C. í•œêµ­ì–´ ì—ëŸ¬ ë©”ì‹œì§€ í™•ì¸
                        try:
                            text_content = first_chunk.decode('utf-8', errors='ignore')
                            error_keywords = [
                                'ì˜ëª»ëœ ì ‘ê·¼', 'ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤', 'íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤', 
                                'access denied', 'permission denied', 'file not found',
                                'ì˜¤ë¥˜ì•ˆë‚´ í˜ì´ì§€', 'error page', '<title>ì˜¤ë¥˜ì•ˆë‚´ í˜ì´ì§€',  # SCHERB íŠ¹í™” ì—ëŸ¬ í‚¤ì›Œë“œ
                                'ì‚°ì²­í•œë°©ì•½ì´ˆì¶•ì œ</title>', 'scherb.or.kr'  # ì‚¬ì´íŠ¸ ë©”ì¸í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ëœ ê²½ìš°
                            ]
                            is_error = any(keyword in text_content for keyword in error_keywords)
                            
                            if is_error:
                                logger.warning(f"HTML ì—ëŸ¬í˜ì´ì§€ ê°ì§€: {file_url}")
                                logger.debug(f"ì—ëŸ¬ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {text_content[:300]}")
                                
                                # ì¬ì‹œë„ ì „ ëŒ€ê¸°
                                if attempt < max_retries - 1:
                                    wait_time = (attempt + 1) * 2
                                    logger.info(f"{wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                                    time.sleep(wait_time)
                                    continue
                                else:
                                    logger.error(f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ - ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {file_url}")
                                    return False
                        except:
                            pass
                
                # 7. íŒŒì¼ í¬ê¸° ê²€ì¦ (ì•Œë ¤ì§„ ì—ëŸ¬ í˜ì´ì§€ í¬ê¸° ì²´í¬)
                known_error_sizes = [4527, 4500, 4600, 2048]  # ì•Œë ¤ì§„ ì—ëŸ¬ í˜ì´ì§€ í¬ê¸°ë“¤ (2048 ì¶”ê°€)
                if content_length and int(content_length) in known_error_sizes:
                    logger.warning(f"ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒŒì¼ í¬ê¸° ê°ì§€: {content_length} bytes")
                    if attempt < max_retries - 1:
                        time.sleep((attempt + 1) * 2)
                        continue
                
                # 8. íŒŒì¼ëª… ì¶”ì¶œ ë° ê²½ë¡œ ì„¤ì •
                actual_filename = self._extract_filename_from_response(response, save_path)
                
                # ë””ë ‰í† ë¦¬ ìƒì„± í™•ë³´
                os.makedirs(os.path.dirname(actual_filename), exist_ok=True)
                
                # 9. ìŠ¤íŠ¸ë¦¬ë° ë‹¤ìš´ë¡œë“œ (ì²« ë²ˆì§¸ ì²­í¬ í¬í•¨)
                total_size = 0
                with open(actual_filename, 'wb') as f:
                    # ì²« ë²ˆì§¸ ì²­í¬ ì“°ê¸°
                    if first_chunk:
                        f.write(first_chunk)
                        total_size += len(first_chunk)
                    
                    # ë‚˜ë¨¸ì§€ ì²­í¬ë“¤ ì“°ê¸°
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            total_size += len(chunk)
                
                # 10. ë‹¤ìš´ë¡œë“œ í›„ ê²€ì¦
                actual_file_size = os.path.getsize(actual_filename)
                
                # A. í¬ê¸° ì´ìƒ ì—¬ë¶€ ì²´í¬ (í™•ì¥)
                known_error_sizes = [4527, 4500, 4600, 2048]  # ì•Œë ¤ì§„ ì—ëŸ¬ í˜ì´ì§€ í¬ê¸°ë“¤
                if actual_file_size in known_error_sizes:
                    logger.error(f"ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì´ ì•Œë ¤ì§„ ì—ëŸ¬ í˜ì´ì§€ í¬ê¸°: {actual_file_size} bytes")
                    os.remove(actual_filename)  # ì˜ëª»ëœ íŒŒì¼ ì‚­ì œ
                    if attempt < max_retries - 1:
                        time.sleep((attempt + 1) * 2)
                        continue
                    return False
                
                # B. SCHERB íŠ¹í™” HTML ì—ëŸ¬í˜ì´ì§€ ìƒì„¸ ê²€ì¦
                if actual_file_size < 5000:  # 5KB ë¯¸ë§Œ íŒŒì¼ë“¤ì€ ëª¨ë‘ ê²€ì¦
                    with open(actual_filename, 'rb') as f:
                        content_sample = f.read(500)  # ë” ë§ì€ ë‚´ìš© ì½ê¸°
                        
                        # HTML ë¬¸ì„œì¸ì§€ í™•ì¸
                        if b'<html' in content_sample.lower() or b'<!doctype' in content_sample.lower():
                            content_text = content_sample.decode('utf-8', errors='ignore')
                            
                            # SCHERB íŠ¹í™” ì—ëŸ¬í˜ì´ì§€ í‚¤ì›Œë“œ ì²´í¬
                            scherb_error_indicators = [
                                'ì˜¤ë¥˜ì•ˆë‚´ í˜ì´ì§€',
                                'ì‚°ì²­í•œë°©ì•½ì´ˆì¶•ì œ</title>',
                                'scherb.or.kr',
                                'og:title" content="ì‚°ì²­í•œë°©ì•½ì´ˆì¶•ì œ"',
                                'meta property="og:site_name" content="ì‚°ì²­í•œë°©ì•½ì´ˆì¶•ì œ"'
                            ]
                            
                            if any(indicator in content_text for indicator in scherb_error_indicators):
                                logger.error(f"SCHERB ì—ëŸ¬í˜ì´ì§€ ê°ì§€: {actual_filename}")
                                logger.debug(f"ì—ëŸ¬ ë‚´ìš©: {content_text[:200]}")
                                os.remove(actual_filename)
                                if attempt < max_retries - 1:
                                    time.sleep((attempt + 1) * 2)
                                    continue
                                return False
                
                # C. ì„±ê³µ
                logger.info(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {actual_filename} ({actual_file_size:,} bytes)")
                return True
                
            except requests.exceptions.RequestException as e:
                logger.error(f"ë‹¤ìš´ë¡œë“œ ìš”ì²­ ì˜¤ë¥˜ {attempt + 1}/{max_retries}: {e}")
                if attempt < max_retries - 1:
                    time.sleep((attempt + 1) * 3)
                    continue
            except Exception as e:
                logger.error(f"ë‹¤ìš´ë¡œë“œ ì˜ˆì™¸ ì˜¤ë¥˜ {attempt + 1}/{max_retries}: {e}")
                if attempt < max_retries - 1:
                    time.sleep((attempt + 1) * 3)
                    continue
        
        logger.error(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ìµœì¢… ì‹¤íŒ¨: {file_url}")
        return False
    
    def _extract_filename_from_response(self, response, default_path):
        """ì‘ë‹µ í—¤ë”ì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ ë° í•œê¸€ ì²˜ë¦¬ (ê°œì„ ëœ ë²„ì „)"""
        save_dir = os.path.dirname(default_path)
        original_filename = os.path.basename(default_path)
        
        content_disposition = response.headers.get('Content-Disposition', '')
        
        if content_disposition:
            # RFC 5987 í˜•ì‹ ìš°ì„  ì²˜ë¦¬ (filename*=UTF-8''encoded_name)
            rfc5987_match = re.search(r"filename\*=([^']*)'([^']*)'(.+)", content_disposition)
            if rfc5987_match:
                encoding, lang, encoded_filename = rfc5987_match.groups()
                try:
                    # URL ë””ì½”ë”© í›„ ì§€ì •ëœ ì¸ì½”ë”©ìœ¼ë¡œ ë””ì½”ë”©
                    decoded_filename = unquote(encoded_filename, encoding=encoding or 'utf-8')
                    clean_filename = self.sanitize_filename(decoded_filename)
                    return os.path.join(save_dir, clean_filename)
                except Exception as e:
                    logger.debug(f"RFC 5987 ë””ì½”ë”© ì‹¤íŒ¨: {e}")
            
            # ì¼ë°˜ filename íŒŒë¼ë¯¸í„° ì²˜ë¦¬
            filename_match = re.search(r'filename[^;=\n]*=([\'"]*)(.*?)\1', content_disposition)
            if filename_match:
                raw_filename = filename_match.group(2)
                
                # í•œê¸€ íŒŒì¼ëª… ë‹¤ë‹¨ê³„ ë””ì½”ë”© ì‹œë„
                decodings_to_try = [
                    # URL ì¸ì½”ë”©ëœ ê²½ìš°
                    lambda x: unquote(x, encoding='utf-8'),
                    lambda x: unquote(x, encoding='euc-kr'),
                    lambda x: unquote(x, encoding='cp949'),
                    # Latin-1 â†’ ì‹¤ì œ ì¸ì½”ë”©
                    lambda x: x.encode('latin-1').decode('utf-8'),
                    lambda x: x.encode('latin-1').decode('euc-kr'),
                    lambda x: x.encode('latin-1').decode('cp949'),
                    # ì›ë³¸ ê·¸ëŒ€ë¡œ
                    lambda x: x
                ]
                
                for decode_func in decodings_to_try:
                    try:
                        decoded = decode_func(raw_filename)
                        if decoded and not decoded.isspace() and len(decoded) > 0:
                            # í•œê¸€ì´ í¬í•¨ë˜ì–´ ìˆê±°ë‚˜ í™•ì¥ìê°€ ìˆìœ¼ë©´ ìœ íš¨í•œ íŒŒì¼ëª…ìœ¼ë¡œ ê°„ì£¼
                            if any(ord(char) > 127 for char in decoded) or '.' in decoded:
                                clean_filename = self.sanitize_filename(decoded.replace('+', ' '))
                                logger.debug(f"íŒŒì¼ëª… ë””ì½”ë”© ì„±ê³µ: {raw_filename} â†’ {clean_filename}")
                                return os.path.join(save_dir, clean_filename)
                    except Exception as e:
                        logger.debug(f"ë””ì½”ë”© ì‹œë„ ì‹¤íŒ¨: {e}")
                        continue
        
        # Content-Dispositionì—ì„œ ì¶”ì¶œ ì‹¤íŒ¨í•œ ê²½ìš°, ê¸°ì¡´ íŒŒì¼ëª…ì„ URL ë””ì½”ë”© ì‹œë„
        if original_filename and '%' in original_filename:
            try:
                decoded_original = unquote(original_filename, encoding='utf-8')
                clean_filename = self.sanitize_filename(decoded_original)
                logger.debug(f"ê¸°ì¡´ íŒŒì¼ëª… ë””ì½”ë”©: {original_filename} â†’ {clean_filename}")
                return os.path.join(save_dir, clean_filename)
            except Exception as e:
                logger.debug(f"ê¸°ì¡´ íŒŒì¼ëª… ë””ì½”ë”© ì‹¤íŒ¨: {e}")
        
        return default_path
    
    def _rebuild_response_with_first_chunk(self, response, first_chunk):
        """ì²« ë²ˆì§¸ ì²­í¬ë¥¼ ì´ë¯¸ ì½ì€ ì‘ë‹µì„ ì¬êµ¬ì„±í•˜ëŠ” í—¬í¼ ë©”ì„œë“œ"""
        class ChunkedResponse:
            def __init__(self, original_response, first_chunk):
                self.original_response = original_response
                self.first_chunk = first_chunk
                self.first_chunk_sent = False
                
            def iter_content(self, chunk_size=8192):
                if not self.first_chunk_sent:
                    self.first_chunk_sent = True
                    yield self.first_chunk
                
                for chunk in self.original_response.iter_content(chunk_size=chunk_size):
                    yield chunk
                    
            def __getattr__(self, name):
                return getattr(self.original_response, name)
        
        return ChunkedResponse(response, first_chunk)
    
    def sanitize_filename(self, filename: str) -> str:
        """íŒŒì¼ëª… ì •ë¦¬"""
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        filename = filename.replace('\n', '').replace('\t', '').strip()
        return filename[:200]  # íŒŒì¼ëª… ê¸¸ì´ ì œí•œ
    
    def scrape_pages(self, max_pages: int = 3, output_base: str = "output") -> dict:
        """í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰"""
        results = {
            'total_announcements': 0,
            'total_files': 0,
            'successful_downloads': 0,
            'failed_downloads': 0,
            'pages_processed': 0
        }
        
        try:
            for page_num in range(1, max_pages + 1):
                logger.info(f"\n{'='*50}")
                logger.info(f"í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì‹œì‘")
                logger.info(f"{'='*50}")
                
                # ëª©ë¡ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
                list_url = self.get_list_url(page_num)
                response = self.get_page(list_url)
                
                if not response:
                    logger.error(f"í˜ì´ì§€ {page_num} ì½˜í…ì¸  ë¡œë”© ì‹¤íŒ¨")
                    break
                
                html_content = response.text
                
                # ê³µê³  ëª©ë¡ íŒŒì‹±
                announcements = self.parse_list_page(html_content)
                
                if not announcements:
                    logger.warning(f"í˜ì´ì§€ {page_num}ì—ì„œ ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    break
                
                results['total_announcements'] += len(announcements)
                
                # ê° ê³µê³  ì²˜ë¦¬
                for announcement in announcements:
                    try:
                        # ìƒì„¸ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
                        detail_response = self.get_page(announcement['url'])
                        if not detail_response:
                            continue
                        
                        detail_html = detail_response.text
                        
                        # ìƒì„¸ ì •ë³´ íŒŒì‹±
                        detail_info = self.parse_detail_page(detail_html)
                        
                        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
                        announcement_dir = os.path.join(output_base, f"{announcement['number']}_{self.sanitize_filename(announcement['title'][:50])}")
                        os.makedirs(announcement_dir, exist_ok=True)
                        
                        # ë³¸ë¬¸ ì €ì¥
                        content_file = os.path.join(announcement_dir, "content.md")
                        with open(content_file, 'w', encoding='utf-8') as f:
                            f.write(f"# {detail_info['title']}\n\n")
                            f.write(f"- ì¹´í…Œê³ ë¦¬: {announcement['category']}\n")
                            f.write(f"- ë²ˆí˜¸: {announcement['number']}\n")
                            f.write(f"- ë‚ ì§œ: {detail_info['date']}\n")
                            f.write(f"- ì›ë³¸ URL: {announcement['url']}\n\n")
                            f.write("## ë³¸ë¬¸\n\n")
                            f.write(detail_info['content'])
                        
                        # ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                        if detail_info['attachments']:
                            # attachments ë””ë ‰í† ë¦¬ ìƒì„±
                            attachments_dir = os.path.join(announcement_dir, "attachments")
                            os.makedirs(attachments_dir, exist_ok=True)
                            
                            for attachment in detail_info['attachments']:
                                file_path = os.path.join(attachments_dir, attachment['filename'])
                                
                                results['total_files'] += 1
                                if self.download_file(attachment['url'], file_path):
                                    results['successful_downloads'] += 1
                                else:
                                    results['failed_downloads'] += 1
                        
                        logger.info(f"ê³µê³  ì²˜ë¦¬ ì™„ë£Œ: {announcement['title'][:50]}...")
                        
                    except Exception as e:
                        logger.error(f"ê³µê³  ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue
                
                results['pages_processed'] += 1
                
                # í˜ì´ì§€ ê°„ ëŒ€ê¸°
                if page_num < max_pages:
                    time.sleep(self.delay_between_pages)
            
        except Exception as e:
            logger.error(f"ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return results

def test_scherb_scraper(pages=1):
    """SCHERB ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸ - ê°œì„ ëœ ë²„ì „"""
    # ë¡œê·¸ ë ˆë²¨ ì„¤ì • (DEBUGë¡œ ì„¤ì •í•˜ì—¬ ë” ìì„¸í•œ ì •ë³´ í™•ì¸)
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    scraper = EnhancedScherbScraper()
    output_dir = "output/scherb_improved"
    os.makedirs(output_dir, exist_ok=True)
    
    logger.info(f"ê°œì„ ëœ SCHERB ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸ ì‹œì‘ - {pages}í˜ì´ì§€")
    results = scraper.scrape_pages(max_pages=pages, output_base=output_dir)
    
    logger.info(f"\n{'='*60}")
    logger.info("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    logger.info(f"{'='*60}")
    logger.info(f"ğŸ“„ ì²˜ë¦¬ëœ í˜ì´ì§€: {results['pages_processed']}")
    logger.info(f"ğŸ“‹ ì´ ê³µê³  ìˆ˜: {results['total_announcements']}")
    logger.info(f"ğŸ“ ë°œê²¬ëœ íŒŒì¼ ìˆ˜: {results['total_files']}")
    logger.info(f"âœ… ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {results['successful_downloads']}")
    logger.info(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {results['failed_downloads']}")
    
    if results['total_files'] > 0:
        success_rate = (results['successful_downloads'] / results['total_files']) * 100
        logger.info(f"ğŸ¯ ì„±ê³µë¥ : {success_rate:.1f}%")
        
        if success_rate < 50:
            logger.warning("âš ï¸  ì„±ê³µë¥ ì´ 50% ë¯¸ë§Œì…ë‹ˆë‹¤. ì¶”ê°€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        elif success_rate < 80:
            logger.info("âš¡ ì„±ê³µë¥ ì´ ì–‘í˜¸í•©ë‹ˆë‹¤. ì¶”ê°€ ìµœì í™” ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        else:
            logger.info("ğŸ‰ ì„±ê³µë¥ ì´ ìš°ìˆ˜í•©ë‹ˆë‹¤!")
    
    # ì‹¤ì œ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ í™•ì¸
    logger.info(f"\nğŸ“ ì‹¤ì œ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ í˜„í™©:")
    try:
        total_files_downloaded = 0
        for root, dirs, files in os.walk(output_dir):
            if 'attachments' in root and files:
                actual_files = [f for f in files if not f.startswith('.')]
                if actual_files:
                    rel_path = os.path.relpath(root, output_dir)
                    logger.info(f"   {rel_path}: {len(actual_files)}ê°œ íŒŒì¼")
                    total_files_downloaded += len(actual_files)
        
        logger.info(f"ğŸ’¾ ì‹¤ì œ ì €ì¥ëœ íŒŒì¼ ì´ ê°œìˆ˜: {total_files_downloaded}ê°œ")
        
        if total_files_downloaded != results['successful_downloads']:
            logger.warning(f"âš ï¸  ë³´ê³ ëœ ì„±ê³µ ìˆ˜({results['successful_downloads']})ì™€ ì‹¤ì œ íŒŒì¼ ìˆ˜({total_files_downloaded})ê°€ ë‹¤ë¦…ë‹ˆë‹¤.")
            
    except Exception as e:
        logger.error(f"íŒŒì¼ í˜„í™© í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
    
    logger.info(f"{'='*60}")
    return results

if __name__ == "__main__":
    test_scherb_scraper(1)  # 1í˜ì´ì§€ë§Œ í…ŒìŠ¤íŠ¸