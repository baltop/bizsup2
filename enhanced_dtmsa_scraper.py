#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DTMSA(ëŒ€êµ¬ì „í†µì‹œì¥ì§„í¥ì¬ë‹¨) ê³µì§€ì‚¬í•­ ìŠ¤í¬ë˜í¼
URL: https://www.dtmsa.or.kr/announcements
"""

import re
import json
import logging
from typing import Dict, List, Any, Optional
from urllib.parse import urljoin, urlparse, parse_qs
from bs4 import BeautifulSoup
from enhanced_base_scraper import EnhancedBaseScraper

logger = logging.getLogger(__name__)

class EnhancedDtmsaScraper(EnhancedBaseScraper):
    """DTMSA(ëŒ€êµ¬ì „í†µì‹œì¥ì§„í¥ì¬ë‹¨) ê³µì§€ì‚¬í•­ ìŠ¤í¬ë˜í¼"""
    
    def __init__(self):
        super().__init__()
        
        # ì‚¬ì´íŠ¸ ê¸°ë³¸ ì„¤ì •
        self.base_url = "https://www.dtmsa.or.kr"
        self.list_url = "https://www.dtmsa.or.kr/announcements"
        self.start_url = self.list_url
        
        # í—¤ë” ì„¤ì •
        self.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        self.session.headers.update(self.headers)
        
        # í˜„ì¬ ìƒì„¸ í˜ì´ì§€ URL ì €ì¥ (Referer ìš©)
        self.current_detail_url = None
        
    def get_list_url(self, page_num: int) -> str:
        """í˜ì´ì§€ ë²ˆí˜¸ì— ë”°ë¥¸ ëª©ë¡ URL ìƒì„±"""
        if page_num == 1:
            return self.list_url
        else:
            return f"{self.list_url}?page={page_num}"
    
    def parse_list_page(self, html_content: str) -> List[Dict[str, Any]]:
        """ëª©ë¡ í˜ì´ì§€ íŒŒì‹±"""
        soup = BeautifulSoup(html_content, 'html.parser')
        announcements = []
        
        # ì‹¤ì œ HTML êµ¬ì¡° í™•ì¸ì„ ìœ„í•œ ë””ë²„ê¹…
        logger.debug(f"HTML êµ¬ì¡° í™•ì¸: {html_content[:500]}...")
        
        # ê³µì§€ì‚¬í•­ ëª©ë¡ ì°¾ê¸° - ì‹¤ì œ HTMLì—ì„œëŠ” list íƒœê·¸ê°€ ì•„ë‹Œ ë‹¤ë¥¸ êµ¬ì¡°ì¼ ìˆ˜ ìˆìŒ
        # ë¸Œë¼ìš°ì €ì—ì„œ í™•ì¸í•œ ë°”ë¡œëŠ” list > listitem > link êµ¬ì¡°
        
        # ë°©ë²• 1: ì§ì ‘ list íƒœê·¸ ì°¾ê¸°
        announcement_list = soup.find('list', recursive=True)
        if announcement_list:
            logger.debug("list íƒœê·¸ ë°œê²¬")
            list_items = announcement_list.find_all('listitem')
            logger.debug(f"listitem ê°œìˆ˜: {len(list_items)}")
            
            for i, item in enumerate(list_items):
                try:
                    # ê° í•­ëª©ì—ì„œ ë§í¬ ì°¾ê¸°
                    link_element = item.find('link')
                    if not link_element:
                        continue
                    
                    # ë§í¬ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ì¶”ì¶œ
                    link_text = link_element.get_text(strip=True)
                    if not link_text:
                        continue
                    
                    # URL ì¶”ì¶œ
                    detail_url = link_element.get('href')
                    if not detail_url:
                        continue
                    
                    # ì ˆëŒ€ URL ìƒì„±
                    detail_url = urljoin(self.base_url, detail_url)
                    
                    # ë§í¬ í…ìŠ¤íŠ¸ íŒŒì‹± (ì˜ˆ: "809 ê³µì§€ 2025 ë™ì„±ë¡œ ë¦¬ë¹™ë© í”„ë¡œê·¸ë¨ ì°¸ê°€ì ëª¨ì§‘ (ì—°ì¥ ê³µê³ ) 2025.7.10 289")
                    # ë˜ëŠ” "812 [ìœ ê´€ê³µê³ ] íì—…Â·íœ´ì—…(ì˜ˆì •) ì†Œìƒê³µì¸ ì¬ê¸°ì§€ì› ì‹¬ë¦¬íšŒë³µ ì‚°ë¦¼ì¹˜ìœ í”„ë¡œê·¸ë¨ ëª¨ì§‘ ì•ˆ 2025.7.18 7"
                    
                    # ë²ˆí˜¸ ì¶”ì¶œ (ì²« ë²ˆì§¸ ìˆ«ì)
                    number_match = re.match(r'^(\d+)', link_text)
                    number = number_match.group(1) if number_match else str(i + 1)
                    
                    # ë‚ ì§œ ì¶”ì¶œ (YYYY.M.D í˜•íƒœ)
                    date_match = re.search(r'(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})', link_text)
                    date = f"{date_match.group(1)}.{date_match.group(2)}.{date_match.group(3)}" if date_match else ''
                    
                    # ì¡°íšŒìˆ˜ ì¶”ì¶œ (ë§ˆì§€ë§‰ ìˆ«ì)
                    views_match = re.search(r'(\d+)$', link_text)
                    views = views_match.group(1) if views_match else ''
                    
                    # ì œëª© ì¶”ì¶œ (ë²ˆí˜¸ ì´í›„ë¶€í„° ë‚ ì§œ ì´ì „ê¹Œì§€)
                    title_start = len(number) + 1 if number_match else 0
                    title_end = date_match.start() if date_match else len(link_text)
                    title = link_text[title_start:title_end].strip()
                    
                    # ì¡°íšŒìˆ˜ ë¶€ë¶„ ì œê±°
                    if views_match:
                        title = title.replace(views_match.group(0), '').strip()
                    
                    # ê³µì§€ì‚¬í•­ ì •ë³´ êµ¬ì„±
                    announcement = {
                        'number': number,
                        'title': title,
                        'url': detail_url,
                        'date': date,
                        'views': views,
                        'has_attachment': False  # ìƒì„¸ í˜ì´ì§€ì—ì„œ í™•ì¸
                    }
                    
                    announcements.append(announcement)
                    logger.debug(f"ê³µê³  íŒŒì‹± ì™„ë£Œ: {title[:50]}...")
                    
                except Exception as e:
                    logger.error(f"ê³µê³  {i+1} íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
        
        # ë°©ë²• 2: ì¼ë°˜ì ì¸ HTML êµ¬ì¡°ë¡œ ì°¾ê¸° (ê¸°ë³¸ HTML íƒœê·¸ ì‚¬ìš©)
        if not announcements:
            logger.debug("list íƒœê·¸ì—ì„œ ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ. ì¼ë°˜ HTML êµ¬ì¡°ë¡œ ì‹œë„")
            
            # ì¼ë°˜ì ì¸ ë§í¬ íŒ¨í„´ìœ¼ë¡œ ì°¾ê¸°
            all_links = soup.find_all('a', href=re.compile(r'/announcement/\d+'))
            logger.debug(f"announcement ë§í¬ ê°œìˆ˜: {len(all_links)}")
            
            for i, link in enumerate(all_links):
                try:
                    link_text = link.get_text(strip=True)
                    if not link_text:
                        continue
                    
                    detail_url = link.get('href')
                    if not detail_url:
                        continue
                    
                    # ì ˆëŒ€ URL ìƒì„±
                    detail_url = urljoin(self.base_url, detail_url)
                    
                    # ë§í¬ í…ìŠ¤íŠ¸ íŒŒì‹±
                    number_match = re.match(r'^(\d+)', link_text)
                    number = number_match.group(1) if number_match else str(i + 1)
                    
                    date_match = re.search(r'(\d{4})\.\s*(\d{1,2})\.\s*(\d{1,2})', link_text)
                    date = f"{date_match.group(1)}.{date_match.group(2)}.{date_match.group(3)}" if date_match else ''
                    
                    views_match = re.search(r'(\d+)$', link_text)
                    views = views_match.group(1) if views_match else ''
                    
                    # ì œëª© ì¶”ì¶œ ê°œì„ 
                    title = link_text
                    if number_match:
                        title = title[len(number):].strip()
                    if date_match:
                        title = title[:title.find(date_match.group(0))].strip()
                    if views_match:
                        title = title.replace(views_match.group(0), '').strip()
                    
                    # ì œëª© ì •ë¦¬
                    title = re.sub(r'\s+', ' ', title).strip()  # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
                    
                    announcement = {
                        'number': number,
                        'title': title,
                        'url': detail_url,
                        'date': date,
                        'views': views,
                        'has_attachment': False
                    }
                    
                    announcements.append(announcement)
                    logger.debug(f"ê³µê³  íŒŒì‹± ì™„ë£Œ: {title[:50]}...")
                    
                except Exception as e:
                    logger.error(f"ê³µê³  {i+1} íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
        
        logger.info(f"ì´ {len(announcements)}ê°œ ê³µê³  íŒŒì‹± ì™„ë£Œ")
        return announcements
    
    def parse_detail_page(self, html_content: str, detail_url: str = None) -> Dict[str, Any]:
        """ìƒì„¸ í˜ì´ì§€ íŒŒì‹±"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # í˜„ì¬ ìƒì„¸ í˜ì´ì§€ URL ì €ì¥
        if detail_url:
            self.current_detail_url = detail_url
        
        # ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ (ë³¸ë¬¸ ì¶”ì¶œ ì „ì— ë¨¼ì € ì‹¤í–‰)
        attachments = self._extract_attachments(soup)
        
        # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
        content = self._extract_content(soup)
        
        return {
            'content': content,
            'attachments': attachments
        }
    
    def _extract_content(self, soup: BeautifulSoup) -> str:
        """ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ"""
        content_parts = []
        
        # ë°©ë²• 1: ì œëª© ì¶”ì¶œ (table êµ¬ì¡°ì—ì„œ)
        title_rows = soup.find_all('row')
        for row in title_rows:
            row_text = row.get_text(strip=True)
            if row_text and len(row_text) > 5 and not 'ì‘ì„±ì¼' in row_text and not 'ì¡°íšŒìˆ˜' in row_text and not 'ì²¨ë¶€íŒŒì¼' in row_text:
                content_parts.append(f"# {row_text}")
                break
        
        # ë°©ë²• 2: ë©”íƒ€ ì •ë³´ ì¶”ì¶œ (ì‘ì„±ì¼, ì¡°íšŒìˆ˜ ë“±)
        meta_rows = soup.find_all('row')
        for row in meta_rows:
            row_text = row.get_text(strip=True)
            if 'ì‘ì„±ì¼' in row_text or 'ì¡°íšŒìˆ˜' in row_text:
                content_parts.append(f"**{row_text}**")
        
        # ë°©ë²• 3: ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ - í…Œì´ë¸” êµ¬ì¡° ê¸°ë°˜
        content_found = False
        for row in soup.find_all('row'):
            cell = row.find('cell')
            if cell:
                # ì²¨ë¶€íŒŒì¼ ë§í¬ê°€ ìˆëŠ” ì…€ì€ ê±´ë„ˆë›°ê¸°
                if cell.find('link') and ('ì²¨ë¶€' in cell.get_text() or '.hwp' in cell.get_text() or '.pdf' in cell.get_text()):
                    continue
                
                # ë³¸ë¬¸ ë‚´ìš©ì´ ìˆëŠ” ì…€ ì°¾ê¸°
                cell_text = cell.get_text(strip=True)
                if cell_text and len(cell_text) > 50:  # ì¶©ë¶„íˆ ê¸´ í…ìŠ¤íŠ¸
                    # generic íƒœê·¸ë“¤ ë‚´ë¶€ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    generics = cell.find_all('generic')
                    for generic in generics:
                        text = generic.get_text(strip=True)
                        if text and len(text) > 10:
                            content_parts.append(text)
                    content_found = True
                    break
        
        # ë°©ë²• 4: ì „ì²´ ë³¸ë¬¸ ì˜ì—­ì—ì„œ ì¶”ì¶œ (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
        if not content_found:
            # ì „ì²´ í˜ì´ì§€ì—ì„œ ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ ì°¾ê¸°
            for element in soup.find_all(['p', 'div', 'article', 'section']):
                text = element.get_text(strip=True)
                if text and len(text) > 20:
                    content_parts.append(text)
        
        # ìµœì¢… ë³¸ë¬¸ êµ¬ì„±
        if content_parts:
            return "\\n\\n".join(content_parts)
        else:
            return "ë³¸ë¬¸ ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def _extract_attachments(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ"""
        attachments = []
        
        # ë°©ë²• 1: ì²¨ë¶€íŒŒì¼ ë§í¬ ì°¾ê¸° (í…Œì´ë¸” êµ¬ì¡°ì—ì„œ)
        for row in soup.find_all('row'):
            cell = row.find('cell')
            if cell:
                # ì²¨ë¶€íŒŒì¼ ë§í¬ê°€ ìˆëŠ” ì…€ ì°¾ê¸°
                file_links = cell.find_all('link')
                for link in file_links:
                    link_text = link.get_text(strip=True)
                    href = link.get('href')
                    
                    # íŒŒì¼ í™•ì¥ìë‚˜ ë‹¤ìš´ë¡œë“œ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
                    if href and any(ext in link_text.lower() for ext in ['.hwp', '.pdf', '.doc', '.xls', '.png', '.jpg', '.zip']):
                        # ì ˆëŒ€ URL ìƒì„±
                        download_url = urljoin(self.base_url, href)
                        
                        # íŒŒì¼ëª… ì¶”ì¶œ
                        filename = link_text.strip()
                        if filename:
                            # íŒŒì¼ í¬ê¸° ì •ë³´ ì œê±° (ì˜ˆ: "( 88KB)" ë¶€ë¶„)
                            filename = re.sub(r'\s*\(\s*[\d.]+\s*[KMG]?B\s*\)', '', filename)
                            
                            # íŒŒì¼ í™•ì¥ì ì¶”ì¶œ
                            file_ext = filename.split('.')[-1].upper() if '.' in filename else 'UNKNOWN'
                            
                            attachments.append({
                                'filename': filename,
                                'url': download_url,
                                'size': '',
                                'type': file_ext
                            })
                            logger.debug(f"ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ ì„±ê³µ: {filename} - {download_url}")
        
        # ë°©ë²• 2: ì¼ë°˜ì ì¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬ ì°¾ê¸°
        if not attachments:
            # ì¼ë°˜ì ì¸ <a> íƒœê·¸ì—ì„œ ë‹¤ìš´ë¡œë“œ ë§í¬ ì°¾ê¸°
            all_links = soup.find_all('a', href=True)
            for link in all_links:
                href = link.get('href', '')
                link_text = link.get_text(strip=True)
                
                # ë‹¤ìš´ë¡œë“œ ê´€ë ¨ URL íŒ¨í„´ í™•ì¸
                if 'download' in href.lower() or 'fileDownload' in href:
                    # ì ˆëŒ€ URL ìƒì„±
                    download_url = urljoin(self.base_url, href)
                    
                    # íŒŒì¼ëª… ì¶”ì¶œ
                    filename = link_text.strip()
                    if filename:
                        # íŒŒì¼ í¬ê¸° ì •ë³´ ì œê±°
                        filename = re.sub(r'\s*\(\s*[\d.]+\s*[KMG]?B\s*\)', '', filename)
                        
                        # íŒŒì¼ í™•ì¥ì ì¶”ì¶œ
                        file_ext = filename.split('.')[-1].upper() if '.' in filename else 'UNKNOWN'
                        
                        attachments.append({
                            'filename': filename,
                            'url': download_url,
                            'size': '',
                            'type': file_ext
                        })
                        logger.debug(f"ì¼ë°˜ íŒŒì¼ ë§í¬ ì¶”ì¶œ: {filename} - {download_url}")
        
        # ë°©ë²• 3: ë¸Œë¼ìš°ì € ìŠ¤ëƒ…ìƒ·ì—ì„œ í™•ì¸í•œ ë‹¤ìš´ë¡œë“œ ë§í¬ íŒ¨í„´
        if not attachments:
            # /common/fileDownload/ íŒ¨í„´ ì°¾ê¸°
            download_links = soup.find_all('a', href=re.compile(r'/common/fileDownload/'))
            logger.debug(f"fileDownload íŒ¨í„´ ë§í¬ {len(download_links)}ê°œ ë°œê²¬")
            
            for link in download_links:
                href = link.get('href', '')
                filename = link.get_text(strip=True)
                
                if href and filename:
                    # ì ˆëŒ€ URL ìƒì„±
                    download_url = urljoin(self.base_url, href)
                    
                    # íŒŒì¼ í¬ê¸° ì •ë³´ ì œê±°
                    filename = re.sub(r'\s*\(\s*[\d.]+\s*[KMG]?B\s*\)', '', filename)
                    
                    # íŒŒì¼ í™•ì¥ì ì¶”ì¶œ
                    file_ext = filename.split('.')[-1].upper() if '.' in filename else 'UNKNOWN'
                    
                    attachments.append({
                        'filename': filename,
                        'url': download_url,
                        'size': '',
                        'type': file_ext
                    })
                    logger.debug(f"fileDownload íŒ¨í„´ íŒŒì¼ ì¶”ì¶œ: {filename} - {download_url}")
        
        logger.info(f"ì²¨ë¶€íŒŒì¼ {len(attachments)}ê°œ ì¶”ì¶œ")
        return attachments


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - 3í˜ì´ì§€ ìˆ˜ì§‘"""
    import sys
    import os
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler('dtmsa_scraper.log', encoding='utf-8')
        ]
    )
    
    logger.info("="*60)
    logger.info("ğŸš€ DTMSA(ëŒ€êµ¬ì „í†µì‹œì¥ì§„í¥ì¬ë‹¨) ê³µì§€ì‚¬í•­ ìŠ¤í¬ë˜í¼ ì‹œì‘")
    logger.info("="*60)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    output_dir = "output/dtmsa"
    
    # ê¸°ì¡´ ì¶œë ¥ ë””ë ‰í† ë¦¬ ì •ë¦¬ (íŒŒì¼ë§Œ ì‚­ì œ, ë””ë ‰í† ë¦¬ êµ¬ì¡° ìœ ì§€)
    if os.path.exists(output_dir):
        import shutil
        logger.info(f"ê¸°ì¡´ ì¶œë ¥ ë””ë ‰í† ë¦¬ ì •ë¦¬: {output_dir}")
        shutil.rmtree(output_dir)
    
    # ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”
    scraper = EnhancedDtmsaScraper()
    
    try:
        # 3í˜ì´ì§€ ìˆ˜ì§‘ ì‹¤í–‰
        success = scraper.scrape_pages(max_pages=3, output_base="output/dtmsa")
        
        if success:
            logger.info("âœ… ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
            
            # í†µê³„ ì¶œë ¥
            stats = scraper.get_stats()
            logger.info(f"ğŸ“Š ì²˜ë¦¬ í†µê³„: {stats}")
            
        else:
            logger.error("âŒ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨")
            return 1
            
    except KeyboardInterrupt:
        logger.info("â¹ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        return 1
    except Exception as e:
        logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    import sys
    sys.exit(main())