#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
EKR(í•œêµ­ë†ì–´ì´Œê³µì‚¬) ê³µì§€ì‚¬í•­ ìŠ¤í¬ë˜í¼
URL: https://www.ekr.or.kr/planweb/board/list.krc?contentUid=402880317cc0644a017cc0c9da9f0120&boardUid=402880317cc0644a017cc5e8000f06b7&contentUid=402880317cc0644a017cc0c9da9f0120&subPath=
"""

import re
import json
import logging
from typing import Dict, List, Any, Optional
from urllib.parse import urljoin, urlparse, parse_qs
from bs4 import BeautifulSoup
from enhanced_base_scraper import EnhancedBaseScraper

logger = logging.getLogger(__name__)

class EnhancedEkrScraper(EnhancedBaseScraper):
    """EKR(í•œêµ­ë†ì–´ì´Œê³µì‚¬) ê³µì§€ì‚¬í•­ ìŠ¤í¬ë˜í¼"""
    
    def __init__(self):
        super().__init__()
        
        # ì‚¬ì´íŠ¸ ê¸°ë³¸ ì„¤ì •
        self.base_url = "https://www.ekr.or.kr"
        self.list_url = "https://www.ekr.or.kr/planweb/board/list.krc?contentUid=402880317cc0644a017cc0c9da9f0120&boardUid=402880317cc0644a017cc5e8000f06b7&contentUid=402880317cc0644a017cc0c9da9f0120&subPath="
        self.start_url = self.list_url
        
        # í—¤ë” ì„¤ì •
        self.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        self.session.headers.update(self.headers)
        
        # í˜„ì¬ ìƒì„¸ í˜ì´ì§€ URL ì €ì¥ (Referer ìš©)
        self.current_detail_url = None
        
        # ì„¸ì…˜ ì´ˆê¸°í™” (ì¿ í‚¤ ì„¤ì •)
        self._initialize_session()
        
    def _initialize_session(self):
        """ì„¸ì…˜ ì´ˆê¸°í™” ë° ì¿ í‚¤ ì„¤ì •"""
        try:
            # ë©”ì¸ í˜ì´ì§€ ì ‘ê·¼ìœ¼ë¡œ ì„¸ì…˜ ì´ˆê¸°í™”
            logger.info("EKR ì‚¬ì´íŠ¸ ì„¸ì…˜ ì´ˆê¸°í™” ì¤‘...")
            response = self.session.get(self.base_url, timeout=10)
            response.raise_for_status()
            
            # ê²Œì‹œíŒ í˜ì´ì§€ ì ‘ê·¼ìœ¼ë¡œ ì„¸ì…˜ í™œì„±í™”
            response = self.session.get(self.list_url, timeout=10)
            response.raise_for_status()
            
            logger.info("EKR ì‚¬ì´íŠ¸ ì„¸ì…˜ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"ì„¸ì…˜ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {e}")
        
    def get_list_url(self, page_num: int) -> str:
        """í˜ì´ì§€ ë²ˆí˜¸ì— ë”°ë¥¸ ëª©ë¡ URL ìƒì„±"""
        base_params = "contentUid=402880317cc0644a017cc0c9da9f0120&boardUid=402880317cc0644a017cc5e8000f06b7&contentUid=402880317cc0644a017cc0c9da9f0120&subPath="
        
        if page_num == 1:
            return f"{self.base_url}/planweb/board/list.krc?{base_params}"
        else:
            return f"{self.base_url}/planweb/board/list.krc?{base_params}&page={page_num}"
    
    def parse_list_page(self, html_content: str) -> List[Dict[str, Any]]:
        """ëª©ë¡ í˜ì´ì§€ íŒŒì‹±"""
        soup = BeautifulSoup(html_content, 'html.parser')
        announcements = []
        
        logger.debug("EKR ì‚¬ì´íŠ¸ ëª©ë¡ í˜ì´ì§€ íŒŒì‹± ì‹œì‘")
        
        # í…Œì´ë¸” êµ¬ì¡° ì°¾ê¸° - í‘œì¤€ HTML í…Œì´ë¸” ê¸°ë°˜ (class="bbs_table")
        table = soup.find('table', class_='bbs_table')
        if not table:
            logger.warning("bbs_table í´ë˜ìŠ¤ë¥¼ ê°€ì§„ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return announcements
        
        # tr ìš”ì†Œë“¤ ì°¾ê¸°
        rows = table.find_all('tr')
        logger.debug(f"ë°œê²¬ëœ í–‰ ìˆ˜: {len(rows)}")
        
        for i, row in enumerate(rows):
            try:
                # ì²« ë²ˆì§¸ í–‰ì€ í—¤ë”ì´ë¯€ë¡œ ê±´ë„ˆë›°ê¸°
                if i == 0:
                    continue
                
                # td ìš”ì†Œë“¤ ì°¾ê¸°
                cells = row.find_all('td')
                if len(cells) < 2:  # ìµœì†Œ 2ê°œ ì´ìƒì˜ tdê°€ ìˆì–´ì•¼ í•¨
                    continue
                
                # ì²« ë²ˆì§¸ tdì—ì„œ ë²ˆí˜¸ ì¶”ì¶œ
                number_cell = cells[0]
                number = number_cell.get_text(strip=True)
                
                # ê³µì§€ì‚¬í•­ ì´ë¯¸ì§€ í™•ì¸
                is_notice = False
                notice_imgs = number_cell.find_all('img')
                for img in notice_imgs:
                    src = img.get('src', '')
                    alt = img.get('alt', '')
                    if 'ê³µì§€' in src or 'ê³µì§€' in alt or 'notice' in src.lower():
                        is_notice = True
                        break
                
                # ë²ˆí˜¸ ì •ë¦¬
                if is_notice or 'ê³µì§€' in number:
                    number = "ê³µì§€"
                elif not number or number.isspace():
                    number = f"row_{i}"
                
                # ì œëª© ë° ë§í¬ ì¶”ì¶œ (ë‘ ë²ˆì§¸ td - class="title")
                title_cell = cells[1] if len(cells) > 1 else cells[0]
                
                # ë§í¬ ì°¾ê¸°
                link_element = title_cell.find('a')
                if not link_element:
                    continue
                
                # ì œëª© ì¶”ì¶œ
                title = link_element.get_text(strip=True)
                if not title:
                    continue
                
                # URL ì¶”ì¶œ
                href = link_element.get('href')
                if not href:
                    continue
                
                # ì ˆëŒ€ URL ìƒì„± - EKR ì‚¬ì´íŠ¸ íŠ¹ì„±ì— ë§ê²Œ ìˆ˜ì •
                # ëª©ë¡ URLì„ ê¸°ë°˜ìœ¼ë¡œ ìƒëŒ€ URL ì²˜ë¦¬
                detail_url = urljoin(self.list_url, href)
                
                # ì‘ì„±ì ì¶”ì¶œ (ì„¸ ë²ˆì§¸ td)
                writer = ''
                if len(cells) > 2:
                    writer_cell = cells[2]
                    writer = writer_cell.get_text(strip=True)
                
                # ì²¨ë¶€íŒŒì¼ ì—¬ë¶€ í™•ì¸ (ë„¤ ë²ˆì§¸ td)
                has_attachment = False
                if len(cells) > 3:
                    attachment_cell = cells[3]
                    attachment_text = attachment_cell.get_text(strip=True)
                    if 'ì²¨ë¶€íŒŒì¼ ìˆìŒ' in attachment_text:
                        has_attachment = True
                
                # ë‚ ì§œ ì¶”ì¶œ (ë‹¤ì„¯ ë²ˆì§¸ td)
                date = ''
                if len(cells) > 4:
                    date_cell = cells[4]
                    date = date_cell.get_text(strip=True)
                
                # ì¡°íšŒìˆ˜ ì¶”ì¶œ (ì—¬ì„¯ ë²ˆì§¸ td)
                views = ''
                if len(cells) > 5:
                    views_cell = cells[5]
                    views = views_cell.get_text(strip=True)
                
                # ê³µì§€ì‚¬í•­ ì •ë³´ êµ¬ì„±
                announcement = {
                    'number': number,
                    'title': title,
                    'url': detail_url,
                    'writer': writer,
                    'has_attachment': has_attachment,
                    'date': date,
                    'views': views
                }
                
                announcements.append(announcement)
                logger.debug(f"ê³µê³  íŒŒì‹± ì™„ë£Œ: {number} - {title[:50]}...")
                
            except Exception as e:
                logger.error(f"ê³µê³  {i+1} íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        logger.info(f"ì´ {len(announcements)}ê°œ ê³µê³  íŒŒì‹± ì™„ë£Œ")
        return announcements
    
    def parse_detail_page(self, html_content: str, detail_url: str = None) -> Dict[str, Any]:
        """ìƒì„¸ í˜ì´ì§€ íŒŒì‹±"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # í˜„ì¬ ìƒì„¸ í˜ì´ì§€ URL ì €ì¥
        if detail_url:
            self.current_detail_url = detail_url
        
        # ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ (ë³¸ë¬¸ ì¶”ì¶œ ì „ì— ë¨¼ì € ì‹¤í–‰)
        attachments = self._extract_attachments(soup)
        
        # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
        content = self._extract_content(soup)
        
        return {
            'content': content,
            'attachments': attachments
        }
    
    def _extract_content(self, soup: BeautifulSoup) -> str:
        """ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ"""
        content_parts = []
        
        # ë°©ë²• 1: ì œëª© ì¶”ì¶œ
        title_element = soup.find('h1') or soup.find('h2')
        if title_element:
            title = title_element.get_text(strip=True)
            if title:
                content_parts.append(f"# {title}")
        
        # ë°©ë²• 2: í…Œì´ë¸” êµ¬ì¡°ì—ì„œ ë©”íƒ€ ì •ë³´ ì¶”ì¶œ
        tables = soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')
            for row in rows:
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 2:
                    label = cells[0].get_text(strip=True)
                    value = cells[1].get_text(strip=True)
                    
                    # ë©”íƒ€ ì •ë³´ í˜•íƒœë¡œ ì €ì¥
                    if any(keyword in label for keyword in ['ì‘ì„±ì¼', 'ì¡°íšŒìˆ˜', 'ì‘ì„±ì', 'ë“±ë¡ì¼', 'ìˆ˜ì •ì¼']):
                        content_parts.append(f"**{label}**: {value}")
        
        # ë°©ë²• 3: ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
        content_found = False
        
        # ì¼ë°˜ì ì¸ ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸°
        content_selectors = [
            'div.content',
            'div.view-content',
            'div.board-content',
            'div#content',
            'div.txt-area',
            'div.view_content',
            'div.board_view_content'
        ]
        
        for selector in content_selectors:
            content_div = soup.select_one(selector)
            if content_div:
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text = content_div.get_text(strip=True)
                if text and len(text) > 20:
                    content_parts.append(text)
                    content_found = True
                    break
        
        # ë°©ë²• 4: í…Œì´ë¸” ë‚´ ê¸´ í…ìŠ¤íŠ¸ ì°¾ê¸°
        if not content_found:
            for table in tables:
                for row in table.find_all('tr'):
                    for cell in row.find_all(['td', 'th']):
                        cell_text = cell.get_text(strip=True)
                        # ì¶©ë¶„íˆ ê¸´ í…ìŠ¤íŠ¸ì´ê³  ì²¨ë¶€íŒŒì¼ ê´€ë ¨ì´ ì•„ë‹Œ ê²½ìš°
                        if (cell_text and len(cell_text) > 50 and 
                            not any(keyword in cell_text for keyword in ['ì²¨ë¶€íŒŒì¼', 'ë‹¤ìš´ë¡œë“œ', 'íŒŒì¼ëª…', 'íŒŒì¼í¬ê¸°'])):
                            content_parts.append(cell_text)
                            content_found = True
        
        # ë°©ë²• 5: ì „ì²´ í˜ì´ì§€ì—ì„œ ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ ì°¾ê¸° (ìµœí›„ ìˆ˜ë‹¨)
        if not content_found:
            for element in soup.find_all(['p', 'div', 'article', 'section']):
                text = element.get_text(strip=True)
                if text and len(text) > 50:
                    content_parts.append(text)
        
        # ìµœì¢… ë³¸ë¬¸ êµ¬ì„±
        if content_parts:
            return "\\n\\n".join(content_parts)
        else:
            return "ë³¸ë¬¸ ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def _extract_attachments(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ"""
        attachments = []
        
        # ë°©ë²• 1: ì²¨ë¶€íŒŒì¼ í…Œì´ë¸”ì—ì„œ ë‹¤ìš´ë¡œë“œ ë§í¬ ì°¾ê¸°
        tables = soup.find_all('table')
        for table in tables:
            for row in table.find_all('tr'):
                for cell in row.find_all(['td', 'th']):
                    cell_text = cell.get_text(strip=True)
                    
                    # ì²¨ë¶€íŒŒì¼ ê´€ë ¨ í…ìŠ¤íŠ¸ í™•ì¸
                    if any(keyword in cell_text for keyword in ['ì²¨ë¶€íŒŒì¼', 'ì²¨ë¶€', 'ë‹¤ìš´ë¡œë“œ', 'íŒŒì¼ëª…']):
                        # í•´ë‹¹ cellì´ë‚˜ ì¸ì ‘í•œ cellì—ì„œ ë§í¬ ì°¾ê¸°
                        links = cell.find_all('a', href=True)
                        if not links:
                            # ë‹¤ìŒ cellì—ì„œ ë§í¬ ì°¾ê¸°
                            next_cell = cell.find_next_sibling(['td', 'th'])
                            if next_cell:
                                links = next_cell.find_all('a', href=True)
                        
                        for link in links:
                            href = link.get('href', '')
                            filename = link.get_text(strip=True)
                            
                            # ë‹¤ìš´ë¡œë“œ ë§í¬ íŒ¨í„´ í™•ì¸
                            if 'download.krc' in href and filename:
                                # ì ˆëŒ€ URL ìƒì„± - EKR ì‚¬ì´íŠ¸ íŠ¹ì„±ì— ë§ëŠ” URL êµ¬ì„±
                                if href.startswith('./'):
                                    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                                    download_url = urljoin(self.base_url + '/planweb/board/', href[2:])
                                elif href.startswith('/'):
                                    # ì ˆëŒ€ ê²½ë¡œ
                                    download_url = urljoin(self.base_url, href)
                                else:
                                    # ê¸°íƒ€ ê²½ìš° í˜„ì¬ ìƒì„¸ í˜ì´ì§€ URL ê¸°ë°˜ìœ¼ë¡œ ìƒì„±
                                    if self.current_detail_url:
                                        download_url = urljoin(self.current_detail_url, href)
                                    else:
                                        download_url = urljoin(self.base_url + '/planweb/board/', href)
                                
                                # íŒŒì¼ í¬ê¸° ì •ë³´ ì œê±°
                                filename = re.sub(r'\\s*\\([\\d.]+\\s*[KMG]?B\\)', '', filename)
                                
                                # íŒŒì¼ í™•ì¥ì ì¶”ì¶œ
                                file_ext = filename.split('.')[-1].upper() if '.' in filename else 'UNKNOWN'
                                
                                attachments.append({
                                    'filename': filename,
                                    'url': download_url,
                                    'size': '',
                                    'type': file_ext
                                })
                                logger.debug(f"ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ ì„±ê³µ: {filename} - {download_url}")
        
        # ë°©ë²• 2: ì¼ë°˜ì ì¸ ë‹¤ìš´ë¡œë“œ ë§í¬ íŒ¨í„´ ì°¾ê¸°
        if not attachments:
            # download.krc íŒ¨í„´ì˜ ëª¨ë“  ë§í¬ ì°¾ê¸°
            download_links = soup.find_all('a', href=re.compile(r'download\.krc'))
            logger.debug(f"download.krc íŒ¨í„´ ë§í¬ {len(download_links)}ê°œ ë°œê²¬")
            
            for link in download_links:
                href = link.get('href', '')
                filename = link.get_text(strip=True)
                
                if href and filename:
                    # ì ˆëŒ€ URL ìƒì„± - EKR ì‚¬ì´íŠ¸ íŠ¹ì„±ì— ë§ëŠ” URL êµ¬ì„±
                    if href.startswith('./'):
                        # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                        download_url = urljoin(self.base_url + '/planweb/board/', href[2:])
                    elif href.startswith('/'):
                        # ì ˆëŒ€ ê²½ë¡œ
                        download_url = urljoin(self.base_url, href)
                    else:
                        # ê¸°íƒ€ ê²½ìš° í˜„ì¬ ìƒì„¸ í˜ì´ì§€ URL ê¸°ë°˜ìœ¼ë¡œ ìƒì„±
                        if self.current_detail_url:
                            download_url = urljoin(self.current_detail_url, href)
                        else:
                            download_url = urljoin(self.base_url + '/planweb/board/', href)
                    
                    # íŒŒì¼ í¬ê¸° ì •ë³´ ì œê±°
                    filename = re.sub(r'\\s*\\([\\d.]+\\s*[KMG]?B\\)', '', filename)
                    
                    # íŒŒì¼ í™•ì¥ì ì¶”ì¶œ
                    file_ext = filename.split('.')[-1].upper() if '.' in filename else 'UNKNOWN'
                    
                    attachments.append({
                        'filename': filename,
                        'url': download_url,
                        'size': '',
                        'type': file_ext
                    })
                    logger.debug(f"ë‹¤ìš´ë¡œë“œ ë§í¬ ì¶”ì¶œ: {filename} - {download_url}")
        
        # ë°©ë²• 3: íŒŒì¼ í™•ì¥ìê°€ í¬í•¨ëœ ë§í¬ ì°¾ê¸°
        if not attachments:
            file_extensions = ['.hwp', '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx', '.zip', '.rar']
            
            for ext in file_extensions:
                ext_links = soup.find_all('a', href=re.compile(f'.*{ext}', re.IGNORECASE))
                for link in ext_links:
                    href = link.get('href', '')
                    filename = link.get_text(strip=True)
                    
                    if href and filename:
                        # ì ˆëŒ€ URL ìƒì„± - EKR ì‚¬ì´íŠ¸ íŠ¹ì„±ì— ë§ëŠ” URL êµ¬ì„±
                        if href.startswith('./'):
                            # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                            download_url = urljoin(self.base_url + '/planweb/board/', href[2:])
                        elif href.startswith('/'):
                            # ì ˆëŒ€ ê²½ë¡œ
                            download_url = urljoin(self.base_url, href)
                        else:
                            # ê¸°íƒ€ ê²½ìš° í˜„ì¬ ìƒì„¸ í˜ì´ì§€ URL ê¸°ë°˜ìœ¼ë¡œ ìƒì„±
                            if self.current_detail_url:
                                download_url = urljoin(self.current_detail_url, href)
                            else:
                                download_url = urljoin(self.base_url + '/planweb/board/', href)
                        
                        # íŒŒì¼ í¬ê¸° ì •ë³´ ì œê±°
                        filename = re.sub(r'\\s*\\([\\d.]+\\s*[KMG]?B\\)', '', filename)
                        
                        # íŒŒì¼ í™•ì¥ì ì¶”ì¶œ
                        file_ext = filename.split('.')[-1].upper() if '.' in filename else 'UNKNOWN'
                        
                        attachments.append({
                            'filename': filename,
                            'url': download_url,
                            'size': '',
                            'type': file_ext
                        })
                        logger.debug(f"í™•ì¥ì ê¸°ë°˜ íŒŒì¼ ì¶”ì¶œ: {filename} - {download_url}")
        
        logger.info(f"ì²¨ë¶€íŒŒì¼ {len(attachments)}ê°œ ì¶”ì¶œ")
        return attachments
    
    def download_file(self, url: str, save_path: str, attachment_info: Dict[str, Any] = None) -> bool:
        """íŒŒì¼ ë‹¤ìš´ë¡œë“œ (EKR íŠ¹ì„±ìƒ Referer í—¤ë” ì¶”ê°€, ì‹¤ì œ ë‹¤ìš´ë¡œë“œ ì‹œë„)"""
        try:
            # ë‹¤ìš´ë¡œë“œ ì „ Referer í—¤ë” ì„¤ì •
            headers = self.session.headers.copy()
            if self.current_detail_url:
                headers['Referer'] = self.current_detail_url
            
            # ë¶€ëª¨ í´ë˜ìŠ¤ì˜ download_file ë©”ì„œë“œ í˜¸ì¶œí•˜ë˜, í—¤ë” ì¶”ê°€
            original_headers = self.session.headers.copy()
            self.session.headers.update(headers)
            
            try:
                result = super().download_file(url, save_path, attachment_info)
                return result
            finally:
                # ì›ë˜ í—¤ë”ë¡œ ë³µì›
                self.session.headers.clear()
                self.session.headers.update(original_headers)
                
        except Exception as e:
            logger.error(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
            return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - 3í˜ì´ì§€ ìˆ˜ì§‘"""
    import sys
    import os
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler('ekr_scraper.log', encoding='utf-8')
        ]
    )
    
    logger.info("="*60)
    logger.info("ğŸš€ EKR(í•œêµ­ë†ì–´ì´Œê³µì‚¬) ê³µì§€ì‚¬í•­ ìŠ¤í¬ë˜í¼ ì‹œì‘")
    logger.info("="*60)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    output_dir = "output/ekr"
    
    # ê¸°ì¡´ ì¶œë ¥ ë””ë ‰í† ë¦¬ ì •ë¦¬ (íŒŒì¼ë§Œ ì‚­ì œ, ë””ë ‰í† ë¦¬ êµ¬ì¡° ìœ ì§€)
    if os.path.exists(output_dir):
        import shutil
        logger.info(f"ê¸°ì¡´ ì¶œë ¥ ë””ë ‰í† ë¦¬ ì •ë¦¬: {output_dir}")
        shutil.rmtree(output_dir)
    
    # ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”
    scraper = EnhancedEkrScraper()
    
    try:
        # 3í˜ì´ì§€ ì „ì²´ ìˆ˜ì§‘ ì‹¤í–‰
        success = scraper.scrape_pages(max_pages=3, output_base="output/ekr")
        
        if success:
            logger.info("âœ… ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
            
            # í†µê³„ ì¶œë ¥
            stats = scraper.get_stats()
            logger.info(f"ğŸ“Š ì²˜ë¦¬ í†µê³„: {stats}")
            
        else:
            logger.error("âŒ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨")
            return 1
            
    except KeyboardInterrupt:
        logger.info("â¹ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        return 1
    except Exception as e:
        logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    import sys
    sys.exit(main())