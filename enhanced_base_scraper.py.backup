# -*- coding: utf-8 -*-
"""
í–¥ìƒëœ ë² ì´ìŠ¤ ìŠ¤í¬ë˜í¼ - ì„¤ì • ì£¼ì… ë° íŠ¹í™”ëœ ë² ì´ìŠ¤ í´ë˜ìŠ¤ë“¤
"""

import requests
from bs4 import BeautifulSoup
import os
import time
import html2text
from urllib.parse import urljoin, urlparse, parse_qs, unquote
import re
import json
import logging
import chardet
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional, Union, Tuple
import hashlib
from datetime import datetime
import threading
from contextlib import contextmanager
import signal
import sys
from pathlib import Path

logger = logging.getLogger(__name__)

class EnhancedBaseScraper(ABC):
    """í–¥ìƒëœ ë² ì´ìŠ¤ ìŠ¤í¬ë˜í¼ - ì„¤ì • ì£¼ì… ì§€ì›"""
    
    def __init__(self):
        # ê¸°ë³¸ ì„¤ì •
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        # HTML to text ë³€í™˜ê¸°
        self.h = html2text.HTML2Text()
        self.h.ignore_links = False
        self.h.ignore_images = False
        
        # ê¸°ë³¸ê°’ë“¤
        self.verify_ssl = True
        self.default_encoding = 'auto'
        self.timeout = 120  # ê¸°ë³¸ íƒ€ì„ì•„ì›ƒ 120ì´ˆë¡œ ëŒ€í­ ì¦ê°€
        self.delay_between_requests = 1
        self.delay_between_pages = 1  # í˜ì´ì§€ ê°„ ëŒ€ê¸°ì‹œê°„ ë‹¨ì¶•
        
        # ì¬ì‹œë„ ì„¤ì •
        self.max_retries = 3
        self.retry_delay = 2
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.stats = {
            'requests_made': 0,
            'files_downloaded': 0,
            'errors_encountered': 0,
            'total_download_size': 0,
            'start_time': None,
            'end_time': None
        }
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self._lock = threading.Lock()
        self._interrupted = False
        
        # ì„¤ì • ê°ì²´ (ì„ íƒì )
        self.config = None
        
        # ë² ì´ìŠ¤ URLë“¤ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ ì„¤ì •)
        self.base_url = None
        self.list_url = None
        
        # ì¤‘ë³µ ì²´í¬ ê´€ë ¨
        self.processed_titles_file = None
        
        # í˜„ì¬ í˜ì´ì§€ ë²ˆí˜¸ (í˜ì´ì§€ë„¤ì´ì…˜ ì§€ì›)
        self.current_page_num = 1
        self.processed_titles = set()  # ì´ì „ ì‹¤í–‰ì—ì„œ ì²˜ë¦¬ëœ ì œëª©ë“¤
        self.current_session_titles = set()  # í˜„ì¬ ì„¸ì…˜ì—ì„œ ì²˜ë¦¬ëœ ì œëª©ë“¤
        self.enable_duplicate_check = True
        self.duplicate_threshold = 3  # ë™ì¼ ì œëª© 3ê°œ ë°œê²¬ì‹œ ì¡°ê¸° ì¢…ë£Œ
        
    def set_config(self, config):
        """ì„¤ì • ê°ì²´ ì£¼ì…"""
        self.config = config
        
        # ì„¤ì •ì—ì„œ ê°’ë“¤ ì ìš©
        if config:
            self.base_url = config.base_url
            self.list_url = config.list_url
            self.verify_ssl = config.ssl_verify
            
            if config.encoding != 'auto':
                self.default_encoding = config.encoding
            
            # í—¤ë” ì—…ë°ì´íŠ¸
            if hasattr(config, 'user_agent') and config.user_agent:
                self.headers['User-Agent'] = config.user_agent
                self.session.headers.update(self.headers)
    
    @abstractmethod
    def get_list_url(self, page_num):
        """í˜ì´ì§€ ë²ˆí˜¸ì— ë”°ë¥¸ ëª©ë¡ URL ë°˜í™˜"""
        pass
        
    @abstractmethod
    def parse_list_page(self, html_content: Union[str, int]) -> List[Dict[str, Any]]:
        """ëª©ë¡ í˜ì´ì§€ íŒŒì‹±"""
        pass
        
    @abstractmethod
    def parse_detail_page(self, html_content: str) -> Dict[str, Any]:
        """ìƒì„¸ í˜ì´ì§€ íŒŒì‹±"""
        pass
    
    def get_page(self, url: str, **kwargs) -> Optional[requests.Response]:
        """í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° - ì¬ì‹œë„ ë¡œì§ í¬í•¨"""
        for attempt in range(self.max_retries + 1):
            try:
                if self._interrupted:
                    logger.info("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
                    return None
                
                # ê¸°ë³¸ ì˜µì…˜ë“¤
                options = {
                    'verify': self.verify_ssl,
                    'timeout': self.timeout,
                    **kwargs
                }
                
                with self._lock:
                    self.stats['requests_made'] += 1
                
                response = self.session.get(url, **options)
                response.raise_for_status()  # HTTP ì—ëŸ¬ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
                
                # ì¸ì½”ë”© ì²˜ë¦¬
                self._fix_encoding(response)
                
                return response
                
            except requests.exceptions.RequestException as e:
                attempt_msg = "ì‹œë„ "+ str({attempt + 1}) +"/"+ str({self.max_retries + 1}) +""
                
                if attempt < self.max_retries:
                    logger.warning("í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨ "+ str({url}) +": "+ str({e}) +" - "+ str({attempt_msg}) +", "+ str({self.retry_delay}) +"ì´ˆ í›„ ì¬ì‹œë„")
                    time.sleep(self.retry_delay)
                    continue
                else:
                    logger.error("í˜ì´ì§€ ìš”ì²­ ìµœì¢… ì‹¤íŒ¨ "+ str({url}) +": "+ str({e}) +" - "+ str({attempt_msg}) +"")
                    with self._lock:
                        self.stats['errors_encountered'] += 1
                    return None
            except Exception as e:
                logger.error("ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ "+ str({url}) +": "+ str({e}) +"")
                with self._lock:
                    self.stats['errors_encountered'] += 1
                return None
        
        return None
    
    def post_page(self, url: str, data: Dict[str, Any] = None, **kwargs) -> Optional[requests.Response]:
        """POST ìš”ì²­ - ì¬ì‹œë„ ë¡œì§ í¬í•¨"""
        for attempt in range(self.max_retries + 1):
            try:
                if self._interrupted:
                    logger.info("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
                    return None
                
                options = {
                    'verify': self.verify_ssl,
                    'timeout': self.timeout,
                    **kwargs
                }
                
                with self._lock:
                    self.stats['requests_made'] += 1
                
                response = self.session.post(url, data=data, **options)
                response.raise_for_status()
                self._fix_encoding(response)
                
                return response
                
            except requests.exceptions.RequestException as e:
                attempt_msg = "ì‹œë„ "+ str({attempt + 1}) +"/"+ str({self.max_retries + 1}) +""
                
                if attempt < self.max_retries:
                    logger.warning("POST ìš”ì²­ ì‹¤íŒ¨ "+ str({url}) +": "+ str({e}) +" - "+ str({attempt_msg}) +", "+ str({self.retry_delay}) +"ì´ˆ í›„ ì¬ì‹œë„")
                    time.sleep(self.retry_delay)
                    continue
                else:
                    logger.error("POST ìš”ì²­ ìµœì¢… ì‹¤íŒ¨ "+ str({url}) +": "+ str({e}) +" - "+ str({attempt_msg}) +"")
                    with self._lock:
                        self.stats['errors_encountered'] += 1
                    return None
            except Exception as e:
                logger.error("POST ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ "+ str({url}) +": "+ str({e}) +"")
                with self._lock:
                    self.stats['errors_encountered'] += 1
                return None
        
        return None
    
    def _fix_encoding(self, response):
        """ì‘ë‹µ ì¸ì½”ë”© ìë™ ìˆ˜ì •"""
        if response.encoding is None or response.encoding == 'ISO-8859-1':
            if self.default_encoding == 'auto':
                # ìë™ ê°ì§€ ì‹œë„
                try:
                    detected = chardet.detect(response.content[:10000])
                    if detected['confidence'] > 0.7:
                        response.encoding = detected['encoding']
                    else:
                        response.encoding = response.apparent_encoding or 'utf-8'
                except:
                    response.encoding = 'utf-8'
            else:
                response.encoding = self.default_encoding
    
    def download_file(self, url: str, save_path: str, attachment_info: Dict[str, Any] = None) -> bool:
        """íŒŒì¼ ë‹¤ìš´ë¡œë“œ - ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ìŠ¤íŠ¸ë¦¬ë° ë‹¤ìš´ë¡œë“œ"""
        for attempt in range(self.max_retries + 1):
            try:
                if self._interrupted:
                    logger.info("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
                    return False
                
                logger.info("íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œì‘: "+ str({url}) +" (ì‹œë„ "+ str({attempt + 1}) +"/"+ str({self.max_retries + 1}) +")")
                
                # ë‹¤ìš´ë¡œë“œ í—¤ë” ì„¤ì •
                download_headers = self.headers.copy()
                if self.base_url:
                    download_headers['Referer'] = self.base_url
                
                # í˜„ì¬ ìƒì„¸ í˜ì´ì§€ URLì´ ìˆë‹¤ë©´ Refererë¡œ ì„¤ì •
                if hasattr(self, 'current_detail_url') and self.current_detail_url:
                    download_headers['Referer'] = self.current_detail_url
                
                with self._lock:
                    self.stats['requests_made'] += 1
                
                response = self.session.get(
                    url, 
                    headers=download_headers, 
                    stream=True, 
                    timeout=self.timeout * 2,  # íŒŒì¼ ë‹¤ìš´ë¡œë“œëŠ” ë” ê¸´ íƒ€ì„ì•„ì›ƒ
                    verify=self.verify_ssl
                )
                response.raise_for_status()
                
                # ì‹¤ì œ íŒŒì¼ëª… ì¶”ì¶œ (attachment_infoê°€ ìˆìœ¼ë©´ í•´ë‹¹ íŒŒì¼ëª… ìš°ì„  ì‚¬ìš©)
                if not attachment_info:
                    actual_filename = self._extract_filename(response, save_path)
                    if actual_filename != save_path:
                        save_path = actual_filename
                
                # ë””ë ‰í† ë¦¬ ìƒì„± ë³´ì¥
                os.makedirs(os.path.dirname(save_path), exist_ok=True)
                
                # ìŠ¤íŠ¸ë¦¬ë° ë‹¤ìš´ë¡œë“œ
                total_size = 0
                chunk_size = 8192
                
                with open(save_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=chunk_size):
                        if self._interrupted:
                            logger.info("íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘ë‹¨ë¨")
                            return False
                        
                        if chunk:
                            f.write(chunk)
                            total_size += len(chunk)
                
                file_size = os.path.getsize(save_path)
                
                with self._lock:
                    self.stats['files_downloaded'] += 1
                    self.stats['total_download_size'] += file_size
                
                logger.info("ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: "+ str({save_path}) +" ("+ str({file_size:,}) +" bytes)")
                return True
                
            except requests.exceptions.RequestException as e:
                attempt_msg = "ì‹œë„ "+ str({attempt + 1}) +"/"+ str({self.max_retries + 1}) +""
                
                if attempt < self.max_retries:
                    logger.warning("íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ "+ str({url}) +": "+ str({e}) +" - "+ str({attempt_msg}) +", "+ str({self.retry_delay}) +"ì´ˆ í›„ ì¬ì‹œë„")
                    time.sleep(self.retry_delay)
                    continue
                else:
                    logger.error("íŒŒì¼ ë‹¤ìš´ë¡œë“œ ìµœì¢… ì‹¤íŒ¨ "+ str({url}) +": "+ str({e}) +" - "+ str({attempt_msg}) +"")
                    with self._lock:
                        self.stats['errors_encountered'] += 1
                    return False
            except Exception as e:
                logger.error("íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ "+ str({url}) +": "+ str({e}) +"")
                with self._lock:
                    self.stats['errors_encountered'] += 1
                return False
        
        return False
    
    def _extract_filename(self, response, default_path: str) -> str:
        """Content-Dispositionì—ì„œ ì‹¤ì œ íŒŒì¼ëª… ì¶”ì¶œ - í–¥ìƒëœ ë²„ì „"""
        content_disposition = response.headers.get('Content-Disposition', '')
        
        if not content_disposition:
            # Content-Dispositionì´ ì—†ìœ¼ë©´ URLì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ ì‹œë„
            try:
                from urllib.parse import urlparse, unquote
                parsed_url = urlparse(response.url)
                url_filename = os.path.basename(unquote(parsed_url.path))
                if url_filename and '.' in url_filename:
                    save_dir = os.path.dirname(default_path)
                    clean_filename = self.sanitize_filename(url_filename)
                    return os.path.join(save_dir, clean_filename)
            except:
                pass
            return default_path
        
        # RFC 5987 í˜•ì‹ ìš°ì„  ì‹œë„ (filename*=UTF-8''filename.ext)
        rfc5987_match = re.search(r"filename\*=([^']*)'([^']*)'(.+)", content_disposition)
        if rfc5987_match:
            encoding = rfc5987_match.group(1) or 'utf-8'
            filename = rfc5987_match.group(3)
            try:
                filename = unquote(filename, encoding=encoding)
                save_dir = os.path.dirname(default_path)
                clean_filename = self.sanitize_filename(filename)
                logger.debug("RFC5987 íŒŒì¼ëª… ì¶”ì¶œ: "+ str({clean_filename}) +"")
                return os.path.join(save_dir, clean_filename)
            except Exception as e:
                logger.debug("RFC5987 íŒŒì¼ëª… ì²˜ë¦¬ ì‹¤íŒ¨: "+ str({e}) +"")
        
        # ì¼ë°˜ì ì¸ filename íŒŒë¼ë¯¸í„° ì‹œë„
        filename_match = re.search(r'filename[^;=\n]*=(([\'"]).*?\2|[^;\n]*)', content_disposition)
        if filename_match:
            filename = filename_match.group(1).strip('"\'')
            
            # ë‹¤ì–‘í•œ ì¸ì½”ë”© ì‹œë„ (ìˆœì„œ ìµœì í™”)
            encoding_attempts = ['utf-8', 'euc-kr', 'cp949', 'iso-8859-1']
            
            for encoding in encoding_attempts:
                try:
                    if encoding == 'utf-8':
                        # UTF-8ë¡œ ì˜ëª» í•´ì„ëœ ê²½ìš° ë³µêµ¬ ì‹œë„
                        decoded = filename.encode('latin-1').decode('utf-8')
                    else:
                        decoded = filename.encode('latin-1').decode(encoding)
                    
                    # ë¹„ì–´ìˆì§€ ì•Šì€ ìœ íš¨í•œ íŒŒì¼ëª…ì¸ì§€ í™•ì¸
                    if decoded and not decoded.isspace() and len(decoded.strip()) > 0:
                        save_dir = os.path.dirname(default_path)
                        clean_filename = self.sanitize_filename(decoded.replace('+', ' ').strip())
                        logger.debug(""+ str({encoding}) +" ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ëª… ì¶”ì¶œ: "+ str({clean_filename}) +"")
                        return os.path.join(save_dir, clean_filename)
                except Exception as e:
                    logger.debug(""+ str({encoding}) +" ì¸ì½”ë”© ì‹œë„ ì‹¤íŒ¨: "+ str({e}) +"")
                    continue
        
        logger.debug("íŒŒì¼ëª… ì¶”ì¶œ ì‹¤íŒ¨, ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©: "+ str({default_path}) +"")
        return default_path
    
    def sanitize_filename(self, filename: str) -> str:
        """íŒŒì¼ëª… ì •ë¦¬ - í–¥ìƒëœ ë²„ì „"""
        if not filename or not filename.strip():
            return "unnamed_file"
        
        # URL ë””ì½”ë”©
        try:
            filename = unquote(filename)
        except:
            pass
        
        # ê¸°ë³¸ ì •ë¦¬
        filename = filename.strip()
        
        # Windows/Linux íŒŒì¼ ì‹œìŠ¤í…œ ê¸ˆì§€ ë¬¸ì ì œê±°
        illegal_chars = r'[<>:"/\\|?*\x00-\x1f]'
        filename = re.sub(illegal_chars, '_', filename)
        
        # ì—°ì†ëœ ê³µë°±/íŠ¹ìˆ˜ë¬¸ìë¥¼ í•˜ë‚˜ë¡œ
        filename = re.sub(r'[\s_]+', '_', filename)
        
        # ì‹œì‘/ë íŠ¹ìˆ˜ë¬¸ì ì œê±°
        filename = filename.strip('._-')
        
        # ë¹ˆ íŒŒì¼ëª… ì²˜ë¦¬
        if not filename:
            return "unnamed_file"
        
        # íŒŒì¼ëª… ê¸¸ì´ ì œí•œ (í™•ì¥ì ë³´ì¡´)
        max_length = 200
        if len(filename) > max_length:
            # í™•ì¥ì ë¶„ë¦¬
            name_parts = filename.rsplit('.', 1)
            if len(name_parts) == 2 and len(name_parts[1]) <= 10:  # í™•ì¥ìê°€ 10ì ì´í•˜ì¸ ê²½ìš°ë§Œ
                name, ext = name_parts
                available_length = max_length - len(ext) - 1  # .ì„ ìœ„í•œ 1ì
                filename = name[:available_length] + '.' + ext
            else:
                filename = filename[:max_length]
        
        # ì˜ˆì•½ëœ íŒŒì¼ëª… ì²˜ë¦¬ (Windows)
        reserved_names = "+ str({'CON', 'PRN', 'AUX', 'NUL', 'COM1', 'COM2', 'COM3', 'COM4', 'COM5', 'COM6', 'COM7', 'COM8', 'COM9', 'LPT1', 'LPT2', 'LPT3', 'LPT4', 'LPT5', 'LPT6', 'LPT7', 'LPT8', 'LPT9'}) +"
        name_without_ext = filename.rsplit('.', 1)[0].upper()
        if name_without_ext in reserved_names:
            filename = '_' + filename
        
        return filename
    
    def normalize_title(self, title: str) -> str:
        """ì œëª© ì •ê·œí™” - ì¤‘ë³µ ì²´í¬ìš©"""
        if not title:
            return ""
        
        # ì•ë’¤ ê³µë°± ì œê±°
        normalized = title.strip()
        
        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        normalized = re.sub(r'\s+', ' ', normalized)
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ì¼ë¶€ í—ˆìš©)
        normalized = re.sub(r'[^\w\sê°€-í£()-]', '', normalized)
        
        # ì†Œë¬¸ì ë³€í™˜ (ì˜ë¬¸ì˜ ê²½ìš°)
        normalized = normalized.lower()
        
        return normalized
    
    def get_title_hash(self, title: str) -> str:
        """ì œëª©ì˜ í•´ì‹œê°’ ìƒì„±"""
        normalized = self.normalize_title(title)
        return hashlib.md5(normalized.encode('utf-8')).hexdigest()
    
    def load_processed_titles(self, output_base: str = 'output'):
        """ì²˜ë¦¬ëœ ì œëª© ëª©ë¡ ë¡œë“œ"""
        if not self.enable_duplicate_check:
            return
        
        # ì‚¬ì´íŠ¸ë³„ íŒŒì¼ëª… ìƒì„± - enhanced í¬í•¨
        site_name = self.__class__.__name__.replace('Scraper', '').lower()
        self.processed_titles_file = os.path.join(output_base, f'processed_titles_"+ str({site_name}) +".json')
        
        try:
            if os.path.exists(self.processed_titles_file):
                with open(self.processed_titles_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # ì œëª© í•´ì‹œë§Œ ë¡œë“œ
                    self.processed_titles = set(data.get('title_hashes', []))
                    logger.info("ê¸°ì¡´ ì²˜ë¦¬ëœ ê³µê³  "+ str({len(self.processed_titles)}) +"ê°œ ë¡œë“œ")
            else:
                self.processed_titles = set()
                logger.info("ìƒˆë¡œìš´ ì²˜ë¦¬ëœ ì œëª© íŒŒì¼ ìƒì„±")
        except Exception as e:
            logger.error("ì²˜ë¦¬ëœ ì œëª© ë¡œë“œ ì‹¤íŒ¨: "+ str({e}) +"")
            self.processed_titles = set()
    
    def save_processed_titles(self):
        """í˜„ì¬ ì„¸ì…˜ì—ì„œ ì²˜ë¦¬ëœ ì œëª©ë“¤ì„ ì´ì „ ì‹¤í–‰ ê¸°ë¡ì— í•©ì³ì„œ ì €ì¥"""
        if not self.enable_duplicate_check or not self.processed_titles_file:
            return
        
        try:
            os.makedirs(os.path.dirname(self.processed_titles_file), exist_ok=True)
            
            # í˜„ì¬ ì„¸ì…˜ì—ì„œ ì²˜ë¦¬ëœ ì œëª©ë“¤ì„ ì´ì „ ì‹¤í–‰ ê¸°ë¡ì— í•©ì¹¨
            all_processed_titles = self.processed_titles | self.current_session_titles
            
            data = {
                'title_hashes': list(all_processed_titles),
                'last_updated': datetime.now().isoformat(),
                'total_count': len(all_processed_titles)
            }
            
            with open(self.processed_titles_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                
            logger.info("ì²˜ë¦¬ëœ ì œëª© "+ str({len(all_processed_titles)}) +"ê°œ ì €ì¥ ì™„ë£Œ (ì´ì „: "+ str({len(self.processed_titles)}) +", í˜„ì¬ ì„¸ì…˜: "+ str({len(self.current_session_titles)}) +")")
        except Exception as e:
            logger.error("ì²˜ë¦¬ëœ ì œëª© ì €ì¥ ì‹¤íŒ¨: "+ str({e}) +"")
    
    def is_title_processed(self, title: str) -> bool:
        """ì œëª©ì´ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        if not self.enable_duplicate_check:
            return False
        
        title_hash = self.get_title_hash(title)
        return title_hash in self.processed_titles
    
    def add_processed_title(self, title: str):
        """í˜„ì¬ ì„¸ì…˜ì—ì„œ ì²˜ë¦¬ëœ ì œëª© ì¶”ê°€ (ì´ì „ ì‹¤í–‰ ê¸°ë¡ê³¼ëŠ” ë³„ë„ ê´€ë¦¬)"""
        if not self.enable_duplicate_check:
            return
        
        title_hash = self.get_title_hash(title)
        self.current_session_titles.add(title_hash)
    
    def filter_new_announcements(self, announcements: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], bool]:
        """ìƒˆë¡œìš´ ê³µê³ ë§Œ í•„í„°ë§ - ì´ì „ ì‹¤í–‰ ê¸°ë¡ê³¼ë§Œ ì¤‘ë³µ ì²´í¬, í˜„ì¬ ì„¸ì…˜ ë‚´ì—ì„œëŠ” ì¤‘ë³µ í—ˆìš©"""
        if not self.enable_duplicate_check:
            return announcements, False
        
        new_announcements = []
        previous_session_duplicate_count = 0  # ì´ì „ ì‹¤í–‰ ì¤‘ë³µë§Œ ì¹´ìš´íŠ¸
        
        for ann in announcements:
            title = ann.get('title', '')
            title_hash = self.get_title_hash(title)
            
            # ì´ì „ ì‹¤í–‰ì—ì„œ ì²˜ë¦¬ëœ ê³µê³ ì¸ì§€ë§Œ í™•ì¸ (í˜„ì¬ ì„¸ì…˜ì€ ì œì™¸)
            if title_hash in self.processed_titles:
                previous_session_duplicate_count += 1
                logger.debug("ì´ì „ ì‹¤í–‰ì—ì„œ ì²˜ë¦¬ëœ ê³µê³  ìŠ¤í‚µ: "+ str({title[:50]}) +"...")
                
                # ì—°ì†ëœ ì´ì „ ì‹¤í–‰ ì¤‘ë³µ ì„ê³„ê°’ ë„ë‹¬ì‹œ ì¡°ê¸° ì¢…ë£Œ ì‹ í˜¸
                if previous_session_duplicate_count >= self.duplicate_threshold:
                    logger.info("ì´ì „ ì‹¤í–‰ ì¤‘ë³µ ê³µê³  "+ str({previous_session_duplicate_count}) +"ê°œ ì—°ì† ë°œê²¬ - ì¡°ê¸° ì¢…ë£Œ ì‹ í˜¸")
                    break
            else:
                # ì´ì „ ì‹¤í–‰ì— ì—†ëŠ” ìƒˆë¡œìš´ ê³µê³ ëŠ” ë¬´ì¡°ê±´ í¬í•¨ (í˜„ì¬ ì„¸ì…˜ ë‚´ ì¤‘ë³µ ì™„ì „ ë¬´ì‹œ)
                new_announcements.append(ann)
                previous_session_duplicate_count = 0  # ìƒˆë¡œìš´ ê³µê³  ë°œê²¬ì‹œ ì¤‘ë³µ ì¹´ìš´íŠ¸ ë¦¬ì…‹
                logger.debug("ìƒˆë¡œìš´ ê³µê³  ì¶”ê°€: "+ str({title[:50]}) +"...")
        
        should_stop = previous_session_duplicate_count >= self.duplicate_threshold
        logger.info("ì „ì²´ "+ str({len(announcements)}) +"ê°œ ì¤‘ ìƒˆë¡œìš´ ê³µê³  "+ str({len(new_announcements)}) +"ê°œ, ì´ì „ ì‹¤í–‰ ì¤‘ë³µ "+ str({previous_session_duplicate_count}) +"ê°œ ë°œê²¬")
        
        return new_announcements, should_stop
    
    def process_announcement(self, announcement: Dict[str, Any], index: int, output_base: str = 'output'):
        """ê°œë³„ ê³µê³  ì²˜ë¦¬ - í–¥ìƒëœ ë²„ì „"""
        logger.info("ê³µê³  ì²˜ë¦¬ ì¤‘ "+ str({index}) +": "+ str({announcement['title']}) +"")
        
        # í´ë” ìƒì„± - íŒŒì¼ì‹œìŠ¤í…œ ì œí•œì„ ê³ ë ¤í•œ ì œëª© ê¸¸ì´ ì¡°ì •
        folder_title = self.sanitize_filename(announcement['title'])[:100]  # 100ìë¡œ ë‹¨ì¶•
        folder_name = ""+ str({index:03d}) +"_"+ str({folder_title}) +""
        
        # ìµœì¢… í´ë”ëª…ì´ 200ì ì´í•˜ê°€ ë˜ë„ë¡ ì¶”ê°€ ì¡°ì •
        if len(folder_name) > 200:
            # ì¸ë±ìŠ¤ ë¶€ë¶„(4ì) + ì–¸ë”ìŠ¤ì½”ì–´(1ì) = 5ìë¥¼ ì œì™¸í•˜ê³  195ìë¡œ ì œí•œ
            folder_title = folder_title[:195]
            folder_name = ""+ str({index:03d}) +"_"+ str({folder_title}) +""
        
        folder_path = os.path.join(output_base, folder_name)
        os.makedirs(folder_path, exist_ok=True)
        
        # ìƒì„¸ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
        # get_detail_content ë©”ì„œë“œê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš© (POST ë°©ì‹ ì‚¬ì´íŠ¸ ì§€ì›)
        html_content = None
        if hasattr(self, 'get_detail_content'):
            html_content = self.get_detail_content(announcement)
        
        # get_detail_contentê°€ ì—†ê±°ë‚˜ ì‹¤íŒ¨í–ˆìœ¼ë©´ ê¸°ë³¸ GET ë°©ì‹ ì‚¬ìš©
        if not html_content:
            response = self.get_page(announcement['url'])
            if not response:
                logger.error("ìƒì„¸ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: "+ str({announcement['title']}) +"")
                return
            html_content = response.text
        
        # ìƒì„¸ ë‚´ìš© íŒŒì‹±
        try:
            # URLì„ í•¨ê»˜ ì „ë‹¬ (URLì´ í•„ìš”í•œ íŠ¹ìˆ˜ ì‚¬ì´íŠ¸ë“¤ì„ ìœ„í•´)
            if hasattr(self, 'parse_detail_page') and 'url' in self.parse_detail_page.__code__.co_varnames:
                detail = self.parse_detail_page(html_content, announcement['url'])
            else:
                detail = self.parse_detail_page(html_content)
            logger.info("ìƒì„¸ í˜ì´ì§€ íŒŒì‹± ì™„ë£Œ - ë‚´ìš©ê¸¸ì´: "+ str({len(detail['content'])}) +", ì²¨ë¶€íŒŒì¼: "+ str({len(detail['attachments'])}) +"")
        except Exception as e:
            logger.error("ìƒì„¸ í˜ì´ì§€ íŒŒì‹± ì‹¤íŒ¨: "+ str({e}) +"")
            return
        
        # ë©”íƒ€ ì •ë³´ ìƒì„±
        meta_info = self._create_meta_info(announcement)
        
        # ë³¸ë¬¸ ì €ì¥
        content_path = os.path.join(folder_path, 'content.md')
        with open(content_path, 'w', encoding='utf-8') as f:
            f.write(meta_info + detail['content'])
        
        logger.info("ë‚´ìš© ì €ì¥ ì™„ë£Œ: "+ str({content_path}) +"")
        
        # ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        self._download_attachments(detail['attachments'], folder_path)
        
        # ì²˜ë¦¬ëœ ì œëª©ìœ¼ë¡œ ì¶”ê°€
        self.add_processed_title(announcement['title'])
        
        # ì¤‘ê°„ ì €ì¥ - ë§¤ ê³µê³  ì²˜ë¦¬ í›„ ì €ì¥ (íƒ€ì„ì•„ì›ƒ ëŒ€ë¹„)
        if len(self.current_session_titles) % 5 == 0:  # 5ê°œë§ˆë‹¤ ì €ì¥
            self.save_processed_titles()
        
        # ìš”ì²­ ê°„ ëŒ€ê¸°
        if self.delay_between_requests > 0:
            time.sleep(self.delay_between_requests)
    
    def _create_meta_info(self, announcement: Dict[str, Any]) -> str:
        """ë©”íƒ€ ì •ë³´ ìƒì„±"""
        meta_lines = ["# "+ str({announcement['title']}) +"", ""]
        
        # ë™ì ìœ¼ë¡œ ë©”íƒ€ ì •ë³´ ì¶”ê°€
        meta_fields = {
            'writer': 'ì‘ì„±ì',
            'date': 'ì‘ì„±ì¼',
            'period': 'ì ‘ìˆ˜ê¸°ê°„',
            'status': 'ìƒíƒœ',
            'organization': 'ê¸°ê´€',
            'views': 'ì¡°íšŒìˆ˜'
        }
        
        for field, label in meta_fields.items():
            if field in announcement and announcement[field]:
                meta_lines.append("**"+ str({label}) +"**: "+ str({announcement[field]}) +"")
        
        meta_lines.extend([
            "**ì›ë³¸ URL**: "+ str({announcement['url']}) +"",
            "",
            "---",
            ""
        ])
        
        return "\n".join(meta_lines)
    
    def _download_attachments(self, attachments: List[Dict[str, Any]], folder_path: str):
        """ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        if not attachments:
            logger.info("ì²¨ë¶€íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return
        
        logger.info(""+ str({len(attachments)}) +"ê°œ ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
        attachments_folder = os.path.join(folder_path, 'attachments')
        os.makedirs(attachments_folder, exist_ok=True)
        
        for i, attachment in enumerate(attachments):
            try:
                # íŒŒì¼ëª… ì¶”ì¶œ - ë‹¤ì–‘í•œ í‚¤ ì§€ì› (name, filename)
                file_name = attachment.get('filename') or attachment.get('name') or "attachment_"+ str({i+1}) +""
                logger.info("  ì²¨ë¶€íŒŒì¼ "+ str({i+1}) +": "+ str({file_name}) +"")
                
                # íŒŒì¼ëª… ì²˜ë¦¬
                file_name = self.sanitize_filename(file_name)
                if not file_name or file_name.isspace():
                    file_name = "attachment_"+ str({i+1}) +""
                
                file_path = os.path.join(attachments_folder, file_name)
                
                # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                success = self.download_file(attachment['url'], file_path, attachment)
                if not success:
                    logger.warning("ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: "+ str({file_name}) +"")
                
            except Exception as e:
                logger.error("ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: "+ str({e}) +"")
    
    def scrape_pages(self, max_pages: int = 4, output_base: str = 'output'):
        """ì—¬ëŸ¬ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ - ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í¬í•¨"""
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self.stats['start_time'] = datetime.now()
        logger.info("ìŠ¤í¬ë˜í•‘ ì‹œì‘: ìµœëŒ€ "+ str({max_pages}) +"í˜ì´ì§€ - "+ str({self.stats['start_time'].strftime('%Y-%m-%d %H:%M:%S')}) +"")
        
        # ì¸í„°ëŸ½íŠ¸ í•¸ë“¤ëŸ¬ ì„¤ì •
        self._setup_interrupt_handler()
        
        # ì²˜ë¦¬ëœ ì œëª© ëª©ë¡ ë¡œë“œ
        self.load_processed_titles(output_base)
        
        announcement_count = 0
        processed_count = 0
        early_stop = False
        stop_reason = ""
        
        try:
            for page_num in range(1, max_pages + 1):
                if self._interrupted:
                    logger.info("ì‚¬ìš©ìì— ì˜í•´ ìŠ¤í¬ë˜í•‘ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
                    early_stop = True
                    stop_reason = "ì‚¬ìš©ì ì¤‘ë‹¨"
                    break
                
                logger.info("í˜ì´ì§€ "+ str({page_num}) +" ì²˜ë¦¬ ì¤‘")
                
                try:
                    # ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ë° íŒŒì‹±
                    announcements = self._get_page_announcements(page_num)
                
                    if not announcements:
                        logger.warning("í˜ì´ì§€ "+ str({page_num}) +"ì— ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤")
                        if page_num == 1:
                            logger.error("ì²« í˜ì´ì§€ì— ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´íŠ¸ êµ¬ì¡°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
                            stop_reason = "ì²« í˜ì´ì§€ ê³µê³  ì—†ìŒ"
                        else:
                            logger.info("ë§ˆì§€ë§‰ í˜ì´ì§€ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
                            stop_reason = "ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬"
                        break
                    
                    logger.info("í˜ì´ì§€ "+ str({page_num}) +"ì—ì„œ "+ str({len(announcements)}) +"ê°œ ê³µê³  ë°œê²¬")
                    
                    # ìƒˆë¡œìš´ ê³µê³ ë§Œ í•„í„°ë§ ë° ì¤‘ë³µ ì„ê³„ê°’ ì²´í¬
                    new_announcements, should_stop = self.filter_new_announcements(announcements)
                    
                    # ê° ê³µê³  ì²˜ë¦¬
                    for ann in new_announcements:
                        announcement_count += 1
                        processed_count += 1
                        self.process_announcement(ann, announcement_count, output_base)

                    # ì¤‘ë³µ ì„ê³„ê°’ ë„ë‹¬ì‹œ ì¡°ê¸° ì¢…ë£Œ
                    if should_stop:
                        logger.info("ì¤‘ë³µ ê³µê³  "+ str({self.duplicate_threshold}) +"ê°œ ì—°ì† ë°œê²¬ìœ¼ë¡œ ì¡°ê¸° ì¢…ë£Œ")
                        early_stop = True
                        stop_reason = "ì¤‘ë³µ "+ str({self.duplicate_threshold}) +"ê°œ ì—°ì†"
                        break
                    
                    # ìƒˆë¡œìš´ ê³µê³ ê°€ ì—†ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ (ì—°ì†ëœ í˜ì´ì§€ì—ì„œ)
                    if not new_announcements and page_num > 1:
                        logger.info("ìƒˆë¡œìš´ ê³µê³ ê°€ ì—†ì–´ ìŠ¤í¬ë˜í•‘ ì¡°ê¸° ì¢…ë£Œ")
                        early_stop = True
                        stop_reason = "ìƒˆë¡œìš´ ê³µê³  ì—†ìŒ"
                        break
                    
                    # í˜ì´ì§€ ê°„ ëŒ€ê¸°
                    if page_num < max_pages and self.delay_between_pages > 0:
                        time.sleep(self.delay_between_pages)
                    
                except Exception as e:
                    logger.error("í˜ì´ì§€ "+ str({page_num}) +" ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: "+ str({e}) +"")
                    stop_reason = "ì˜¤ë¥˜: "+ str({e}) +""
                    break
        
        except Exception as e:
            logger.error("ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: "+ str({e}) +"")
            early_stop = True
            stop_reason = "ì˜¤ë¥˜: "+ str({e}) +""
        finally:
            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ
            self.stats['end_time'] = datetime.now()
            
            # ì²˜ë¦¬ëœ ì œëª© ëª©ë¡ ì €ì¥
            self.save_processed_titles()
            
            # ìµœì¢… í†µê³„ ì¶œë ¥
            self._print_final_stats(processed_count, early_stop, stop_reason)
        
        return True
    
    def _get_page_announcements(self, page_num: int) -> List[Dict[str, Any]]:
        """í˜ì´ì§€ë³„ ê³µê³  ëª©ë¡ ê°€ì ¸ì˜¤ê¸° - ê¸°ë³¸ êµ¬í˜„"""
        page_url = self.get_list_url(page_num)
        response = self.get_page(page_url)
        
        if not response:
            logger.warning("í˜ì´ì§€ "+ str({page_num}) +" ì‘ë‹µì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return []
        
        # í˜ì´ì§€ê°€ ì—ëŸ¬ ìƒíƒœê±°ë‚˜ ì˜ëª»ëœ ê²½ìš° ê°ì§€
        if response.status_code >= 400:
            logger.warning("í˜ì´ì§€ "+ str({page_num}) +" HTTP ì—ëŸ¬: "+ str({response.status_code}) +"")
            return []
        
        # í˜„ì¬ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ë¡œ ì €ì¥
        self.current_page_num = page_num
        announcements = self.parse_list_page(response.text)
        
        # ì¶”ê°€ ë§ˆì§€ë§‰ í˜ì´ì§€ ê°ì§€ ë¡œì§
        if not announcements and page_num > 1:
            logger.info("í˜ì´ì§€ "+ str({page_num}) +"ì— ê³µê³ ê°€ ì—†ì–´ ë§ˆì§€ë§‰ í˜ì´ì§€ë¡œ íŒë‹¨ë©ë‹ˆë‹¤")
        
        return announcements
    
    def _setup_interrupt_handler(self):
        """Ctrl+C ì¸í„°ëŸ½íŠ¸ í•¸ë“¤ëŸ¬ ì„¤ì •"""
        def signal_handler(signum, frame):
            logger.info("ì¤‘ë‹¨ ì‹ í˜¸ë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤. ì•ˆì „í•˜ê²Œ ì¢…ë£Œë©ë‹ˆë‹¤...")
            self._interrupted = True
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
    
    def _print_final_stats(self, processed_count: int, early_stop: bool, stop_reason: str):
        """ìµœì¢… í†µê³„ ì¶œë ¥"""
        if not self.stats['start_time'] or not self.stats['end_time']:
            return
        
        duration = self.stats['end_time'] - self.stats['start_time']
        duration_seconds = duration.total_seconds()
        
        logger.info("="*60)
        logger.info("ğŸ“Š ìŠ¤í¬ë˜í•‘ ì™„ë£Œ í†µê³„")
        logger.info("="*60)
        
        if early_stop:
            logger.info("â±ï¸  ì‹¤í–‰ ê¸°ê°„: "+ str({duration_seconds:.1f}) +"ì´ˆ (ì¡°ê¸°ì¢…ë£Œ: "+ str({stop_reason}) +")")
        else:
            logger.info("â±ï¸  ì‹¤í–‰ ì‹œê°„: "+ str({duration_seconds:.1f}) +"ì´ˆ")
        
        logger.info("ğŸ“„ ì²˜ë¦¬ëœ ê³µê³ : "+ str({processed_count}) +"ê°œ")
        logger.info("ğŸŒ HTTP ìš”ì²­: "+ str({self.stats['requests_made']}) +"ê°œ")
        logger.info("ğŸ“ ë‹¤ìš´ë¡œë“œ íŒŒì¼: "+ str({self.stats['files_downloaded']}) +"ê°œ")
        logger.info("ğŸ’¾ ì „ì²´ ë‹¤ìš´ë¡œë“œ í¬ê¸°: "+ str({self._format_size(self.stats['total_download_size'])}) +"")
        
        if self.stats['errors_encountered'] > 0:
            logger.warning("âš ï¸  ë°œìƒí•œ ì˜¤ë¥˜: "+ str({self.stats['errors_encountered']}) +"ê°œ")
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        if duration_seconds > 0:
            requests_per_second = self.stats['requests_made'] / duration_seconds
            logger.info("ğŸš€ ì´ˆë‹¹ ìš”ì²­ ìˆ˜: "+ str({requests_per_second:.2f}) +"")
        
        logger.info("="*60)
    
    def _format_size(self, size_bytes: int) -> str:
        """ë°”ì´íŠ¸ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜"""
        if size_bytes == 0:
            return "0 B"
        
        size_names = ["B", "KB", "MB", "GB"]
        i = 0
        while size_bytes >= 1024 and i < len(size_names) - 1:
            size_bytes /= 1024.0
            i += 1
        
        return ""+ str({size_bytes:.1f}) +" "+ str({size_names[i]}) +""
    
    def get_stats(self) -> Dict[str, Any]:
        """í˜„ì¬ í†µê³„ ë°˜í™˜"""
        stats = self.stats.copy()
        if stats['start_time'] and stats['end_time']:
            duration = stats['end_time'] - stats['start_time']
            stats['duration_seconds'] = duration.total_seconds()
        return stats
    
    @contextmanager
    def performance_monitor(self, operation_name: str):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        start_time = time.time()
        logger.debug("ğŸ”„ "+ str({operation_name}) +" ì‹œì‘")
        
        try:
            yield
        finally:
            end_time = time.time()
            duration = end_time - start_time
            logger.debug("âœ… "+ str({operation_name}) +" ì™„ë£Œ - "+ str({duration:.2f}) +"ì´ˆ")
    
    def is_healthy(self) -> bool:
        """ìŠ¤í¬ë˜í¼ ìƒíƒœ ì²´í¬"""
        try:
            # ê¸°ë³¸ URL ì—°ê²° í…ŒìŠ¤íŠ¸
            if self.base_url:
                response = self.session.head(self.base_url, timeout=10, verify=self.verify_ssl)
                return response.status_code < 400
            return True
        except:
            return False
    
    def reset_stats(self):
        """í†µê³„ ë¦¬ì…‹"""
        self.stats = {
            'requests_made': 0,
            'files_downloaded': 0,
            'errors_encountered': 0,
            'total_download_size': 0,
            'start_time': None,
            'end_time': None
        }
    
    def process_notice_detection(self, cell, row_index: int = 0, use_playwright: bool = False) -> str:
        """ê³µì§€ ì´ë¯¸ì§€ ê°ì§€ ë° ë²ˆí˜¸ ì²˜ë¦¬ - ëª¨ë“  CCIì—ì„œ ì¬ì‚¬ìš© ê°€ëŠ¥"""
        if use_playwright:
            # Playwright ë²„ì „
            number = cell.inner_text().strip() if hasattr(cell, 'inner_text') else ""
            is_notice = False
            
            if hasattr(cell, 'locator'):
                notice_imgs = cell.locator('img').all()
                for img in notice_imgs:
                    src = img.get_attribute('src') or ''
                    alt = img.get_attribute('alt') or ''
                    if 'ê³µì§€' in src or 'ê³µì§€' in alt or 'notice' in src.lower():
                        is_notice = True
                        break
        else:
            # BeautifulSoup ë²„ì „
            number = cell.get_text(strip=True) if hasattr(cell, 'get_text') else str(cell).strip()
            is_notice = False
            
            if hasattr(cell, 'find_all'):
                notice_imgs = cell.find_all('img')
                for img in notice_imgs:
                    src = img.get('src', '')
                    alt = img.get('alt', '')
                    if 'ê³µì§€' in src or 'ê³µì§€' in alt or 'notice' in src.lower():
                        is_notice = True
                        break
        
        # ë²ˆí˜¸ ê²°ì •
        if is_notice:
            return "ê³µì§€"
        elif not number or number.isspace():
            return "row_"+ str({row_index}) +""
        else:
            return number.strip()


class StandardTableScraper(EnhancedBaseScraper):
    """í‘œì¤€ HTML í…Œì´ë¸” ê¸°ë°˜ ê²Œì‹œíŒìš© ìŠ¤í¬ë˜í¼"""
    
    def get_list_url(self, page_num):
        """í‘œì¤€ í˜ì´ì§€ë„¤ì´ì…˜ URL ìƒì„±"""
        if not self.config:
            # í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ ì§ì ‘ êµ¬í˜„
            return super().get_list_url(page_num)
        
        pagination = self.config.pagination
        if pagination.get('type') == 'query_param':
            param = pagination.get('param', 'page')
            if page_num == 1:
                return self.list_url
            else:
                separator = '&' if '?' in self.list_url else '?'
                return ""+ str({self.list_url}) +""+ str({separator}) +""+ str({param}) +"="+ str({page_num}) +""
        
        return self.list_url
    
    def parse_list_page(self, html_content: str) -> List[Dict[str, Any]]:
        """í‘œì¤€ í…Œì´ë¸” íŒŒì‹± - ì„¤ì • ê¸°ë°˜"""
        if not self.config or not self.config.selectors:
            # í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ ì§ì ‘ êµ¬í˜„
            return super().parse_list_page(html_content)
        
        soup = BeautifulSoup(html_content, 'html.parser')
        announcements = []
        
        selectors = self.config.selectors
        
        # í…Œì´ë¸” ì°¾ê¸°
        table = soup.select_one(selectors.get('table', 'table'))
        if not table:
            return announcements
        
        # í–‰ë“¤ ì°¾ê¸°
        rows = table.select(selectors.get('rows', 'tr'))
        
        for row in rows:
            try:
                # ì œëª© ë§í¬ ì°¾ê¸°
                link_elem = row.select_one(selectors.get('title_link', 'a[href]'))
                if not link_elem:
                    continue
                
                title = link_elem.get_text(strip=True)
                if not title:
                    continue
                
                # URL êµ¬ì„±
                href = link_elem.get('href', '')
                detail_url = urljoin(self.base_url, href)
                
                announcement = {
                    'title': title,
                    'url': detail_url
                }
                
                # ì¶”ê°€ í•„ë“œë“¤ (ì„ íƒì )
                field_selectors = {
                    'status': 'status',
                    'writer': 'writer', 
                    'date': 'date',
                    'period': 'period'
                }
                
                for field, selector_key in field_selectors.items():
                    if selector_key in selectors:
                        elem = row.select_one(selectors[selector_key])
                        if elem:
                            announcement[field] = elem.get_text(strip=True)
                
                announcements.append(announcement)
                
            except Exception as e:
                logger.error("í–‰ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: "+ str({e}) +"")
                continue
        
        return announcements


class AjaxAPIScraper(EnhancedBaseScraper):
    """AJAX/JSON API ê¸°ë°˜ ìŠ¤í¬ë˜í¼"""
    
    def get_list_url(self, page_num):
        """API URL ë°˜í™˜"""
        return getattr(self.config, 'api_url', self.list_url)
    
    def _get_page_announcements(self, page_num: int) -> List[Dict[str, Any]]:
        """APIë¥¼ í†µí•œ ê³µê³  ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        if not self.config or not self.config.api_config:
            return super()._get_page_announcements(page_num)
        
        api_config = self.config.api_config
        api_url = getattr(self.config, 'api_url', self.list_url)
        
        # ìš”ì²­ ë°ì´í„° êµ¬ì„±
        data = api_config.get('data_fields', "+ str({}) +").copy()
        
        # í˜ì´ì§€ ë²ˆí˜¸ ì¶”ê°€
        pagination = self.config.pagination
        if pagination.get('type') == 'post_data':
            param = pagination.get('param', 'page')
            data[param] = str(page_num)
        
        # API í˜¸ì¶œ
        if api_config.get('method', 'POST').upper() == 'POST':
            response = self.post_page(api_url, data=data)
        else:
            response = self.get_page(api_url, params=data)
        
        if not response:
            return []
        
        try:
            json_data = response.json()
            return self.parse_api_response(json_data, page_num)
        except json.JSONDecodeError as e:
            logger.error("JSON íŒŒì‹± ì‹¤íŒ¨: "+ str({e}) +"")
            return []
    
    def parse_api_response(self, json_data: Dict[str, Any], page_num: int) -> List[Dict[str, Any]]:
        """API ì‘ë‹µ íŒŒì‹± - í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„"""
        return self.parse_list_page(json_data)


class JavaScriptScraper(EnhancedBaseScraper):
    """JavaScript ì‹¤í–‰ì´ í•„ìš”í•œ ì‚¬ì´íŠ¸ìš© ìŠ¤í¬ë˜í¼"""
    
    def __init__(self):
        super().__init__()
        # Playwrightë‚˜ Selenium ë“±ì„ ìœ„í•œ ì„¤ì •
        self.browser_options = {
            'headless': True,
            'timeout': 30000
        }
    
    def extract_js_data(self, html_content: str, pattern: str) -> List[str]:
        """JavaScriptì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
        matches = re.findall(pattern, html_content, re.DOTALL)
        return matches


class SessionBasedScraper(EnhancedBaseScraper):
    """ì„¸ì…˜ ê´€ë¦¬ê°€ í•„ìš”í•œ ì‚¬ì´íŠ¸ìš© ìŠ¤í¬ë˜í¼"""
    
    def __init__(self):
        super().__init__()
        self.session_initialized = False
        self.session_data = "+ str({}) +"
    
    def initialize_session(self):
        """ì„¸ì…˜ ì´ˆê¸°í™” - í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„"""
        if self.session_initialized:
            return True
        
        # ê¸°ë³¸ì ìœ¼ë¡œ ì²« í˜ì´ì§€ ë°©ë¬¸ìœ¼ë¡œ ì„¸ì…˜ ì´ˆê¸°í™”
        try:
            response = self.get_page(self.base_url or self.list_url)
            if response:
                self.session_initialized = True
                return True
        except Exception as e:
            logger.error("ì„¸ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: "+ str({e}) +"")
        
        return False
    
    def _get_page_announcements(self, page_num: int) -> List[Dict[str, Any]]:
        """ì„¸ì…˜ í™•ì¸ í›„ ê³µê³  ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        if not self.initialize_session():
            logger.error("ì„¸ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨")
            return []
        
        return super()._get_page_announcements(page_num)


class PlaywrightScraper(EnhancedBaseScraper):
    """Playwright ë¸Œë¼ìš°ì € ìë™í™” ê¸°ë°˜ ìŠ¤í¬ë˜í¼"""
    
    def __init__(self):
        super().__init__()
        self.browser = None
        self.page = None
        self.browser_options = {
            'headless': True,
            'timeout': 30000
        }
    
    async def initialize_browser(self):
        """ë¸Œë¼ìš°ì € ì´ˆê¸°í™” - í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ Playwright êµ¬í˜„"""
        # ì‹¤ì œ Playwright êµ¬í˜„ì€ í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ
        pass
    
    async def cleanup_browser(self):
        """ë¸Œë¼ìš°ì € ì •ë¦¬"""
        if self.page:
            await self.page.close()
        if self.browser:
            await self.browser.close()