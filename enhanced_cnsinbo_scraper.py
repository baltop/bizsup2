#!/usr/bin/env python3
"""
Enhanced CNSINBO Scraper
ì¶©ë‚¨ì‹ ìš©ë³´ì¦ì¬ë‹¨ (CNSINBO) ê³µê³  ìˆ˜ì§‘ ìŠ¤í¬ë˜í¼
"""

import requests
from bs4 import BeautifulSoup
import os
import time
import urllib.parse
import re
from urllib.parse import urljoin
import html2text
import hashlib
from pathlib import Path
import json

class CNSINBOScraper:
    def __init__(self, base_url, site_code, output_dir="output"):
        self.base_url = base_url
        self.site_code = site_code
        self.output_dir = os.path.join(output_dir, site_code)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        Path(self.output_dir).mkdir(parents=True, exist_ok=True)
        
        # HTML to markdown ë³€í™˜ê¸°
        self.h2t = html2text.HTML2Text()
        self.h2t.ignore_links = False
        self.h2t.ignore_images = False
        self.h2t.body_width = 0
        
        # ë‹¤ìš´ë¡œë“œ í†µê³„
        self.stats = {
            'total_posts': 0,
            'attachments_downloaded': 0,
            'pages_processed': 0,
            'errors': []
        }
    
    def clean_filename(self, filename):
        """íŒŒì¼ëª…ì„ ì•ˆì „í•˜ê²Œ ì •ë¦¬"""
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ê¸¸ì´ ì œí•œ
        cleaned = re.sub(r'[<>:"/\\|?*]', '_', filename)
        cleaned = cleaned.strip()
        
        # íŒŒì¼ëª…ì´ ë„ˆë¬´ ê¸¸ë©´ ì¤„ì„
        if len(cleaned) > 200:
            name, ext = os.path.splitext(cleaned)
            cleaned = name[:200-len(ext)] + ext
        
        return cleaned
    
    def download_file(self, url, filepath):
        """íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        try:
            response = self.session.get(url, stream=True, timeout=30)
            response.raise_for_status()
            
            # Content-Type í™•ì¸
            content_type = response.headers.get('content-type', '').lower()
            
            # HTML ì‘ë‹µì¸ ê²½ìš° (ì˜¤ë¥˜ í˜ì´ì§€) ê°ì§€
            if 'text/html' in content_type:
                print(f"    ê²½ê³ : HTML ì‘ë‹µ ê°ì§€ - {url}")
                return 0
            
            total_size = 0
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        total_size += len(chunk)
            
            file_size = os.path.getsize(filepath)
            
            # íŒŒì¼ì´ ë„ˆë¬´ ì‘ì€ ê²½ìš° (ì˜¤ë¥˜ í˜ì´ì§€ì¼ ê°€ëŠ¥ì„±)
            if file_size < 1024:
                print(f"    ê²½ê³ : íŒŒì¼ í¬ê¸° ë„ˆë¬´ ì‘ìŒ ({file_size} bytes) - {url}")
                # íŒŒì¼ ë‚´ìš© í™•ì¸
                with open(filepath, 'rb') as f:
                    content = f.read(512)
                    if b'<html' in content.lower() or b'<!doctype' in content.lower():
                        print(f"    ì˜¤ë¥˜: HTML í˜ì´ì§€ ë‹¤ìš´ë¡œë“œë¨")
                        os.remove(filepath)
                        return 0
            
            return file_size
        except Exception as e:
            self.stats['errors'].append(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ {url}: {str(e)}")
            return 0
    
    def extract_content(self, soup):
        """ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ"""
        # CNSINBO ì‚¬ì´íŠ¸ì˜ ë³¸ë¬¸ êµ¬ì¡° ë¶„ì„ ê²°ê³¼
        content_div = soup.find('div', class_='viewBox')
        if not content_div:
            # ëŒ€ì•ˆì  ë°©ë²•
            content_div = soup.find('article', class_='board-text')
            if content_div:
                content_div = content_div.find('div', class_='viewBox')
        
        if not content_div:
            return ""
        
        # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
        for element in content_div.find_all(['script', 'style']):
            element.decompose()
        
        return str(content_div)
    
    def extract_attachments(self, soup):
        """ì²¨ë¶€íŒŒì¼ ì •ë³´ ì¶”ì¶œ"""
        attachments = []
        
        # CNSINBO ì‚¬ì´íŠ¸ì˜ ì²¨ë¶€íŒŒì¼ êµ¬ì¡°
        file_section = soup.find('div', class_='fieldBox')
        if file_section:
            for link in file_section.find_all('a', href=True):
                href = link.get('href')
                if href and 'fileDown.do' in href:
                    filename = link.text.strip()
                    if filename:
                        attachments.append({
                            'url': urljoin(self.base_url, href),
                            'filename': filename,
                            'size': "Unknown"
                        })
        
        return attachments
    
    def extract_post_info_from_list(self, soup):
        """ëª©ë¡ í˜ì´ì§€ì—ì„œ ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ"""
        posts = []
        
        # CNSINBO ì‚¬ì´íŠ¸ì˜ ê²Œì‹œê¸€ ëª©ë¡ êµ¬ì¡°
        tbody = soup.find('tbody')
        if tbody:
            for tr in tbody.find_all('tr'):
                # JavaScript onclick íŒ¨í„´ ì°¾ê¸°
                link_elem = tr.find('a', onclick=True)
                if link_elem:
                    onclick = link_elem.get('onclick', '')
                    # goView('134','33299', '0', 'null', 'W', '1', 'N', '') íŒ¨í„´ íŒŒì‹±
                    match = re.search(r"goView\('(\d+)','(\d+)'", onclick)
                    if match:
                        board_id = match.group(1)
                        board_seq = match.group(2)
                        title = link_elem.text.strip()
                        
                        # ìƒì„¸ í˜ì´ì§€ URL êµ¬ì„±
                        post_url = f"https://www.cnsinbo.co.kr/boardCnts/view.do?boardID={board_id}&boardSeq={board_seq}&lev=0&searchType=null&statusYN=W&page=1&s=cnsinbo"
                        
                        posts.append({
                            'id': board_seq,
                            'title': title,
                            'url': post_url,
                            'board_id': board_id
                        })
        
        return posts
    
    def scrape_post_detail(self, post_url, post_id):
        """ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘"""
        try:
            print(f"  ìƒì„¸ í˜ì´ì§€ ì ‘ì†: {post_url}")
            response = self.session.get(post_url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ
            title_elem = soup.find('h1', class_='tit')
            title = "ì œëª© ì—†ìŒ"
            if title_elem:
                title = title_elem.text.strip()
            
            # ë©”íƒ€ ì •ë³´ ì¶”ì¶œ
            meta_info = {}
            info_box = soup.find('ul', class_='infoBox')
            if info_box:
                for li in info_box.find_all('li'):
                    text = li.text.strip()
                    if 'ì‘ì„±ì' in text:
                        meta_info['author'] = text.replace('ì‘ì„±ì', '').strip()
                    elif 'ì¡°íšŒìˆ˜' in text:
                        meta_info['views'] = text.replace('ì¡°íšŒìˆ˜', '').strip()
                    elif 'ì‘ì„±ì¼' in text:
                        meta_info['date'] = text.replace('ì‘ì„±ì¼', '').strip()
            
            # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
            content_html = self.extract_content(soup)
            content_md = self.h2t.handle(content_html)
            
            # ì²¨ë¶€íŒŒì¼ ì •ë³´ ì¶”ì¶œ
            attachments = self.extract_attachments(soup)
            
            # ê²Œì‹œê¸€ë³„ ë””ë ‰í† ë¦¬ ìƒì„±
            safe_title = self.clean_filename(title)
            post_dir = os.path.join(self.output_dir, f"{post_id}_{safe_title}")
            os.makedirs(post_dir, exist_ok=True)
            
            # ì²¨ë¶€íŒŒì¼ ë””ë ‰í† ë¦¬ ìƒì„±
            attachments_dir = os.path.join(post_dir, "attachments")
            if attachments:
                os.makedirs(attachments_dir, exist_ok=True)
            
            # ë³¸ë¬¸ì„ content.md íŒŒì¼ë¡œ ì €ì¥
            md_filepath = os.path.join(post_dir, "content.md")
            
            with open(md_filepath, 'w', encoding='utf-8') as f:
                f.write(f"# {title}\n\n")
                f.write(f"**ì‘ì„±ì:** {meta_info.get('author', 'N/A')}\n")
                f.write(f"**ë“±ë¡ì¼:** {meta_info.get('date', 'N/A')}\n")
                f.write(f"**ì¡°íšŒìˆ˜:** {meta_info.get('views', 'N/A')}\n")
                f.write(f"**URL:** {post_url}\n\n")
                f.write("## ë³¸ë¬¸\n\n")
                f.write(content_md)
                
                if attachments:
                    f.write("\n## ì²¨ë¶€íŒŒì¼\n\n")
                    for att in attachments:
                        f.write(f"- {att['filename']}\n")
            
            print(f"  ë³¸ë¬¸ ì €ì¥: {md_filepath}")
            
            # ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            for i, attachment in enumerate(attachments):
                try:
                    filename = self.clean_filename(attachment['filename'])
                    file_path = os.path.join(attachments_dir, filename)
                    
                    print(f"  ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ: {filename}")
                    file_size = self.download_file(attachment['url'], file_path)
                    
                    if file_size > 0:
                        self.stats['attachments_downloaded'] += 1
                        print(f"    ì™„ë£Œ: {filename} ({file_size} bytes)")
                    else:
                        print(f"    ì‹¤íŒ¨: {filename}")
                        
                except Exception as e:
                    self.stats['errors'].append(f"ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ {attachment['filename']}: {str(e)}")
                    print(f"    ì˜¤ë¥˜: {str(e)}")
            
            return {
                'title': title,
                'author': meta_info.get('author', 'N/A'),
                'date': meta_info.get('date', 'N/A'),
                'views': meta_info.get('views', 'N/A'),
                'attachments': len(attachments),
                'post_dir': post_dir
            }
            
        except Exception as e:
            self.stats['errors'].append(f"ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ {post_url}: {str(e)}")
            print(f"    ì˜¤ë¥˜: {str(e)}")
            return None
    
    def scrape_page(self, page_num):
        """í˜ì´ì§€ë³„ ê²Œì‹œê¸€ ëª©ë¡ ìŠ¤í¬ë˜í•‘"""
        try:
            if page_num == 1:
                url = self.base_url
            else:
                url = f"{self.base_url}&page={page_num}"
                
            print(f"\ní˜ì´ì§€ {page_num} ìŠ¤í¬ë˜í•‘ ì‹œì‘: {url}")
            
            response = self.session.get(url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ
            posts = self.extract_post_info_from_list(soup)
            
            print(f"í˜ì´ì§€ {page_num}ì—ì„œ {len(posts)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")
            
            # ê° ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘
            for post in posts:
                print(f"\n[{post['id']}] {post['title']}")
                result = self.scrape_post_detail(post['url'], post['id'])
                if result:
                    self.stats['total_posts'] += 1
                
                # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                time.sleep(1)
            
            self.stats['pages_processed'] += 1
            return len(posts)
            
        except Exception as e:
            self.stats['errors'].append(f"í˜ì´ì§€ {page_num} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {str(e)}")
            print(f"í˜ì´ì§€ {page_num} ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {str(e)}")
            return 0
    
    def run(self, max_pages=3):
        """ìŠ¤í¬ë˜í•‘ ì‹¤í–‰"""
        print(f"CNSINBO ìŠ¤í¬ë˜í•‘ ì‹œì‘ - ìµœëŒ€ {max_pages}í˜ì´ì§€")
        print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.output_dir}")
        print(f"ì‚¬ì´íŠ¸ ì½”ë“œ: {self.site_code}")
        
        start_time = time.time()
        
        for page in range(1, max_pages + 1):
            posts_count = self.scrape_page(page)
            
            if posts_count == 0:
                print(f"í˜ì´ì§€ {page}ì—ì„œ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            print(f"í˜ì´ì§€ {page} ì™„ë£Œ - {posts_count}ê°œ ê²Œì‹œê¸€ ì²˜ë¦¬")
            
            # í˜ì´ì§€ ê°„ ê°„ê²©
            if page < max_pages:
                time.sleep(2)
        
        end_time = time.time()
        
        # í†µê³„ ì¶œë ¥
        print(f"\n{'='*50}")
        print("ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
        print(f"{'='*50}")
        print(f"ì²˜ë¦¬ëœ í˜ì´ì§€: {self.stats['pages_processed']}")
        print(f"ìˆ˜ì§‘ëœ ê²Œì‹œê¸€: {self.stats['total_posts']}")
        print(f"ë‹¤ìš´ë¡œë“œëœ ì²¨ë¶€íŒŒì¼: {self.stats['attachments_downloaded']}")
        print(f"ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
        
        if self.stats['errors']:
            print(f"\nì˜¤ë¥˜ {len(self.stats['errors'])}ê°œ:")
            for error in self.stats['errors']:
                print(f"  - {error}")
        
        # í†µê³„ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        stats_file = os.path.join(self.output_dir, f"{self.site_code}_stats.json")
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, ensure_ascii=False, indent=2)
        
        return self.stats

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    base_url = "https://www.cnsinbo.co.kr/boardCnts/list.do?boardID=134&m=030101&s=cnsinbo"
    site_code = "cnsinbo"
    
    scraper = CNSINBOScraper(base_url, site_code)
    stats = scraper.run(max_pages=3)
    
    # íŒŒì¼ í¬ê¸° ê²€ì¦
    print(f"\n{'='*50}")
    print("íŒŒì¼ í¬ê¸° ê²€ì¦")
    print(f"{'='*50}")
    
    attachment_sizes = {}
    for root, dirs, files in os.walk(scraper.output_dir):
        for file in files:
            if not file.endswith('.md') and not file.endswith('.json'):
                filepath = os.path.join(root, file)
                size = os.path.getsize(filepath)
                
                if size in attachment_sizes:
                    attachment_sizes[size].append(filepath)
                else:
                    attachment_sizes[size] = [filepath]
    
    # ê°™ì€ í¬ê¸°ì˜ íŒŒì¼ë“¤ ì°¾ê¸°
    duplicate_sizes = {size: files for size, files in attachment_sizes.items() if len(files) > 1}
    
    if duplicate_sizes:
        print("âš ï¸  ê°™ì€ í¬ê¸°ì˜ íŒŒì¼ë“¤ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤ (ì˜¤ë¥˜ ê°€ëŠ¥ì„±):")
        for size, files in duplicate_sizes.items():
            print(f"  í¬ê¸° {size} bytes:")
            for file in files:
                print(f"    - {file}")
    else:
        print("âœ… ëª¨ë“  ì²¨ë¶€íŒŒì¼ì´ ì„œë¡œ ë‹¤ë¥¸ í¬ê¸°ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.")
    
    # í•œê¸€ íŒŒì¼ëª… ê²€ì¦
    print(f"\n{'='*50}")
    print("í•œê¸€ íŒŒì¼ëª… ê²€ì¦")
    print(f"{'='*50}")
    
    korean_files = []
    for root, dirs, files in os.walk(scraper.output_dir):
        for file in files:
            if re.search(r'[ê°€-í£]', file):
                korean_files.append(os.path.join(root, file))
    
    if korean_files:
        print(f"âœ… í•œê¸€ íŒŒì¼ëª… {len(korean_files)}ê°œê°€ ì •ìƒì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤:")
        for file in korean_files[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
            print(f"  - {os.path.basename(file)}")
        if len(korean_files) > 5:
            print(f"  ... ì´ {len(korean_files)}ê°œ")
    else:
        print("í•œê¸€ íŒŒì¼ëª…ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # ì™„ë£Œ ì‚¬ìš´ë“œ ì¬ìƒ
    try:
        import subprocess
        subprocess.run(["mpg123", "./ding.mp3"], capture_output=True)
        print("\nğŸ”” ì™„ë£Œ ì‚¬ìš´ë“œ ì¬ìƒ")
    except Exception as e:
        print(f"\nâš ï¸  ì™„ë£Œ ì‚¬ìš´ë“œ ì¬ìƒ ì‹¤íŒ¨: {str(e)}")

if __name__ == "__main__":
    main()