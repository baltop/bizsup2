# -*- coding: utf-8 -*-
"""
Enhanced Async Base Scraper - ë¹„ë™ê¸° ì²˜ë¦¬ ì§€ì› ë² ì´ìŠ¤ í´ë˜ìŠ¤
"""

import asyncio
import aiohttp
import aiofiles
import os
import time
import logging
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from contextlib import asynccontextmanager
import re
import json
from urllib.parse import urljoin, urlparse, unquote
from bs4 import BeautifulSoup
import chardet
import hashlib
import signal
import threading

logger = logging.getLogger(__name__)

class EnhancedAsyncBaseScraper(ABC):
    """í–¥ìƒëœ ë¹„ë™ê¸° ë² ì´ìŠ¤ ìŠ¤í¬ë˜í¼"""
    
    def __init__(self):
        # ê¸°ë³¸ ì„¤ì •
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # ë¹„ë™ê¸° ì„¸ì…˜
        self.session: Optional[aiohttp.ClientSession] = None
        
        # ê¸°ë³¸ê°’ë“¤
        self.verify_ssl = True
        self.default_encoding = 'auto'
        self.timeout = 30
        self.delay_between_requests = 1
        self.delay_between_pages = 2
        
        # ì¬ì‹œë„ ì„¤ì •
        self.max_retries = 3
        self.retry_delay = 2
        
        # ë¹„ë™ê¸° ì„¤ì •
        self.max_concurrent_requests = 5
        self.max_concurrent_downloads = 3
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.stats = {
            'requests_made': 0,
            'files_downloaded': 0,
            'errors_encountered': 0,
            'total_download_size': 0,
            'start_time': None,
            'end_time': None,
            'concurrent_requests': 0,
            'peak_concurrent_requests': 0
        }
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self._lock = asyncio.Lock()
        self._interrupted = False
        
        # ë² ì´ìŠ¤ URLë“¤ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ ì„¤ì •)
        self.base_url = None
        self.list_url = None
        
        # ì¤‘ë³µ ì²´í¬ ê´€ë ¨
        self.processed_titles_file = None
        self.current_page_num = 1
        self.processed_titles = set()
        self.current_session_titles = set()
        self.enable_duplicate_check = True
        self.duplicate_threshold = 3
    
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        await self.initialize_session()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        await self.cleanup_session()
    
    async def initialize_session(self):
        """ë¹„ë™ê¸° ì„¸ì…˜ ì´ˆê¸°í™”"""
        if self.session is None:
            connector = aiohttp.TCPConnector(
                verify_ssl=self.verify_ssl,
                limit=self.max_concurrent_requests,
                limit_per_host=self.max_concurrent_requests
            )
            
            timeout = aiohttp.ClientTimeout(total=self.timeout)
            
            self.session = aiohttp.ClientSession(
                headers=self.headers,
                connector=connector,
                timeout=timeout
            )
    
    async def cleanup_session(self):
        """ë¹„ë™ê¸° ì„¸ì…˜ ì •ë¦¬"""
        if self.session and not self.session.closed:
            await self.session.close()
            self.session = None
    
    @abstractmethod
    async def get_list_url(self, page_num: int) -> str:
        """í˜ì´ì§€ ë²ˆí˜¸ì— ë”°ë¥¸ ëª©ë¡ URL ë°˜í™˜"""
        pass
    
    @abstractmethod
    async def parse_list_page(self, html_content: str) -> List[Dict[str, Any]]:
        """ëª©ë¡ í˜ì´ì§€ íŒŒì‹±"""
        pass
    
    @abstractmethod
    async def parse_detail_page(self, html_content: str) -> Dict[str, Any]:
        """ìƒì„¸ í˜ì´ì§€ íŒŒì‹±"""
        pass
    
    async def get_page(self, url: str, **kwargs) -> Optional[aiohttp.ClientResponse]:
        """ë¹„ë™ê¸° í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°"""
        if not self.session:
            await self.initialize_session()
        
        async with self._lock:
            self.stats['concurrent_requests'] += 1
            if self.stats['concurrent_requests'] > self.stats['peak_concurrent_requests']:
                self.stats['peak_concurrent_requests'] = self.stats['concurrent_requests']
        
        try:
            for attempt in range(self.max_retries + 1):
                try:
                    if self._interrupted:
                        logger.info("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
                        return None
                    
                    async with self._lock:
                        self.stats['requests_made'] += 1
                    
                    async with self.session.get(url, **kwargs) as response:
                        response.raise_for_status()
                        
                        # ì¸ì½”ë”© ì²˜ë¦¬
                        await self._fix_encoding_async(response)
                        
                        return response
                        
                except aiohttp.ClientError as e:
                    attempt_msg = f"ì‹œë„ {attempt + 1}/{self.max_retries + 1}"
                    
                    if attempt < self.max_retries:
                        logger.warning(f"í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨ {url}: {e} - {attempt_msg}, {self.retry_delay}ì´ˆ í›„ ì¬ì‹œë„")
                        await asyncio.sleep(self.retry_delay)
                        continue
                    else:
                        logger.error(f"í˜ì´ì§€ ìš”ì²­ ìµœì¢… ì‹¤íŒ¨ {url}: {e} - {attempt_msg}")
                        async with self._lock:
                            self.stats['errors_encountered'] += 1
                        return None
                        
                except Exception as e:
                    logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ {url}: {e}")
                    async with self._lock:
                        self.stats['errors_encountered'] += 1
                    return None
            
            return None
            
        finally:
            async with self._lock:
                self.stats['concurrent_requests'] -= 1
    
    async def post_page(self, url: str, data: Dict[str, Any] = None, **kwargs) -> Optional[aiohttp.ClientResponse]:
        """ë¹„ë™ê¸° POST ìš”ì²­"""
        if not self.session:
            await self.initialize_session()
        
        async with self._lock:
            self.stats['concurrent_requests'] += 1
            if self.stats['concurrent_requests'] > self.stats['peak_concurrent_requests']:
                self.stats['peak_concurrent_requests'] = self.stats['concurrent_requests']
        
        try:
            for attempt in range(self.max_retries + 1):
                try:
                    if self._interrupted:
                        logger.info("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
                        return None
                    
                    async with self._lock:
                        self.stats['requests_made'] += 1
                    
                    async with self.session.post(url, data=data, **kwargs) as response:
                        response.raise_for_status()
                        await self._fix_encoding_async(response)
                        return response
                        
                except aiohttp.ClientError as e:
                    attempt_msg = f"ì‹œë„ {attempt + 1}/{self.max_retries + 1}"
                    
                    if attempt < self.max_retries:
                        logger.warning(f"POST ìš”ì²­ ì‹¤íŒ¨ {url}: {e} - {attempt_msg}, {self.retry_delay}ì´ˆ í›„ ì¬ì‹œë„")
                        await asyncio.sleep(self.retry_delay)
                        continue
                    else:
                        logger.error(f"POST ìš”ì²­ ìµœì¢… ì‹¤íŒ¨ {url}: {e} - {attempt_msg}")
                        async with self._lock:
                            self.stats['errors_encountered'] += 1
                        return None
                        
                except Exception as e:
                    logger.error(f"POST ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ {url}: {e}")
                    async with self._lock:
                        self.stats['errors_encountered'] += 1
                    return None
            
            return None
            
        finally:
            async with self._lock:
                self.stats['concurrent_requests'] -= 1
    
    async def _fix_encoding_async(self, response: aiohttp.ClientResponse):
        """ë¹„ë™ê¸° ì‘ë‹µ ì¸ì½”ë”© ìë™ ìˆ˜ì •"""
        # aiohttpëŠ” ìë™ìœ¼ë¡œ ì¸ì½”ë”©ì„ ì²˜ë¦¬í•˜ë¯€ë¡œ ê¸°ë³¸ì ìœ¼ë¡œ ë³„ë„ ì²˜ë¦¬ ë¶ˆí•„ìš”
        # í•„ìš”ì‹œ í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ ì˜¤ë²„ë¼ì´ë“œ
        pass
    
    async def download_file(self, url: str, save_path: str, attachment_info: Dict[str, Any] = None) -> bool:
        """ë¹„ë™ê¸° íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        for attempt in range(self.max_retries + 1):
            try:
                if self._interrupted:
                    logger.info("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
                    return False
                
                logger.info(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {url} (ì‹œë„ {attempt + 1}/{self.max_retries + 1})")
                
                # ë‹¤ìš´ë¡œë“œ í—¤ë” ì„¤ì •
                download_headers = self.headers.copy()
                if self.base_url:
                    download_headers['Referer'] = self.base_url
                
                async with self._lock:
                    self.stats['requests_made'] += 1
                
                async with self.session.get(url, headers=download_headers) as response:
                    response.raise_for_status()
                    
                    # ì‹¤ì œ íŒŒì¼ëª… ì¶”ì¶œ
                    actual_filename = await self._extract_filename_async(response, save_path)
                    if actual_filename != save_path:
                        save_path = actual_filename
                    
                    # ë””ë ‰í† ë¦¬ ìƒì„± ë³´ì¥
                    os.makedirs(os.path.dirname(save_path), exist_ok=True)
                    
                    # ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ë‹¤ìš´ë¡œë“œ
                    total_size = 0
                    
                    async with aiofiles.open(save_path, 'wb') as f:
                        async for chunk in response.content.iter_chunked(8192):
                            if self._interrupted:
                                logger.info("íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘ë‹¨ë¨")
                                return False
                            
                            await f.write(chunk)
                            total_size += len(chunk)
                    
                    file_size = os.path.getsize(save_path)
                    
                    async with self._lock:
                        self.stats['files_downloaded'] += 1
                        self.stats['total_download_size'] += file_size
                    
                    logger.info(f"ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {save_path} ({file_size:,} bytes)")
                    return True
                    
            except aiohttp.ClientError as e:
                attempt_msg = f"ì‹œë„ {attempt + 1}/{self.max_retries + 1}"
                
                if attempt < self.max_retries:
                    logger.warning(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ {url}: {e} - {attempt_msg}, {self.retry_delay}ì´ˆ í›„ ì¬ì‹œë„")
                    await asyncio.sleep(self.retry_delay)
                    continue
                else:
                    logger.error(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ìµœì¢… ì‹¤íŒ¨ {url}: {e} - {attempt_msg}")
                    async with self._lock:
                        self.stats['errors_encountered'] += 1
                    return False
                    
            except Exception as e:
                logger.error(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ {url}: {e}")
                async with self._lock:
                    self.stats['errors_encountered'] += 1
                return False
        
        return False
    
    async def _extract_filename_async(self, response: aiohttp.ClientResponse, default_path: str) -> str:
        """ë¹„ë™ê¸° íŒŒì¼ëª… ì¶”ì¶œ"""
        content_disposition = response.headers.get('Content-Disposition', '')
        
        if not content_disposition:
            # Content-Dispositionì´ ì—†ìœ¼ë©´ URLì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ ì‹œë„
            try:
                parsed_url = urlparse(str(response.url))
                url_filename = os.path.basename(unquote(parsed_url.path))
                if url_filename and '.' in url_filename:
                    save_dir = os.path.dirname(default_path)
                    clean_filename = self.sanitize_filename(url_filename)
                    return os.path.join(save_dir, clean_filename)
            except:
                pass
            return default_path
        
        # RFC 5987 í˜•ì‹ ìš°ì„  ì‹œë„
        rfc5987_match = re.search(r"filename\\*=([^']*)'([^']*)'(.+)", content_disposition)
        if rfc5987_match:
            encoding = rfc5987_match.group(1) or 'utf-8'
            filename = rfc5987_match.group(3)
            try:
                filename = unquote(filename, encoding=encoding)
                save_dir = os.path.dirname(default_path)
                clean_filename = self.sanitize_filename(filename)
                logger.debug(f"RFC5987 íŒŒì¼ëª… ì¶”ì¶œ: {clean_filename}")
                return os.path.join(save_dir, clean_filename)
            except Exception as e:
                logger.debug(f"RFC5987 íŒŒì¼ëª… ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        # ì¼ë°˜ì ì¸ filename íŒŒë¼ë¯¸í„° ì‹œë„
        filename_match = re.search(r'filename[^;=\\n]*=([\\\"]*)(.*?)\\1', content_disposition)
        if filename_match:
            filename = filename_match.group(2)
            
            # ë‹¤ì–‘í•œ ì¸ì½”ë”© ì‹œë„
            encoding_attempts = ['utf-8', 'euc-kr', 'cp949', 'iso-8859-1']
            
            for encoding in encoding_attempts:
                try:
                    if encoding == 'utf-8':
                        decoded = filename.encode('latin-1').decode('utf-8')
                    else:
                        decoded = filename.encode('latin-1').decode(encoding)
                    
                    if decoded and not decoded.isspace() and len(decoded.strip()) > 0:
                        save_dir = os.path.dirname(default_path)
                        clean_filename = self.sanitize_filename(decoded.replace('+', ' ').strip())
                        logger.debug(f"{encoding} ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ëª… ì¶”ì¶œ: {clean_filename}")
                        return os.path.join(save_dir, clean_filename)
                except Exception as e:
                    logger.debug(f"{encoding} ì¸ì½”ë”© ì‹œë„ ì‹¤íŒ¨: {e}")
                    continue
        
        logger.debug(f"íŒŒì¼ëª… ì¶”ì¶œ ì‹¤íŒ¨, ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©: {default_path}")
        return default_path
    
    def sanitize_filename(self, filename: str) -> str:
        """íŒŒì¼ëª… ì •ë¦¬"""
        if not filename or not filename.strip():
            return "unnamed_file"
        
        # URL ë””ì½”ë”©
        try:
            filename = unquote(filename)
        except:
            pass
        
        # ê¸°ë³¸ ì •ë¦¬
        filename = filename.strip()
        
        # Windows/Linux íŒŒì¼ ì‹œìŠ¤í…œ ê¸ˆì§€ ë¬¸ì ì œê±°
        illegal_chars = r'[<>:"/\\\\|?*\\x00-\\x1f]'
        filename = re.sub(illegal_chars, '_', filename)
        
        # ì—°ì†ëœ ê³µë°±/íŠ¹ìˆ˜ë¬¸ìë¥¼ í•˜ë‚˜ë¡œ
        filename = re.sub(r'[\\s_]+', '_', filename)
        
        # ì‹œì‘/ë íŠ¹ìˆ˜ë¬¸ì ì œê±°
        filename = filename.strip('._-')
        
        # ë¹ˆ íŒŒì¼ëª… ì²˜ë¦¬
        if not filename:
            return "unnamed_file"
        
        # íŒŒì¼ëª… ê¸¸ì´ ì œí•œ
        max_length = 200
        if len(filename) > max_length:
            name_parts = filename.rsplit('.', 1)
            if len(name_parts) == 2 and len(name_parts[1]) <= 10:
                name, ext = name_parts
                available_length = max_length - len(ext) - 1
                filename = name[:available_length] + '.' + ext
            else:
                filename = filename[:max_length]
        
        # ì˜ˆì•½ëœ íŒŒì¼ëª… ì²˜ë¦¬ (Windows)
        reserved_names = {'CON', 'PRN', 'AUX', 'NUL', 'COM1', 'COM2', 'COM3', 'COM4', 'COM5', 'COM6', 'COM7', 'COM8', 'COM9', 'LPT1', 'LPT2', 'LPT3', 'LPT4', 'LPT5', 'LPT6', 'LPT7', 'LPT8', 'LPT9'}
        name_without_ext = filename.rsplit('.', 1)[0].upper()
        if name_without_ext in reserved_names:
            filename = '_' + filename
        
        return filename
    
    async def scrape_pages_async(self, max_pages: int = 4, output_base: str = 'output'):
        """ë¹„ë™ê¸° ì—¬ëŸ¬ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘"""
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self.stats['start_time'] = datetime.now()
        logger.info(f"ë¹„ë™ê¸° ìŠ¤í¬ë˜í•‘ ì‹œì‘: ìµœëŒ€ {max_pages}í˜ì´ì§€ - {self.stats['start_time'].strftime('%Y-%m-%d %H:%M:%S')}")
        
        # ì²˜ë¦¬ëœ ì œëª© ëª©ë¡ ë¡œë“œ
        self.load_processed_titles(output_base)
        
        processed_count = 0
        early_stop = False
        stop_reason = ""
        
        try:
            # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì²˜ë¦¬ ì œí•œ
            semaphore = asyncio.Semaphore(self.max_concurrent_requests)
            
            # í˜ì´ì§€ë³„ íƒœìŠ¤í¬ ìƒì„±
            page_tasks = []
            for page_num in range(1, max_pages + 1):
                task = self._process_page_async(semaphore, page_num, output_base)
                page_tasks.append(task)
            
            # ëª¨ë“  í˜ì´ì§€ ë¹„ë™ê¸° ì²˜ë¦¬
            page_results = await asyncio.gather(*page_tasks, return_exceptions=True)
            
            # ê²°ê³¼ ì§‘ê³„
            for i, result in enumerate(page_results):
                if isinstance(result, Exception):
                    logger.error(f"í˜ì´ì§€ {i+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {result}")
                    continue
                
                if result:
                    processed_count += len(result)
            
        except Exception as e:
            logger.error(f"ë¹„ë™ê¸° ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            early_stop = True
            stop_reason = f"ì˜¤ë¥˜: {e}"
        finally:
            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ
            self.stats['end_time'] = datetime.now()
            
            # ì²˜ë¦¬ëœ ì œëª© ëª©ë¡ ì €ì¥
            self.save_processed_titles()
            
            # ìµœì¢… í†µê³„ ì¶œë ¥
            self._print_final_stats_async(processed_count, early_stop, stop_reason)
        
        return True
    
    async def _process_page_async(self, semaphore: asyncio.Semaphore, page_num: int, output_base: str) -> List[Dict[str, Any]]:
        """ë‹¨ì¼ í˜ì´ì§€ ë¹„ë™ê¸° ì²˜ë¦¬"""
        async with semaphore:
            try:
                logger.info(f"í˜ì´ì§€ {page_num} ë¹„ë™ê¸° ì²˜ë¦¬ ì¤‘")
                
                # í˜ì´ì§€ ê³µê³  ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
                announcements = await self._get_page_announcements_async(page_num)
                
                if not announcements:
                    logger.warning(f"í˜ì´ì§€ {page_num}ì— ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤")
                    return []
                
                logger.info(f"í˜ì´ì§€ {page_num}ì—ì„œ {len(announcements)}ê°œ ê³µê³  ë°œê²¬")
                
                # ìƒˆë¡œìš´ ê³µê³ ë§Œ í•„í„°ë§
                new_announcements, should_stop = await self.filter_new_announcements_async(announcements)
                
                if not new_announcements:
                    logger.info(f"í˜ì´ì§€ {page_num}ì— ìƒˆë¡œìš´ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤")
                    return []
                
                # ê³µê³ ë³„ ì²˜ë¦¬ íƒœìŠ¤í¬ ìƒì„±
                announcement_tasks = []
                for i, ann in enumerate(new_announcements):
                    task = self.process_announcement_async(ann, i + 1, output_base)
                    announcement_tasks.append(task)
                
                # ê³µê³ ë“¤ ë³‘ë ¬ ì²˜ë¦¬ (ì œí•œëœ ë™ì‹œì„±)
                announcement_semaphore = asyncio.Semaphore(self.max_concurrent_downloads)
                
                async def process_with_semaphore(ann_task):
                    async with announcement_semaphore:
                        return await ann_task
                
                # ì„¸ë§ˆí¬ì–´ ì ìš©ëœ íƒœìŠ¤í¬ë“¤ ì‹¤í–‰
                bounded_tasks = [process_with_semaphore(task) for task in announcement_tasks]
                await asyncio.gather(*bounded_tasks, return_exceptions=True)
                
                # í˜ì´ì§€ ê°„ ëŒ€ê¸°
                if self.delay_between_pages > 0:
                    await asyncio.sleep(self.delay_between_pages)
                
                return new_announcements
                
            except Exception as e:
                logger.error(f"í˜ì´ì§€ {page_num} ë¹„ë™ê¸° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                return []
    
    async def _get_page_announcements_async(self, page_num: int) -> List[Dict[str, Any]]:
        """í˜ì´ì§€ë³„ ê³µê³  ëª©ë¡ ë¹„ë™ê¸° ê°€ì ¸ì˜¤ê¸°"""
        page_url = await self.get_list_url(page_num)
        response = await self.get_page(page_url)
        
        if not response:
            logger.warning(f"í˜ì´ì§€ {page_num} ì‘ë‹µì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return []
        
        # HTTP ì—ëŸ¬ ì²´í¬
        if response.status >= 400:
            logger.warning(f"í˜ì´ì§€ {page_num} HTTP ì—ëŸ¬: {response.status}")
            return []
        
        # HTML ë‚´ìš© ì½ê¸°
        html_content = await response.text()
        
        # í˜„ì¬ í˜ì´ì§€ ë²ˆí˜¸ ì €ì¥
        self.current_page_num = page_num
        announcements = await self.parse_list_page(html_content)
        
        return announcements
    
    async def filter_new_announcements_async(self, announcements: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], bool]:
        """ìƒˆë¡œìš´ ê³µê³ ë§Œ ë¹„ë™ê¸° í•„í„°ë§"""
        if not self.enable_duplicate_check:
            return announcements, False
        
        new_announcements = []
        previous_session_duplicate_count = 0
        
        for ann in announcements:
            title = ann.get('title', '')
            title_hash = self.get_title_hash(title)
            
            # ì´ì „ ì‹¤í–‰ì—ì„œ ì²˜ë¦¬ëœ ê³µê³ ì¸ì§€ë§Œ í™•ì¸
            if title_hash in self.processed_titles:
                previous_session_duplicate_count += 1
                logger.debug(f"ì´ì „ ì‹¤í–‰ì—ì„œ ì²˜ë¦¬ëœ ê³µê³  ìŠ¤í‚µ: {title[:50]}...")
                
                # ì—°ì†ëœ ì´ì „ ì‹¤í–‰ ì¤‘ë³µ ì„ê³„ê°’ ë„ë‹¬ì‹œ ì¡°ê¸° ì¢…ë£Œ ì‹ í˜¸
                if previous_session_duplicate_count >= self.duplicate_threshold:
                    logger.info(f"ì´ì „ ì‹¤í–‰ ì¤‘ë³µ ê³µê³  {previous_session_duplicate_count}ê°œ ì—°ì† ë°œê²¬ - ì¡°ê¸° ì¢…ë£Œ ì‹ í˜¸")
                    break
            else:
                # ì´ì „ ì‹¤í–‰ì— ì—†ëŠ” ìƒˆë¡œìš´ ê³µê³ ëŠ” ë¬´ì¡°ê±´ í¬í•¨
                new_announcements.append(ann)
                previous_session_duplicate_count = 0
                logger.debug(f"ìƒˆë¡œìš´ ê³µê³  ì¶”ê°€: {title[:50]}...")
        
        should_stop = previous_session_duplicate_count >= self.duplicate_threshold
        logger.info(f"ì „ì²´ {len(announcements)}ê°œ ì¤‘ ìƒˆë¡œìš´ ê³µê³  {len(new_announcements)}ê°œ, ì´ì „ ì‹¤í–‰ ì¤‘ë³µ {previous_session_duplicate_count}ê°œ ë°œê²¬")
        
        return new_announcements, should_stop
    
    async def process_announcement_async(self, announcement: Dict[str, Any], index: int, output_base: str = 'output'):
        """ê°œë³„ ê³µê³  ë¹„ë™ê¸° ì²˜ë¦¬"""
        logger.info(f"ê³µê³  ë¹„ë™ê¸° ì²˜ë¦¬ ì¤‘ {index}: {announcement['title']}")
        
        # í´ë” ìƒì„±
        folder_title = self.sanitize_filename(announcement['title'])[:100]
        folder_name = f"{index:03d}_{folder_title}"
        
        if len(folder_name) > 200:
            folder_title = folder_title[:195]
            folder_name = f"{index:03d}_{folder_title}"
        
        folder_path = os.path.join(output_base, folder_name)
        os.makedirs(folder_path, exist_ok=True)
        
        # ìƒì„¸ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
        response = await self.get_page(announcement['url'])
        if not response:
            logger.error(f"ìƒì„¸ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {announcement['title']}")
            return
        
        html_content = await response.text()
        
        # ìƒì„¸ ë‚´ìš© íŒŒì‹±
        try:
            detail = await self.parse_detail_page(html_content)
            logger.info(f"ìƒì„¸ í˜ì´ì§€ íŒŒì‹± ì™„ë£Œ - ë‚´ìš©ê¸¸ì´: {len(detail['content'])}, ì²¨ë¶€íŒŒì¼: {len(detail['attachments'])}")
        except Exception as e:
            logger.error(f"ìƒì„¸ í˜ì´ì§€ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return
        
        # ë©”íƒ€ ì •ë³´ ìƒì„±
        meta_info = self._create_meta_info(announcement)
        
        # ë³¸ë¬¸ ì €ì¥
        content_path = os.path.join(folder_path, 'content.md')
        async with aiofiles.open(content_path, 'w', encoding='utf-8') as f:
            await f.write(meta_info + detail['content'])
        
        logger.info(f"ë‚´ìš© ì €ì¥ ì™„ë£Œ: {content_path}")
        
        # ì²¨ë¶€íŒŒì¼ ë¹„ë™ê¸° ë‹¤ìš´ë¡œë“œ
        await self._download_attachments_async(detail['attachments'], folder_path)
        
        # ì²˜ë¦¬ëœ ì œëª©ìœ¼ë¡œ ì¶”ê°€
        self.add_processed_title(announcement['title'])
        
        # ìš”ì²­ ê°„ ëŒ€ê¸°
        if self.delay_between_requests > 0:
            await asyncio.sleep(self.delay_between_requests)
    
    async def _download_attachments_async(self, attachments: List[Dict[str, Any]], folder_path: str):
        """ì²¨ë¶€íŒŒì¼ ë¹„ë™ê¸° ë‹¤ìš´ë¡œë“œ"""
        if not attachments:
            logger.info("ì²¨ë¶€íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return
        
        logger.info(f"{len(attachments)}ê°œ ì²¨ë¶€íŒŒì¼ ë¹„ë™ê¸° ë‹¤ìš´ë¡œë“œ ì‹œì‘")
        attachments_folder = os.path.join(folder_path, 'attachments')
        os.makedirs(attachments_folder, exist_ok=True)
        
        # ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ íƒœìŠ¤í¬ ìƒì„±
        download_tasks = []
        for i, attachment in enumerate(attachments):
            task = self._download_single_attachment_async(attachment, attachments_folder, i)
            download_tasks.append(task)
        
        # ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ (ì œí•œëœ ë™ì‹œì„±)
        semaphore = asyncio.Semaphore(self.max_concurrent_downloads)
        
        async def download_with_semaphore(download_task):
            async with semaphore:
                return await download_task
        
        # ì„¸ë§ˆí¬ì–´ ì ìš©ëœ ë‹¤ìš´ë¡œë“œ íƒœìŠ¤í¬ë“¤ ì‹¤í–‰
        bounded_tasks = [download_with_semaphore(task) for task in download_tasks]
        await asyncio.gather(*bounded_tasks, return_exceptions=True)
    
    async def _download_single_attachment_async(self, attachment: Dict[str, Any], attachments_folder: str, index: int):
        """ë‹¨ì¼ ì²¨ë¶€íŒŒì¼ ë¹„ë™ê¸° ë‹¤ìš´ë¡œë“œ"""
        try:
            # íŒŒì¼ëª… ì¶”ì¶œ
            file_name = attachment.get('filename') or attachment.get('name') or f"attachment_{index+1}"
            logger.info(f"  ì²¨ë¶€íŒŒì¼ {index+1}: {file_name}")
            
            # íŒŒì¼ëª… ì²˜ë¦¬
            file_name = self.sanitize_filename(file_name)
            if not file_name or file_name.isspace():
                file_name = f"attachment_{index+1}"
            
            file_path = os.path.join(attachments_folder, file_name)
            
            # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            success = await self.download_file(attachment['url'], file_path, attachment)
            if not success:
                logger.warning(f"ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {file_name}")
            
        except Exception as e:
            logger.error(f"ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _create_meta_info(self, announcement: Dict[str, Any]) -> str:
        """ë©”íƒ€ ì •ë³´ ìƒì„±"""
        meta_lines = [f"# {announcement['title']}", ""]
        
        # ë™ì ìœ¼ë¡œ ë©”íƒ€ ì •ë³´ ì¶”ê°€
        meta_fields = {
            'writer': 'ì‘ì„±ì',
            'date': 'ì‘ì„±ì¼',
            'period': 'ì ‘ìˆ˜ê¸°ê°„',
            'status': 'ìƒíƒœ',
            'organization': 'ê¸°ê´€',
            'views': 'ì¡°íšŒìˆ˜',
            'number': 'ë²ˆí˜¸'
        }
        
        for field, label in meta_fields.items():
            if field in announcement and announcement[field]:
                meta_lines.append(f"**{label}**: {announcement[field]}")
        
        meta_lines.extend([
            f"**ì›ë³¸ URL**: {announcement['url']}",
            "",
            "---",
            ""
        ])
        
        return "\\n".join(meta_lines)
    
    def load_processed_titles(self, output_base: str = 'output'):
        """ì²˜ë¦¬ëœ ì œëª© ëª©ë¡ ë¡œë“œ"""
        if not self.enable_duplicate_check:
            return
        
        # ì‚¬ì´íŠ¸ë³„ íŒŒì¼ëª… ìƒì„±
        site_name = self.__class__.__name__.replace('Scraper', '').lower()
        self.processed_titles_file = os.path.join(output_base, f'processed_titles_{site_name}.json')
        
        try:
            if os.path.exists(self.processed_titles_file):
                with open(self.processed_titles_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.processed_titles = set(data.get('title_hashes', []))
                    logger.info(f"ê¸°ì¡´ ì²˜ë¦¬ëœ ê³µê³  {len(self.processed_titles)}ê°œ ë¡œë“œ")
            else:
                self.processed_titles = set()
                logger.info("ìƒˆë¡œìš´ ì²˜ë¦¬ëœ ì œëª© íŒŒì¼ ìƒì„±")
        except Exception as e:
            logger.error(f"ì²˜ë¦¬ëœ ì œëª© ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.processed_titles = set()
    
    def save_processed_titles(self):
        """í˜„ì¬ ì„¸ì…˜ì—ì„œ ì²˜ë¦¬ëœ ì œëª©ë“¤ì„ ì´ì „ ì‹¤í–‰ ê¸°ë¡ì— í•©ì³ì„œ ì €ì¥"""
        if not self.enable_duplicate_check or not self.processed_titles_file:
            return
        
        try:
            os.makedirs(os.path.dirname(self.processed_titles_file), exist_ok=True)
            
            # í˜„ì¬ ì„¸ì…˜ì—ì„œ ì²˜ë¦¬ëœ ì œëª©ë“¤ì„ ì´ì „ ì‹¤í–‰ ê¸°ë¡ì— í•©ì¹¨
            all_processed_titles = self.processed_titles | self.current_session_titles
            
            data = {
                'title_hashes': list(all_processed_titles),
                'last_updated': datetime.now().isoformat(),
                'total_count': len(all_processed_titles)
            }
            
            with open(self.processed_titles_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                
            logger.info(f"ì²˜ë¦¬ëœ ì œëª© {len(all_processed_titles)}ê°œ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ì²˜ë¦¬ëœ ì œëª© ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get_title_hash(self, title: str) -> str:
        """ì œëª©ì˜ í•´ì‹œê°’ ìƒì„±"""
        normalized = self.normalize_title(title)
        return hashlib.md5(normalized.encode('utf-8')).hexdigest()
    
    def normalize_title(self, title: str) -> str:
        """ì œëª© ì •ê·œí™”"""
        if not title:
            return ""
        
        normalized = title.strip()
        normalized = re.sub(r'\\s+', ' ', normalized)
        normalized = re.sub(r'[^\\w\\sê°€-í£()-]', '', normalized)
        normalized = normalized.lower()
        
        return normalized
    
    def add_processed_title(self, title: str):
        """í˜„ì¬ ì„¸ì…˜ì—ì„œ ì²˜ë¦¬ëœ ì œëª© ì¶”ê°€"""
        if not self.enable_duplicate_check:
            return
        
        title_hash = self.get_title_hash(title)
        self.current_session_titles.add(title_hash)
    
    def _print_final_stats_async(self, processed_count: int, early_stop: bool, stop_reason: str):
        """ë¹„ë™ê¸° ìµœì¢… í†µê³„ ì¶œë ¥"""
        if not self.stats['start_time'] or not self.stats['end_time']:
            return
        
        duration = self.stats['end_time'] - self.stats['start_time']
        duration_seconds = duration.total_seconds()
        
        logger.info("="*60)
        logger.info("ğŸ“Š ë¹„ë™ê¸° ìŠ¤í¬ë˜í•‘ ì™„ë£Œ í†µê³„")
        logger.info("="*60)
        
        if early_stop:
            logger.info(f"â±ï¸  ì‹¤í–‰ ê¸°ê°„: {duration_seconds:.1f}ì´ˆ (ì¡°ê¸°ì¢…ë£Œ: {stop_reason})")
        else:
            logger.info(f"â±ï¸  ì‹¤í–‰ ì‹œê°„: {duration_seconds:.1f}ì´ˆ")
        
        logger.info(f"ğŸ“„ ì²˜ë¦¬ëœ ê³µê³ : {processed_count}ê°œ")
        logger.info(f"ğŸŒ HTTP ìš”ì²­: {self.stats['requests_made']}ê°œ")
        logger.info(f"ğŸ“ ë‹¤ìš´ë¡œë“œ íŒŒì¼: {self.stats['files_downloaded']}ê°œ")
        logger.info(f"ğŸ’¾ ì „ì²´ ë‹¤ìš´ë¡œë“œ í¬ê¸°: {self._format_size(self.stats['total_download_size'])}")
        logger.info(f"ğŸ”„ ìµœëŒ€ ë™ì‹œ ìš”ì²­: {self.stats['peak_concurrent_requests']}ê°œ")
        
        if self.stats['errors_encountered'] > 0:
            logger.warning(f"âš ï¸  ë°œìƒí•œ ì˜¤ë¥˜: {self.stats['errors_encountered']}ê°œ")
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        if duration_seconds > 0:
            requests_per_second = self.stats['requests_made'] / duration_seconds
            logger.info(f"ğŸš€ ì´ˆë‹¹ ìš”ì²­ ìˆ˜: {requests_per_second:.2f}")
        
        logger.info("="*60)
    
    def _format_size(self, size_bytes: int) -> str:
        """ë°”ì´íŠ¸ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜"""
        if size_bytes == 0:
            return "0 B"
        
        size_names = ["B", "KB", "MB", "GB"]
        i = 0
        while size_bytes >= 1024 and i < len(size_names) - 1:
            size_bytes /= 1024.0
            i += 1
        
        return f"{size_bytes:.1f} {size_names[i]}"
    
    def get_stats(self) -> Dict[str, Any]:
        """í˜„ì¬ í†µê³„ ë°˜í™˜"""
        stats = self.stats.copy()
        if stats['start_time'] and stats['end_time']:
            duration = stats['end_time'] - stats['start_time']
            stats['duration_seconds'] = duration.total_seconds()
        return stats


# ë¹„ë™ê¸° í‘œì¤€ í…Œì´ë¸” ìŠ¤í¬ë˜í¼
class AsyncStandardTableScraper(EnhancedAsyncBaseScraper):
    """ë¹„ë™ê¸° í‘œì¤€ HTML í…Œì´ë¸” ê¸°ë°˜ ê²Œì‹œíŒìš© ìŠ¤í¬ë˜í¼"""
    
    async def get_list_url(self, page_num: int) -> str:
        """í‘œì¤€ í˜ì´ì§€ë„¤ì´ì…˜ URL ìƒì„±"""
        # ê¸°ë³¸ êµ¬í˜„ - í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ ì˜¤ë²„ë¼ì´ë“œ
        if page_num == 1:
            return self.list_url
        else:
            separator = '&' if '?' in self.list_url else '?'
            return f"{self.list_url}{separator}page={page_num}"
    
    async def parse_list_page(self, html_content: str) -> List[Dict[str, Any]]:
        """í‘œì¤€ í…Œì´ë¸” íŒŒì‹± - í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„"""
        # ê¸°ë³¸ êµ¬í˜„ - í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ ì˜¤ë²„ë¼ì´ë“œ í•„ìš”
        soup = BeautifulSoup(html_content, 'html.parser')
        announcements = []
        
        # í…Œì´ë¸” ì°¾ê¸°
        table = soup.find('table')
        if not table:
            return announcements
        
        # í–‰ë“¤ ì°¾ê¸°
        rows = table.find_all('tr')
        
        for row in rows:
            try:
                cells = row.find_all('td')
                if len(cells) < 2:
                    continue
                
                # ì œëª© ë§í¬ ì°¾ê¸°
                link_elem = cells[1].find('a')
                if not link_elem:
                    continue
                
                title = link_elem.get_text(strip=True)
                if not title:
                    continue
                
                # URL êµ¬ì„±
                href = link_elem.get('href', '')
                detail_url = urljoin(self.base_url, href)
                
                announcement = {
                    'title': title,
                    'url': detail_url
                }
                
                announcements.append(announcement)
                
            except Exception as e:
                logger.error(f"í–‰ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        return announcements
    
    async def parse_detail_page(self, html_content: str) -> Dict[str, Any]:
        """ìƒì„¸ í˜ì´ì§€ íŒŒì‹± - í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„"""
        # ê¸°ë³¸ êµ¬í˜„ - í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ ì˜¤ë²„ë¼ì´ë“œ í•„ìš”
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ê°„ë‹¨í•œ ë³¸ë¬¸ ì¶”ì¶œ
        content = soup.get_text(strip=True)[:1000] + "..."
        
        return {
            'title': '',
            'content': f"## ê³µê³  ë‚´ìš©\\n\\n{content}\\n\\n",
            'attachments': [],
            'date': ''
        }