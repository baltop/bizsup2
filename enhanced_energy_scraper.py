#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í•œêµ­ì—ë„ˆì§€ê³µë‹¨ ê³µì§€ì‚¬í•­ ìŠ¤í¬ë˜í¼
URL: https://www.energy.or.kr/front/board/List2.do
"""

import re
import json
import logging
from typing import Dict, List, Any, Optional
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from enhanced_base_scraper import EnhancedBaseScraper

logger = logging.getLogger(__name__)

class EnhancedEnergyScraper(EnhancedBaseScraper):
    """í•œêµ­ì—ë„ˆì§€ê³µë‹¨ ê³µì§€ì‚¬í•­ ìŠ¤í¬ë˜í¼"""
    
    def __init__(self):
        super().__init__()
        
        # ì‚¬ì´íŠ¸ ê¸°ë³¸ ì„¤ì •
        self.base_url = "https://www.energy.or.kr"
        self.list_url = "https://www.energy.or.kr/front/board/List2.do"
        self.detail_url = "https://www.energy.or.kr/front/board/View2.do"
        self.download_url = "https://www.energy.or.kr/commonFile/fileDownload.do"
        self.start_url = self.list_url
        
        # í—¤ë” ì„¤ì •
        self.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        self.session.headers.update(self.headers)
        
        # ì„¸ì…˜ ì´ˆê¸°í™”
        self._initialize_session()
        
    def _initialize_session(self):
        """ì„¸ì…˜ ì´ˆê¸°í™” ë° ì¿ í‚¤ ì„¤ì •"""
        try:
            logger.info("ì—ë„ˆì§€ê³µë‹¨ ì‚¬ì´íŠ¸ ì„¸ì…˜ ì´ˆê¸°í™” ì¤‘...")
            response = self.session.get(self.list_url, timeout=10)
            response.raise_for_status()
            logger.info("ì—ë„ˆì§€ê³µë‹¨ ì‚¬ì´íŠ¸ ì„¸ì…˜ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"ì„¸ì…˜ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {e}")
    
    def get_list_url(self, page_num: int) -> str:
        """í˜ì´ì§€ ë²ˆí˜¸ì— ë”°ë¥¸ ëª©ë¡ URL ìƒì„± (POST ë°©ì‹ì´ë¯€ë¡œ ë™ì¼)"""
        return self.list_url
    
    def _get_page_content(self, page_num: int) -> str:
        """í˜ì´ì§€ë³„ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (POST ë°©ì‹)"""
        try:
            data = {
                'page': str(page_num),
                'searchfield': 'ALL',
                'searchword': ''
            }
            
            response = self.session.post(self.list_url, data=data, timeout=10)
            response.raise_for_status()
            
            return response.text
        except Exception as e:
            logger.error(f"í˜ì´ì§€ {page_num} ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return ""
    
    def parse_list_page(self, html_content: str) -> List[Dict[str, Any]]:
        """ëª©ë¡ í˜ì´ì§€ íŒŒì‹±"""
        soup = BeautifulSoup(html_content, 'html.parser')
        announcements = []
        
        logger.debug("ì—ë„ˆì§€ê³µë‹¨ ì‚¬ì´íŠ¸ ëª©ë¡ í˜ì´ì§€ íŒŒì‹± ì‹œì‘")
        
        # í…Œì´ë¸” êµ¬ì¡° ì°¾ê¸°
        table = soup.find('table')
        if not table:
            logger.warning("ê²Œì‹œíŒ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return announcements
        
        # tbody ë‚´ì˜ tr ìš”ì†Œë“¤ ì°¾ê¸°
        tbody = table.find('tbody')
        if not tbody:
            logger.warning("tbodyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return announcements
        
        rows = tbody.find_all('tr')
        logger.debug(f"ë°œê²¬ëœ í–‰ ìˆ˜: {len(rows)}")
        
        for i, row in enumerate(rows):
            try:
                # td ìš”ì†Œë“¤ ì°¾ê¸°
                cells = row.find_all('td')
                if len(cells) < 5:  # ë²ˆí˜¸, ì œëª©, ì²¨ë¶€, ì‘ì„±ì¼, ì¡°íšŒìˆ˜
                    continue
                
                # ë²ˆí˜¸ ì¶”ì¶œ
                number_cell = cells[0]
                number = number_cell.get_text(strip=True)
                
                # ì œëª© ë° ë§í¬ ì¶”ì¶œ
                title_cell = cells[1]
                link_element = title_cell.find('a')
                if not link_element:
                    continue
                
                title = link_element.get_text(strip=True)
                if not title:
                    continue
                
                # onclick ì†ì„±ì—ì„œ boardMngNo, boardNo ì¶”ì¶œ
                onclick = link_element.get('onclick', '')
                if not onclick or 'fn_Detail' not in onclick:
                    continue
                
                # fn_Detail('2','24437') íŒ¨í„´ íŒŒì‹±
                match = re.search(r"fn_Detail\('(\d+)','(\d+)'\)", onclick)
                if not match:
                    continue
                
                board_mng_no = match.group(1)
                board_no = match.group(2)
                
                # ì²¨ë¶€íŒŒì¼ ì—¬ë¶€ í™•ì¸
                attachment_cell = cells[2]
                attachment_text = attachment_cell.get_text(strip=True)
                has_attachment = 'ì²¨ë¶€' in attachment_text
                
                # ì‘ì„±ì¼ ì¶”ì¶œ
                date_cell = cells[3]
                date = date_cell.get_text(strip=True)
                
                # ì¡°íšŒìˆ˜ ì¶”ì¶œ
                views_cell = cells[4]
                views = views_cell.get_text(strip=True)
                
                # ìƒì„¸ í˜ì´ì§€ URL ìƒì„± (POST ë°©ì‹ì´ë¯€ë¡œ íŒŒë¼ë¯¸í„° ì €ì¥)
                detail_url = self.detail_url
                
                # ê³µì§€ì‚¬í•­ ì •ë³´ êµ¬ì„±
                announcement = {
                    'number': number,
                    'title': title,
                    'url': detail_url,
                    'board_mng_no': board_mng_no,
                    'board_no': board_no,
                    'has_attachment': has_attachment,
                    'date': date,
                    'views': views
                }
                
                announcements.append(announcement)
                logger.debug(f"ê³µê³  íŒŒì‹± ì™„ë£Œ: {number} - {title[:50]}...")
                
            except Exception as e:
                logger.error(f"ê³µê³  {i+1} íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        logger.info(f"ì´ {len(announcements)}ê°œ ê³µê³  íŒŒì‹± ì™„ë£Œ")
        return announcements
    
    def _get_page_announcements(self, page_num: int) -> List[Dict[str, Any]]:
        """íŠ¹ì • í˜ì´ì§€ì˜ ê³µê³  ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        try:
            html_content = self._get_page_content(page_num)
            if not html_content:
                return []
            
            return self.parse_list_page(html_content)
        except Exception as e:
            logger.error(f"í˜ì´ì§€ {page_num} ê³µê³  ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return []
    
    def parse_detail_page(self, html_content: str, detail_url: str = None) -> Dict[str, Any]:
        """ìƒì„¸ í˜ì´ì§€ íŒŒì‹±"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ
        attachments = self._extract_attachments(soup)
        
        # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
        content = self._extract_content(soup)
        
        return {
            'content': content,
            'attachments': attachments
        }
    
    def _extract_content(self, soup: BeautifulSoup) -> str:
        """ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ (ì •í™•í•œ ì„ íƒì ì‚¬ìš©)"""
        try:
            # ì‹¤ì œ ë³¸ë¬¸ ë‚´ìš©ì´ ìˆëŠ” ì»¨í…Œì´ë„ˆ ì°¾ê¸°
            content_selectors = [
                'div.view_cont',       # ê°€ì¥ ì •í™•í•œ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ
                'div.view_inner',      # ëŒ€ì•ˆ ì»¨í…Œì´ë„ˆ
                'article div.board_view',  # ì „ì²´ ê²Œì‹œê¸€ ì»¨í…Œì´ë„ˆ
                'article'              # ìµœí›„ ì˜µì…˜
            ]
            
            content_element = None
            for selector in content_selectors:
                content_element = soup.select_one(selector)
                if content_element:
                    logger.debug(f"ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ë°œê²¬: {selector}")
                    break
            
            if not content_element:
                logger.warning("ë³¸ë¬¸ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return ""
            
            # ì‹¤ì œ ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
            content_parts = []
            
            # 1. p íƒœê·¸ì—ì„œ ì‹¤ì œ ë‚´ìš© ì¶”ì¶œ
            paragraphs = content_element.find_all('p')
            for p in paragraphs:
                text = p.get_text(strip=True)
                if text and len(text) > 5:  # ì˜ë¯¸ìˆëŠ” ë‚´ìš©ë§Œ
                    # ì²¨ë¶€íŒŒì¼ì´ë‚˜ ë„¤ë¹„ê²Œì´ì…˜ ì„¹ì…˜ì— ë„ë‹¬í•˜ë©´ ì¤‘ë‹¨
                    if any(keyword in text for keyword in ['ì²¨ë¶€íŒŒì¼', 'ì´ì „ê¸€', 'ë‹¤ìŒê¸€', 'ëª©ë¡ë³´ê¸°']):
                        break
                    content_parts.append(text)
            
            # 2. div íƒœê·¸ì—ì„œ ë‚´ìš© ì¶”ì¶œ (p íƒœê·¸ê°€ ì—†ëŠ” ê²½ìš°)
            if not content_parts:
                divs = content_element.find_all('div')
                for div in divs:
                    # ì²¨ë¶€íŒŒì¼ ì„¹ì…˜ì€ ì œì™¸
                    if div.get('class') and ('file' in str(div.get('class')) or 'attach' in str(div.get('class'))):
                        continue
                    
                    text = div.get_text(strip=True)
                    if text and len(text) > 10:
                        if any(keyword in text for keyword in ['ì²¨ë¶€íŒŒì¼', 'ì´ì „ê¸€', 'ë‹¤ìŒê¸€', 'ëª©ë¡ë³´ê¸°']):
                            break
                        content_parts.append(text)
            
            # 3. ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ í›„ ì •ë¦¬ (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
            if not content_parts:
                text = content_element.get_text(strip=True)
                if text:
                    # ë¶ˆí•„ìš”í•œ ë„¤ë¹„ê²Œì´ì…˜ í…ìŠ¤íŠ¸ ì œê±°
                    unwanted_phrases = [
                        "ì‘ì„±ì¼ :", "URL ë³µì‚¬í•˜ê¸°", "ì²¨ë¶€íŒŒì¼", "ë“±ë¡ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.",
                        "ì´ì „ê¸€", "ë‹¤ìŒê¸€", "ì´ì „ ê²Œì‹œê¸€ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
                        "ë‹¤ìŒ ê²Œì‹œê¸€ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.", "ëª©ë¡", "ì¡°íšŒìˆ˜",
                        "íŒŒì¼ ì•„ì´ì½˜", "ê³µì§€ì‚¬í•­", "í•œê¸€íŒŒì¼ ì•„ì´ì½˜", "pdfíŒŒì¼ ì•„ì´ì½˜"
                    ]
                    
                    for phrase in unwanted_phrases:
                        text = text.replace(phrase, "")
                    
                    # ì—°ì†ëœ ê³µë°± ë° ì¤„ë°”ê¿ˆ ì •ë¦¬
                    text = re.sub(r'\s+', ' ', text).strip()
                    
                    if text and len(text) > 20:
                        content_parts.append(text)
            
            # ê²°ê³¼ ë°˜í™˜
            if content_parts:
                return '\n\n'.join(content_parts)
            else:
                logger.warning("ë³¸ë¬¸ ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return ""
                
        except Exception as e:
            logger.error(f"ë³¸ë¬¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return ""
    
    def _extract_attachments(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ"""
        attachments = []
        
        # ì²¨ë¶€íŒŒì¼ ì„¹ì…˜ ì°¾ê¸° (ul.view_file)
        attachment_section = soup.find('ul', class_='view_file')
        if not attachment_section:
            logger.debug("ì²¨ë¶€íŒŒì¼ ì„¹ì…˜(ul.view_file)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return attachments
        
        # ì²¨ë¶€íŒŒì¼ ë§í¬ ì°¾ê¸° (onclick ì†ì„±ì— fileDownload í•¨ìˆ˜ í˜¸ì¶œ)
        file_links = attachment_section.find_all('a', onclick=re.compile(r'fileDownload'))
        
        logger.debug(f"ì²¨ë¶€íŒŒì¼ ë§í¬ {len(file_links)}ê°œ ë°œê²¬")
        
        for link in file_links:
            try:
                onclick = link.get('onclick', '')
                
                # span ìš”ì†Œì—ì„œ ì‹¤ì œ íŒŒì¼ëª… ì¶”ì¶œ
                span_elem = link.find('span')
                if not span_elem:
                    continue
                
                # em íƒœê·¸(ì•„ì´ì½˜) ì œê±° í›„ íŒŒì¼ëª… ì¶”ì¶œ
                filename_text = span_elem.get_text(strip=True)
                
                # fileDownload('fileNo','fileSeq','boardMngNo') íŒ¨í„´ íŒŒì‹±
                match = re.search(r"fileDownload\('([^']+)','([^']+)','([^']+)'\)", onclick)
                if not match:
                    logger.debug(f"onclick íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨: {onclick}")
                    continue
                
                file_no = match.group(1)
                file_seq = match.group(2)
                board_mng_no = match.group(3)
                
                # íŒŒì¼ëª… ì •ë¦¬ (ì•„ì´ì½˜ í…ìŠ¤íŠ¸ ì œê±°)
                filename = re.sub(r'^[^[]*\[ì²¨ë¶€\d*\]', '[ì²¨ë¶€' + file_seq + ']', filename_text)
                filename = filename.strip()
                
                if not filename:
                    logger.debug(f"íŒŒì¼ëª… ì¶”ì¶œ ì‹¤íŒ¨: {filename_text}")
                    continue
                
                # íŒŒì¼ í™•ì¥ì ì¶”ì¶œ
                if '.' in filename:
                    file_ext = filename.split('.')[-1].upper()
                else:
                    file_ext = 'UNKNOWN'
                
                attachments.append({
                    'filename': filename,
                    'file_no': file_no,
                    'file_seq': file_seq,
                    'board_mng_no': board_mng_no,
                    'url': self.download_url,
                    'size': '',
                    'type': file_ext
                })
                
                logger.debug(f"ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ ì„±ê³µ: {filename} (fileNo: {file_no}, fileSeq: {file_seq})")
                
            except Exception as e:
                logger.error(f"ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        logger.info(f"ì²¨ë¶€íŒŒì¼ {len(attachments)}ê°œ ì¶”ì¶œ")
        return attachments
    
    def get_detail_content(self, announcement: Dict[str, Any]) -> str:
        """ìƒì„¸ í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
        try:
            data = {
                'boardMngNo': announcement['board_mng_no'],
                'boardNo': announcement['board_no']
            }
            
            response = self.session.post(self.detail_url, data=data, timeout=10)
            response.raise_for_status()
            
            return response.text
        except Exception as e:
            logger.error(f"ìƒì„¸ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return ""
    
    
    def download_file(self, url: str, save_path: str, attachment_info: Dict[str, Any] = None) -> bool:
        """ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        try:
            if not attachment_info:
                return False
            
            # POST ë°ì´í„° êµ¬ì„±
            data = {
                'fileNo': attachment_info['file_no'],
                'fileSeq': attachment_info['file_seq'],
                'boardMngNo': attachment_info['board_mng_no']
            }
            
            response = self.session.post(self.download_url, data=data, timeout=30)
            response.raise_for_status()
            
            # íŒŒì¼ ë‚´ìš© ê²€ì¦
            if len(response.content) < 100:
                logger.warning(f"íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìŒ: {len(response.content)} bytes")
                return False
            
            # Content-Type í™•ì¸
            content_type = response.headers.get('Content-Type', '')
            if 'text/html' in content_type:
                logger.warning(f"HTML ì‘ë‹µ ê°ì§€ë¨: {content_type}")
                return False
            
            # íŒŒì¼ ì €ì¥
            with open(save_path, 'wb') as f:
                f.write(response.content)
            
            logger.info(f"ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {save_path} ({len(response.content):,} bytes)")
            return True
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - 3í˜ì´ì§€ ìˆ˜ì§‘"""
    import sys
    import os
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler('energy_scraper.log', encoding='utf-8')
        ]
    )
    
    logger.info("="*60)
    logger.info("ğŸš€ í•œêµ­ì—ë„ˆì§€ê³µë‹¨ ê³µì§€ì‚¬í•­ ìŠ¤í¬ë˜í¼ ì‹œì‘")
    logger.info("="*60)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    output_dir = "output/energy"
    
    # ê¸°ì¡´ ì¶œë ¥ ë””ë ‰í† ë¦¬ ì •ë¦¬
    if os.path.exists(output_dir):
        import shutil
        logger.info(f"ê¸°ì¡´ ì¶œë ¥ ë””ë ‰í† ë¦¬ ì •ë¦¬: {output_dir}")
        shutil.rmtree(output_dir)
    
    # ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”
    scraper = EnhancedEnergyScraper()
    
    try:
        # 3í˜ì´ì§€ ìˆ˜ì§‘ ì‹¤í–‰ (ì²¨ë¶€íŒŒì¼ í¬í•¨)
        success = scraper.scrape_pages(max_pages=3, output_base="output/energy")
        
        if success:
            logger.info("âœ… ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
            
            # í†µê³„ ì¶œë ¥
            stats = scraper.get_stats()
            logger.info(f"ğŸ“Š ì²˜ë¦¬ í†µê³„: {stats}")
            
        else:
            logger.error("âŒ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨")
            return 1
            
    except KeyboardInterrupt:
        logger.info("â¹ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        return 1
    except Exception as e:
        logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    import sys
    sys.exit(main())