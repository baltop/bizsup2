# "지원사업 공고 수집" Project Guidelines

## 목표
주어진 URL의 사이트에 접속하여 사이트에 공지된 지원사업 공고문을 수집하고 첨부파일을 다운로드 하여 로컬디렉토리에 저장한다.
사이트의 공고는 대부분 pagination 기반의 BBS 형태로 되어 있다.
주어진 URL은 1 페이지의  목록 페이지 이다. 목록들은 대부분 10개에서 30개 내외이다.
수집을 위해 목록 상의 첫번째 공고의 링크를 타고 상세 페이지로 이동하여, 상세페이지에서 공고 내용을 markdown 방식으로 저장한다.
공고 본문이 있는 상세페이지에는 공고본문과 첨부파일 링크가 있다. 첨부파일 링크가 있는 경우 첨부파일들을 다운로드 하여 로컬에 저장한다.
각 공고 별로 본문 파일과 첨부파일은 분리되어 저장되어야 한다.

## 세부 단계
- 주어진 URL을 접속하여 html을 다운로드 받는다.
- html의 태그와 엘레멘트를 분석하여 DOM 구조를 파악한다.
- 공고 목록을 파악하여 각 공고의 세부페이지로 이동할 수 있는 링크나 javacript 방법을 찾아낸다.
- 첫번째 공고 상세 페이지로 이동한다.
- 상세페이지의 html를 분석하여 헤더 제목 등은 제외하고 공고 본문만 추려 낼수 있는 태그, 엘레멘트를 찾아내서 해당 부분만을 markdown으로 변환하여 파일로 저장한다.
- 상세패이지의 html을 분석하여 첨부파일이 있으면 첨부파일을 다운로드 받는다.
- 다시 목록 페이지로 되돌아 가서 다음 공고를 위와 같은 방식으로 처리하고 목록의 전체 공고가 처리될때까지 반복한다.
- 주어진 URL의 공고들을 전부 처리했으면 하단의 pagination bar를 분석하여 다음 페이지로 이동한다.
- 다음 페이지도 공고들이 있으므로 이것들으 처리하고 전부 처리되고 나면 다시 다음 페이지로 이동한다.
- 4페이지까지만 반복하고 4페이지가 끝나면 프로그램을 중단한다.

## 새로운 메모리
- 스크래퍼 개발 시 robots.txt는 무시하고 진행한다.
- 각 웹사이트마다 스크래핑 구조가 다르므로 유연한 접근 방식 필요
- 추가 메모리 - 스크래핑 속도와 효율성을 높이기 위해 병렬 처리 고려
- 일반적인 용도로 사용

## 파일 구조 및 저장 규칙 (중요!)
- **출력 디렉토리**: 반드시 `output/{site_code}` 형태로 저장
- **공고 내용**: 각 게시글 폴더 내 `content.md` 파일로 저장
- **첨부파일**: 각 게시글 폴더 내 `attachments/` 서브폴더에 저장
- **파일 다운로드 검증**: 
  - HTML 응답 감지 (Content-Type: text/html)
  - 파일 크기 1KB 미만 시 HTML 페이지 여부 확인
  - 오류 파일 자동 삭제
- **한글 파일명**: UTF-8 인코딩으로 완전 지원
- **예시 구조**:
  ```
  output/cpri/
  ├── 404_게시글제목/
  │   ├── content.md
  │   └── attachments/
  │       ├── 파일1.hwp
  │       └── 파일2.pdf
  ```

## 개발 프로세스 및 체크포인트

### 1. 개발 준비 단계
- **기본 구조 확인**: `enhanced_base_scraper.py` 클래스 상속 필수
- **instruction.md 읽기**: 개발 가이드라인 및 요구사항 숙지
- **사이트 분석**: 브라우저 개발자 도구로 DOM 구조 파악
- **사이트별 특성 파악**: GET/POST 방식, 세션 필요 여부 확인

### 2. 개발 단계별 체크포인트
#### 단계 1: 사이트 구조 분석
- [ ] 페이지네이션 방식 확인 (GET/POST)
- [ ] 목록 페이지 테이블 구조 파악
- [ ] 상세 페이지 본문 위치 확인
- [ ] 첨부파일 다운로드 링크 패턴 분석

#### 단계 2: 기본 스크래퍼 구현
- [ ] `EnhancedBaseScraper` 클래스 상속
- [ ] 사이트별 초기화 설정 (헤더, 세션 등)
- [ ] `_get_page_announcements()` 메서드 구현
- [ ] `parse_list_page()` 메서드 구현

#### 단계 3: 상세 페이지 처리
- [ ] `parse_detail_page()` 메서드 구현
- [ ] 본문 내용 추출 및 마크다운 변환
- [ ] 첨부파일 링크 추출
- [ ] 한글 파일명 처리 확인

#### 단계 4: 파일 다운로드 및 검증
- [ ] `download_file()` 메서드 구현 또는 기본 메서드 사용
- [ ] HTML 응답 감지 로직 구현
- [ ] 파일 크기 검증 (1KB 미만 HTML 페이지 감지)
- [ ] 오류 파일 자동 삭제

#### 단계 5: 테스트 및 검증
- [ ] 1페이지 테스트 실행
- [ ] 중복 방지 JSON 파일 생성 확인
- [ ] 한글 파일명 정상 처리 확인
- [ ] 첨부파일 다운로드 정상 작동 확인
- [ ] 3페이지 전체 수집 테스트

#### 단계 6: 완료 및 문서화
- [ ] insight 폴더에 개발 인사이트 문서 작성
- [ ] 완료음 실행 (`mpg123 ./ding.mp3`)
- [ ] 수집 결과 요약 보고

### 3. 개발 시 주의사항

#### 기술적 주의사항
- **세션 관리**: 정부 사이트는 세션 초기화 필수
- **POST 방식 처리**: 폼 데이터 정확히 전송
- **한글 인코딩**: UTF-8 완전 지원 확인
- **파일 검증**: Content-Type 및 파일 크기 이중 검증
- **요청 간격**: 1초 이상 간격으로 요청 (서버 부하 방지)

#### 파싱 관련 주의사항
- **CSS 선택자**: 정확한 선택자 사용 (예: `table.artclTable`)
- **다중 방법**: 본문 추출 시 여러 방법 구현 (`div.content` → `article`)
- **첨부파일**: 여러 패턴으로 링크 탐지 (`dt:contains("첨부파일")`, `a[href*="/download.do"]`)
- **공지사항**: 일반 게시글과 공지사항 모두 처리

#### 에러 처리
- **HTML 응답**: 파일 다운로드 시 HTML 페이지 반환 감지
- **빈 내용**: 본문 파싱 실패 시 대안 방법 구현
- **파일 크기**: 1KB 미만 파일의 HTML 내용 검사
- **예외 처리**: 각 단계별 try-catch 구현

### 4. 성능 최적화
- **병렬 처리**: 가능한 경우 비동기 처리 고려
- **메모리 관리**: 대용량 파일 스트리밍 다운로드
- **캐싱**: 세션 재사용 및 중복 요청 방지
- **통계 수집**: 진행 상황 및 성능 모니터링

### 5. 품질 보증
- **중복 방지**: 제목 해시 기반 중복 검사
- **파일 무결성**: 다운로드 파일 크기 및 형식 검증
- **로깅**: 상세한 로그 메시지로 디버깅 지원
- **결과 검증**: 수집된 데이터의 정확성 확인

## 기존 개발 시 주의사항
- 사이트 분석 시 Playwright 사용하여 실제 브라우저 렌더링 확인
- 게시글 목록 파싱 시 정확한 CSS 선택자 사용 (예: td.td_subject)
- 공지사항 포함하여 모든 게시글 처리
- 첨부파일 다운로드 시 세션 유지 및 적절한 User-Agent 설정
- 요청 간격 조절 (1초 이상)
- 파일 크기 검증으로 오류 파일 감지 및 제거

## ⚠️ BASE 클래스 수정 금지 (중요!)
- **절대 금지**: `enhanced_base_scraper.py` 파일을 수정하거나 변경하지 말 것
- **절대 금지**: 새로운 base 클래스 (minimal_base_scraper.py 등) 생성 금지
- **이유**: 128개 이상의 기존 스크래퍼들이 `enhanced_base_scraper.py`를 상속받아 사용 중
- **올바른 방법**: 개별 스크래퍼에서만 사이트별 특성에 맞게 메서드 오버라이드
- **문법 오류 발견 시**: base 클래스를 수정하지 말고, 다른 정상 작동하는 스크래퍼 참고
- **검증 방법**: 기존 작동 스크래퍼(예: enhanced_gidp_scraper.py) 실행하여 정상 작동 확인

- to memorize