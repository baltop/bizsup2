# -*- coding: utf-8 -*-
"""
ì¸ì œêµ°ë¬¸í™”ì¬ë‹¨ ê³µì§€ì‚¬í•­ ìŠ¤í¬ë˜í¼
URL: http://www.injeart.or.kr/?p=19&page=1
"""

import os
import time
import logging
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from enhanced_base_scraper import EnhancedBaseScraper
from typing import List, Dict, Any
import re

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('injeart_scraper.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

class EnhancedInjeartScraper(EnhancedBaseScraper):
    """ì¸ì œêµ°ë¬¸í™”ì¬ë‹¨ ê³µì§€ì‚¬í•­ ìŠ¤í¬ë˜í¼"""
    
    def __init__(self):
        super().__init__()
        self.base_url = "http://www.injeart.or.kr"
        self.list_url = "http://www.injeart.or.kr/?p=19&page=1"
        self.site_code = "injeart"
        
        # í—¤ë” ì„¤ì •
        self.headers.update({
            'Referer': self.base_url,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        self.session.headers.update(self.headers)
        
        # ì‚¬ì´íŠ¸ë³„ ì„¤ì •
        self.verify_ssl = False
        self.timeout = 30
        self.delay_between_requests = 1
        
    def get_list_url(self, page_num):
        """í˜ì´ì§€ ë²ˆí˜¸ì— ë”°ë¥¸ ëª©ë¡ URL ë°˜í™˜"""
        return f"{self.base_url}/?p=19&page={page_num}"
    
    def parse_list_page(self, html_content: str) -> List[Dict[str, Any]]:
        """ëª©ë¡ í˜ì´ì§€ì—ì„œ ê³µê³  ëª©ë¡ ì¶”ì¶œ"""
        soup = BeautifulSoup(html_content, 'html.parser')
        announcements = []
        
        # ê³µê³  ëª©ë¡ì´ ë‹´ê¸´ í…Œì´ë¸” ì°¾ê¸° - injeart ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
        table = soup.find('table')  # ì²« ë²ˆì§¸ í…Œì´ë¸”ì´ ëª©ë¡ í…Œì´ë¸”
        if not table:
            # ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ í…Œì´ë¸” ì°¾ê¸°
            tables = soup.find_all('table')
            for t in tables:
                if t.find('th', string=lambda text: text and 'ì œëª©' in text):
                    table = t
                    break
        
        if not table:
            logger.warning("ê³µê³  ëª©ë¡ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return announcements
        
        # í…Œì´ë¸”ì˜ í–‰ë“¤ ì°¾ê¸° (í—¤ë” ì œì™¸)
        rows = table.select('tbody tr') or table.select('tr')[1:]
        
        if not rows:
            logger.warning("í…Œì´ë¸”ì— ë°ì´í„° í–‰ì´ ì—†ìŠµë‹ˆë‹¤")
            return announcements
        
        logger.info(f"ì´ {len(rows)}ê°œì˜ í–‰ ë°œê²¬")
        
        for i, row in enumerate(rows):
            try:
                cells = row.find_all(['td', 'th'])
                if len(cells) < 3:
                    logger.debug(f"í–‰ {i}: ì…€ ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ ({len(cells)}ê°œ)")
                    continue
                
                # ì œëª©ê³¼ ë§í¬ ì°¾ê¸° - ì—¬ëŸ¬ ë°©ë²• ì‹œë„
                title_link = None
                title = ""
                
                # ë°©ë²• 1: a íƒœê·¸ê°€ ìˆëŠ” ì…€ ì°¾ê¸°
                for cell in cells:
                    link = cell.find('a', href=True)
                    if link and link.get_text(strip=True):
                        title_link = link
                        title = link.get_text(strip=True)
                        break
                
                if not title_link:
                    logger.debug(f"í–‰ {i}: ì œëª© ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    continue
                
                # URL êµ¬ì„±
                href = title_link.get('href', '')
                if not href or href.startswith('#'):
                    logger.debug(f"í–‰ {i}: ìœ íš¨í•˜ì§€ ì•Šì€ href ({href})")
                    continue
                
                detail_url = urljoin(self.base_url, href)
                
                # ê¸°ë³¸ ê³µê³  ì •ë³´
                announcement = {
                    'title': title.strip(),
                    'url': detail_url
                }
                
                # ì¶”ê°€ ì •ë³´ ì¶”ì¶œ (ë²ˆí˜¸, ë¶„ë¥˜, ì‘ì„±ì¼, ì¡°íšŒìˆ˜)
                try:
                    # ë²ˆí˜¸ (ì²«ë²ˆì§¸ ì…€)
                    if cells[0]:
                        number = cells[0].get_text(strip=True)
                        if number and not number.lower() in ['ë²ˆí˜¸', 'no']:
                            announcement['number'] = number
                    
                    # ë¶„ë¥˜ (ë‘ë²ˆì§¸ ì…€, ìˆë‹¤ë©´)
                    if len(cells) >= 4 and cells[1]:
                        category = cells[1].get_text(strip=True)
                        if category and not category.lower() in ['ë¶„ë¥˜', 'category']:
                            announcement['category'] = category
                    
                    # ì‘ì„±ì¼ (ëì—ì„œ ë‘ë²ˆì§¸ ì…€)
                    if len(cells) >= 4:
                        date_text = cells[-2].get_text(strip=True)
                        if date_text and not date_text.lower() in ['ë“±ë¡ì¼', 'ì‘ì„±ì¼', 'date']:
                            announcement['date'] = date_text
                    
                    # ì¡°íšŒìˆ˜ (ë§ˆì§€ë§‰ ì…€)
                    views_text = cells[-1].get_text(strip=True)
                    if views_text and views_text.isdigit():
                        announcement['views'] = views_text
                        
                except Exception as e:
                    logger.debug(f"í–‰ {i} ì¶”ê°€ ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                
                announcements.append(announcement)
                logger.debug(f"ê³µê³  ì¶”ê°€: {title[:50]}...")
                
            except Exception as e:
                logger.error(f"í–‰ {i} íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        logger.info(f"ì´ {len(announcements)}ê°œ ê³µê³  ì¶”ì¶œ ì™„ë£Œ")
        return announcements
    
    def parse_detail_page(self, html_content: str) -> Dict[str, Any]:
        """ìƒì„¸ í˜ì´ì§€ì—ì„œ ë‚´ìš©ê³¼ ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
        content = ""
        content_area = None
        
        # ë°©ë²• 1: ê³µì§€ì‚¬í•­ ìƒì„¸ë³´ê¸° í…Œì´ë¸”ì—ì„œ ë‚´ìš© ì°¾ê¸°
        detail_table = soup.find('table', string=lambda text: text and 'ê³µì§€ì‚¬í•­ ìƒì„¸ë³´ê¸°' in text)
        if not detail_table:
            detail_table = soup.select_one('table[summary*="ìƒì„¸ë³´ê¸°"]')
        
        if detail_table:
            # ë³¸ë¬¸ì´ ë‹´ê¸´ ì…€ ì°¾ê¸° (ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ì…€)
            cells = detail_table.find_all('td')
            max_content = ""
            for cell in cells:
                cell_text = cell.get_text(strip=True)
                if len(cell_text) > len(max_content) and len(cell_text) > 50:
                    max_content = cell_text
                    content_area = cell
        
        if not content_area:
            # ë°©ë²• 2: ì¼ë°˜ì ì¸ ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸°
            selectors = [
                '.content', '.board_view', '.view_content',
                '#content', '#board_content', '#view_content'
            ]
            
            for selector in selectors:
                content_area = soup.select_one(selector)
                if content_area:
                    break
        
        if content_area:
            # HTMLì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜
            content = self.h.handle(str(content_area)).strip()
        else:
            # ë°©ë²• 3: ì œëª© ë‹¤ìŒì˜ ê¸´ í…ìŠ¤íŠ¸ ì°¾ê¸°
            all_text = soup.get_text()
            if len(all_text) > 100:
                content = all_text[:1000] + "..."
                
        # ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ
        attachments = []
        
        # ë°©ë²• 1: "ì²¨ë¶€íŒŒì¼" ë¼ë²¨ì´ ìˆëŠ” í–‰ì—ì„œ ì°¾ê¸° - ì¸ì œ ì‚¬ì´íŠ¸ íŠ¹í™”
        attach_cells = soup.find_all(['th', 'td'], string=lambda text: text and 'ì²¨ë¶€íŒŒì¼' in text)
        for cell in attach_cells:
            parent_row = cell.find_parent('tr')
            if parent_row:
                file_links = parent_row.find_all('a')
                for link in file_links:
                    onclick = link.get('onclick', '')
                    filename = link.get_text(strip=True)
                    
                    # chkDownAuth('id') íŒ¨í„´ íŒŒì‹±
                    if onclick and 'chkDownAuth(' in onclick:
                        import re
                        match = re.search(r"chkDownAuth\('([^']+)'\)", onclick)
                        if match and filename:
                            file_id = match.group(1)
                            download_url = f"{self.base_url}/inc/down.php?fileidx={file_id}"
                            attachments.append({
                                'filename': filename,
                                'url': download_url
                            })
                            logger.debug(f"ì²¨ë¶€íŒŒì¼ ë°œê²¬: {filename} -> {download_url}")
        
        # ë°©ë²• 2: ë‹¤ìš´ë¡œë“œ ë§í¬ íŒ¨í„´ìœ¼ë¡œ ì°¾ê¸°
        if not attachments:
            download_links = soup.find_all('a', href=lambda href: href and ('download' in href.lower() or 'file' in href.lower()))
            for link in download_links:
                href = link.get('href', '')
                filename = link.get_text(strip=True)
                if filename and href:
                    attachments.append({
                        'filename': filename,
                        'url': urljoin(self.base_url, href)
                    })
        
        # ë°©ë²• 3: PDF, HWP, DOC ë“± íŒŒì¼ í™•ì¥ìë¡œ ëë‚˜ëŠ” ë§í¬ ì°¾ê¸°
        if not attachments:
            file_extensions = ['.pdf', '.hwp', '.doc', '.docx', '.xls', '.xlsx', '.zip', '.jpg', '.png']
            all_links = soup.find_all('a', href=True)
            for link in all_links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                if any(ext in href.lower() for ext in file_extensions) or any(ext in text.lower() for ext in file_extensions):
                    filename = text if text else os.path.basename(href)
                    if filename:
                        attachments.append({
                            'filename': filename,
                            'url': urljoin(self.base_url, href)
                        })
        
        logger.info(f"ë³¸ë¬¸ ê¸¸ì´: {len(content)}, ì²¨ë¶€íŒŒì¼: {len(attachments)}ê°œ")
        
        return {
            'content': content,
            'attachments': attachments
        }
    
    def download_file(self, url: str, save_path: str, attachment_info: Dict[str, Any] = None) -> bool:
        """íŒŒì¼ ë‹¤ìš´ë¡œë“œ - ì¸ì œ ì‚¬ì´íŠ¸ íŠ¹í™”"""
        # íŠ¹ìˆ˜ URL íŒ¨í„´ ì²˜ë¦¬
        if '#url' in url:
            logger.warning(f"ì˜ëª»ëœ ë‹¤ìš´ë¡œë“œ URL: {url}")
            return False
        
        # ê¸°ë³¸ ë‹¤ìš´ë¡œë“œ ë©”ì„œë“œ í˜¸ì¶œ
        success = super().download_file(url, save_path, attachment_info)
        
        if success:
            # íŒŒì¼ í¬ê¸° ê²€ì¦ (HTML í˜ì´ì§€ê°€ ë‹¤ìš´ë¡œë“œëœ ê²½ìš° ê°ì§€)
            if os.path.exists(save_path):
                file_size = os.path.getsize(save_path)
                if file_size < 1024:  # 1KB ë¯¸ë§Œ
                    # íŒŒì¼ ë‚´ìš©ì´ HTMLì¸ì§€ í™•ì¸
                    with open(save_path, 'rb') as f:
                        content = f.read(500).decode('utf-8', errors='ignore')
                        if '<html' in content.lower() or '<!doctype' in content.lower():
                            logger.warning(f"HTML í˜ì´ì§€ê°€ ë‹¤ìš´ë¡œë“œë¨. íŒŒì¼ ì‚­ì œ: {save_path}")
                            os.remove(save_path)
                            return False
        
        return success
    

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    scraper = EnhancedInjeartScraper()
    
    # output/injeart ë””ë ‰í† ë¦¬ ì„¤ì •
    output_dir = os.path.join('output', scraper.site_code)
    
    logger.info("="*60)
    logger.info("ğŸ›ï¸ ì¸ì œêµ°ë¬¸í™”ì¬ë‹¨ ê³µì§€ì‚¬í•­ ìŠ¤í¬ë˜í¼ ì‹œì‘")
    logger.info(f"ğŸ“‚ ì €ì¥ ê²½ë¡œ: {output_dir}")
    logger.info(f"ğŸŒ ëŒ€ìƒ ì‚¬ì´íŠ¸: {scraper.base_url}")
    logger.info("="*60)
    
    try:
        # 3í˜ì´ì§€ê¹Œì§€ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
        success = scraper.scrape_pages(max_pages=3, output_base=output_dir)
        
        if success:
            logger.info("âœ… ìŠ¤í¬ë˜í•‘ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            logger.error("âŒ ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            
    except KeyboardInterrupt:
        logger.info("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        raise


if __name__ == "__main__":
    main()