#!/usr/bin/env python3
"""
ì„œìš¸ì‹ ë³´ ì‚¬ì´íŠ¸ ìµœì¢… í…ŒìŠ¤íŠ¸ - ì˜¬ë°”ë¥¸ ë¶„ì„ ë¡œì§ ì ìš©
"""

import requests
import re
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import os

class SeoulshinboFinalTester:
    def __init__(self):
        self.base_url = "https://www.seoulshinbo.co.kr"
        self.list_url = "https://www.seoulshinbo.co.kr/wbase/contents/bbs/list.do?mng_cd=STRY9788"
        self.session = requests.Session()
        
        # í—¤ë” ì„¤ì •
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })

    def get_list_page(self):
        """ëª©ë¡ í˜ì´ì§€ ì ‘ê·¼ ë° ì¿ í‚¤ íšë“"""
        print("1. ëª©ë¡ í˜ì´ì§€ ì ‘ê·¼...")
        
        response = self.session.get(self.list_url, timeout=30)
        response.raise_for_status()
        
        print(f"   âœ… Status: {response.status_code}")
        print(f"   ì¿ í‚¤: {dict(self.session.cookies)}")
        
        return response.text

    def parse_announcements(self, html_content):
        """ê³µê³  ëª©ë¡ íŒŒì‹±"""
        print("2. ê³µê³  ëª©ë¡ íŒŒì‹±...")
        
        soup = BeautifulSoup(html_content, 'html.parser')
        announcements = []
        
        # ì˜¬ë°”ë¥¸ í…Œì´ë¸” ì°¾ê¸° (ë‘ ë²ˆì§¸ í…Œì´ë¸”ì´ ê³µê³  ëª©ë¡)
        tables = soup.find_all('table')
        if len(tables) < 2:
            print("   âŒ ê³µê³  í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return []
        
        table = tables[1]  # ë‘ ë²ˆì§¸ í…Œì´ë¸”
        tbody = table.find('tbody')
        if not tbody:
            tbody = table
        
        rows = tbody.find_all('tr')
        print(f"   í…Œì´ë¸”ì—ì„œ {len(rows)}ê°œ í–‰ ë°œê²¬")
        
        for row in rows[:5]:  # ì²˜ìŒ 5ê°œë§Œ í…ŒìŠ¤íŠ¸
            cells = row.find_all('td')
            if len(cells) < 2:
                continue
            
            # ì œëª© ì…€ (ë‘ ë²ˆì§¸ ì…€)
            title_cell = cells[1]
            link = title_cell.find('a')
            
            if link:
                title = link.get_text(strip=True)
                href = link.get('href', '')
                
                # JavaScript ë§í¬ì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
                match = re.search(r"bbs\.goView\('(\d+)',\s*'(\d+)'\)", href)
                if match:
                    page_index = match.group(1)
                    bno = match.group(2)
                    
                    announcement = {
                        'title': title,
                        'page_index': page_index,
                        'bno': bno,
                        'detail_url': f"{self.base_url}/wbase/contents/bbs/view/{bno}.do?mng_cd=STRY9788&pageIndex={page_index}"
                    }
                    announcements.append(announcement)
                    
                    print(f"   ê³µê³  {len(announcements)}: {title[:60]}...")
        
        return announcements

    def get_detail_page(self, announcement):
        """ìƒì„¸í˜ì´ì§€ ì ‘ê·¼"""
        print(f"3. ìƒì„¸í˜ì´ì§€ ì ‘ê·¼: {announcement['title'][:50]}...")
        
        detail_url = announcement['detail_url']
        headers = {'Referer': self.list_url}
        
        response = self.session.get(detail_url, headers=headers, timeout=30)
        response.raise_for_status()
        
        print(f"   âœ… Status: {response.status_code}, í¬ê¸°: {len(response.text)} bytes")
        
        return response.text

    def parse_detail_page(self, html_content):
        """ìƒì„¸í˜ì´ì§€ íŒŒì‹±"""
        print("4. ìƒì„¸í˜ì´ì§€ ë‚´ìš© íŒŒì‹±...")
        
        soup = BeautifulSoup(html_content, 'html.parser')
        result = {
            'title': '',
            'content': '',
            'attachments': []
        }
        
        # í…Œì´ë¸”ì—ì„œ ì •ë³´ ì¶”ì¶œ (ì²« ë²ˆì§¸ í…Œì´ë¸”ì´ ìƒì„¸ ì •ë³´)
        tables = soup.find_all('table')
        if tables:
            table = tables[0]
            rows = table.find_all('tr')
            
            for row in rows:
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 2:
                    header = cells[0].get_text(strip=True)
                    content = cells[1].get_text(strip=True)
                    
                    if header == 'ì œëª©':
                        result['title'] = content
                        print(f"   ì œëª©: {content}")
                    elif header == 'ë‚´ìš©':
                        result['content'] = content
                        print(f"   ë³¸ë¬¸ ê¸¸ì´: {len(content)} ë¬¸ì")
                        print(f"   ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°: {content[:100]}...")
                    elif header == 'ì²¨ë¶€íŒŒì¼':
                        # ì²¨ë¶€íŒŒì¼ ë§í¬ ì°¾ê¸°
                        file_links = cells[1].find_all('a')
                        for link in file_links:
                            href = link.get('href', '')
                            text = link.get_text(strip=True)
                            if href and text:
                                full_url = urljoin(self.base_url, href)
                                result['attachments'].append({
                                    'name': text,
                                    'url': full_url
                                })
                                print(f"   ì²¨ë¶€íŒŒì¼: {text} -> {href}")
        
        return result

    def test_file_download(self, file_info):
        """ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸"""
        print(f"5. ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸: {file_info['name']}")
        
        try:
            headers = {'Referer': self.list_url}
            response = self.session.get(file_info['url'], headers=headers, timeout=30, stream=True)
            response.raise_for_status()
            
            content_type = response.headers.get('Content-Type', '')
            content_length = response.headers.get('Content-Length', 'Unknown')
            
            print(f"   âœ… Content-Type: {content_type}")
            print(f"   âœ… Content-Length: {content_length} bytes")
            
            # ì²« ëª‡ ë°”ì´íŠ¸ í™•ì¸
            first_bytes = next(response.iter_content(chunk_size=10))
            print(f"   ì²« ë°”ì´íŠ¸: {first_bytes[:10].hex()}")
            
            return True
            
        except Exception as e:
            print(f"   âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

def main():
    print("ì„œìš¸ì‹ ë³´ ì‚¬ì´íŠ¸ ìµœì¢… í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    tester = SeoulshinboFinalTester()
    
    try:
        # 1. ëª©ë¡ í˜ì´ì§€ ì ‘ê·¼
        html_content = tester.get_list_page()
        
        # 2. ê³µê³  ëª©ë¡ íŒŒì‹±
        announcements = tester.parse_announcements(html_content)
        if not announcements:
            print("âŒ ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
        
        # 3. ì²« ë²ˆì§¸ ê³µê³  ìƒì„¸í˜ì´ì§€ ì ‘ê·¼
        first_announcement = announcements[0]
        detail_html = tester.get_detail_page(first_announcement)
        
        # 4. ìƒì„¸í˜ì´ì§€ íŒŒì‹±
        detail_info = tester.parse_detail_page(detail_html)
        
        # 5. ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸
        if detail_info['attachments']:
            for file_info in detail_info['attachments']:
                tester.test_file_download(file_info)
        else:
            print("5. ì²¨ë¶€íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
        
        # 6. ê²°ê³¼ ìš”ì•½
        print("\n" + "=" * 60)
        print("ğŸ¯ ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"   ëª©ë¡ í˜ì´ì§€ ì ‘ê·¼: âœ… ì„±ê³µ")
        print(f"   ê³µê³  íŒŒì‹±: âœ… {len(announcements)}ê°œ ì„±ê³µ")
        print(f"   ìƒì„¸í˜ì´ì§€ ì ‘ê·¼: âœ… ì„±ê³µ")
        print(f"   ì œëª© ì¶”ì¶œ: âœ… {detail_info['title']}")
        print(f"   ë³¸ë¬¸ ì¶”ì¶œ: âœ… {len(detail_info['content'])}ì")
        print(f"   ì²¨ë¶€íŒŒì¼: âœ… {len(detail_info['attachments'])}ê°œ")
        
        print("\nğŸš€ Python requests êµ¬í˜„ ë°©ë²• ìš”ì•½:")
        print("1. âœ… session.get()ìœ¼ë¡œ ëª©ë¡ í˜ì´ì§€ ì ‘ê·¼í•˜ì—¬ ì¿ í‚¤ ìë™ íšë“")
        print("2. âœ… JavaScript hrefì—ì„œ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ bno, pageIndex ì¶”ì¶œ")
        print("3. âœ… URL íŒ¨í„´: /wbase/contents/bbs/view/{bno}.do?mng_cd=STRY9788&pageIndex={pageIndex}")
        print("4. âœ… Referer í—¤ë” ì„¤ì •í•˜ì—¬ GET ìš”ì²­")
        print("5. âœ… ì‘ë‹µì€ ì •ìƒì ìœ¼ë¡œ ë°›ì•„ì§ (404 ì•„ë‹˜)")
        print("6. âœ… í…Œì´ë¸” êµ¬ì¡°ë¡œ íŒŒì‹±: ì œëª©, ë‚´ìš©, ì²¨ë¶€íŒŒì¼")
        print("7. âœ… ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œë„ ë™ì¼í•œ ì„¸ì…˜ìœ¼ë¡œ ê°€ëŠ¥")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()