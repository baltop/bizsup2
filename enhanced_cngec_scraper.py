#!/usr/bin/env python3
"""
Enhanced CNGEC Scraper
ì¶©ë‚¨ë…¹ìƒ‰í™˜ê²½ì§€ì›ì„¼í„° (CNGEC) ê³µê³  ìˆ˜ì§‘ ìŠ¤í¬ë˜í¼
"""

import requests
from bs4 import BeautifulSoup
import os
import time
import urllib.parse
import re
from urllib.parse import urljoin
import html2text
import hashlib
from pathlib import Path
import json

class CNGECScraper:
    def __init__(self, base_url, site_code, output_dir="output"):
        self.base_url = base_url
        self.site_code = site_code
        self.output_dir = os.path.join(output_dir, site_code)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        Path(self.output_dir).mkdir(parents=True, exist_ok=True)
        
        # HTML to markdown ë³€í™˜ê¸°
        self.h2t = html2text.HTML2Text()
        self.h2t.ignore_links = False
        self.h2t.ignore_images = False
        self.h2t.body_width = 0
        
        # ë‹¤ìš´ë¡œë“œ í†µê³„
        self.stats = {
            'total_posts': 0,
            'attachments_downloaded': 0,
            'pages_processed': 0,
            'errors': []
        }
    
    def clean_filename(self, filename):
        """íŒŒì¼ëª…ì„ ì•ˆì „í•˜ê²Œ ì •ë¦¬"""
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ê¸¸ì´ ì œí•œ
        cleaned = re.sub(r'[<>:"/\\|?*]', '_', filename)
        cleaned = cleaned.strip()
        
        # íŒŒì¼ëª…ì´ ë„ˆë¬´ ê¸¸ë©´ ì¤„ì„
        if len(cleaned) > 200:
            name, ext = os.path.splitext(cleaned)
            cleaned = name[:200-len(ext)] + ext
        
        return cleaned
    
    def download_file(self, url, filepath):
        """íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        try:
            response = self.session.get(url, stream=True, timeout=30)
            response.raise_for_status()
            
            # Content-Type í™•ì¸
            content_type = response.headers.get('content-type', '').lower()
            
            # HTML ì‘ë‹µì¸ ê²½ìš° (ì˜¤ë¥˜ í˜ì´ì§€) ê°ì§€
            if 'text/html' in content_type:
                print(f"    ê²½ê³ : HTML ì‘ë‹µ ê°ì§€ - {url}")
                return 0
            
            total_size = 0
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        total_size += len(chunk)
            
            file_size = os.path.getsize(filepath)
            
            # íŒŒì¼ì´ ë„ˆë¬´ ì‘ì€ ê²½ìš° (ì˜¤ë¥˜ í˜ì´ì§€ì¼ ê°€ëŠ¥ì„±)
            if file_size < 1024:
                print(f"    ê²½ê³ : íŒŒì¼ í¬ê¸° ë„ˆë¬´ ì‘ìŒ ({file_size} bytes) - {url}")
                # íŒŒì¼ ë‚´ìš© í™•ì¸
                with open(filepath, 'rb') as f:
                    content = f.read(512)
                    if b'<html' in content.lower() or b'<!doctype' in content.lower():
                        print(f"    ì˜¤ë¥˜: HTML í˜ì´ì§€ ë‹¤ìš´ë¡œë“œë¨")
                        os.remove(filepath)
                        return 0
            
            return file_size
        except Exception as e:
            self.stats['errors'].append(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ {url}: {str(e)}")
            return 0
    
    def extract_content(self, soup):
        """ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ"""
        content_div = soup.find('div', {'id': 'bo_v_con'})
        if not content_div:
            return ""
        
        # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
        for element in content_div.find_all(['script', 'style']):
            element.decompose()
        
        return str(content_div)
    
    def extract_attachments(self, soup):
        """ì²¨ë¶€íŒŒì¼ ì •ë³´ ì¶”ì¶œ"""
        attachments = []
        
        file_section = soup.find('section', {'id': 'bo_v_file'})
        if file_section:
            for link in file_section.find_all('a', class_='view_file_download'):
                href = link.get('href')
                if href:
                    strong_elem = link.find('strong')
                    if strong_elem:
                        filename = strong_elem.text.strip()
                        # íŒŒì¼ í¬ê¸° ì¶”ì¶œ
                        parent_li = link.parent
                        if parent_li:
                            size_match = re.search(r'\(([^)]+)\)', parent_li.text)
                            size = size_match.group(1) if size_match else "Unknown"
                        else:
                            size = "Unknown"
                        
                        attachments.append({
                            'url': urljoin(self.base_url, href),
                            'filename': filename,
                            'size': size
                        })
        
        return attachments
    
    def scrape_post_detail(self, post_url, post_id):
        """ê²Œì‹œê¸€ ìƒì„¸ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘"""
        try:
            print(f"  ìƒì„¸ í˜ì´ì§€ ì ‘ì†: {post_url}")
            response = self.session.get(post_url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ
            title_elem = soup.find('span', class_='bo_v_tit')
            title = "ì œëª© ì—†ìŒ"
            if title_elem:
                title = title_elem.text.strip()
            
            # ë©”íƒ€ ì •ë³´ ì¶”ì¶œ
            meta_info = {}
            info_section = soup.find('section', {'id': 'bo_v_info'})
            if info_section:
                date_elem = info_section.find('strong', class_='if_date')
                if date_elem:
                    date_text = date_elem.text.strip()
                    # ë‚ ì§œ í˜•ì‹ì—ì„œ ì‹¤ì œ ë‚ ì§œ ì¶”ì¶œ
                    date_match = re.search(r'(\d{2}-\d{2}-\d{2} \d{2}:\d{2})', date_text)
                    if date_match:
                        meta_info['date'] = date_match.group(1)
                
                # ì‘ì„±ìëŠ” í•­ìƒ "ìµœê³ ê´€ë¦¬ì"ë¡œ ë³´ì„
                meta_info['author'] = "ìµœê³ ê´€ë¦¬ì"
            
            # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
            content_html = self.extract_content(soup)
            content_md = self.h2t.handle(content_html)
            
            # ì²¨ë¶€íŒŒì¼ ì •ë³´ ì¶”ì¶œ
            attachments = self.extract_attachments(soup)
            
            # ê²Œì‹œê¸€ë³„ ë””ë ‰í† ë¦¬ ìƒì„±
            safe_title = self.clean_filename(title)
            post_dir = os.path.join(self.output_dir, f"{post_id}_{safe_title}")
            os.makedirs(post_dir, exist_ok=True)
            
            # ì²¨ë¶€íŒŒì¼ ë””ë ‰í† ë¦¬ ìƒì„±
            attachments_dir = os.path.join(post_dir, "attachments")
            if attachments:
                os.makedirs(attachments_dir, exist_ok=True)
            
            # ë³¸ë¬¸ì„ content.md íŒŒì¼ë¡œ ì €ì¥
            md_filepath = os.path.join(post_dir, "content.md")
            
            with open(md_filepath, 'w', encoding='utf-8') as f:
                f.write(f"# {title}\n\n")
                f.write(f"**ì‘ì„±ì:** {meta_info.get('author', 'N/A')}\n")
                f.write(f"**ë“±ë¡ì¼:** {meta_info.get('date', 'N/A')}\n")
                f.write(f"**URL:** {post_url}\n\n")
                f.write("## ë³¸ë¬¸\n\n")
                f.write(content_md)
                
                if attachments:
                    f.write("\n## ì²¨ë¶€íŒŒì¼\n\n")
                    for att in attachments:
                        f.write(f"- {att['filename']} ({att['size']})\n")
            
            print(f"  ë³¸ë¬¸ ì €ì¥: {md_filepath}")
            
            # ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            for i, attachment in enumerate(attachments):
                try:
                    filename = self.clean_filename(attachment['filename'])
                    file_path = os.path.join(attachments_dir, filename)
                    
                    print(f"  ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ: {filename}")
                    file_size = self.download_file(attachment['url'], file_path)
                    
                    if file_size > 0:
                        self.stats['attachments_downloaded'] += 1
                        print(f"    ì™„ë£Œ: {filename} ({file_size} bytes)")
                    else:
                        print(f"    ì‹¤íŒ¨: {filename}")
                        
                except Exception as e:
                    self.stats['errors'].append(f"ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ {attachment['filename']}: {str(e)}")
                    print(f"    ì˜¤ë¥˜: {str(e)}")
            
            return {
                'title': title,
                'author': meta_info.get('author', 'N/A'),
                'date': meta_info.get('date', 'N/A'),
                'attachments': len(attachments),
                'post_dir': post_dir
            }
            
        except Exception as e:
            self.stats['errors'].append(f"ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ {post_url}: {str(e)}")
            print(f"    ì˜¤ë¥˜: {str(e)}")
            return None
    
    def scrape_page(self, page_num):
        """í˜ì´ì§€ë³„ ê²Œì‹œê¸€ ëª©ë¡ ìŠ¤í¬ë˜í•‘"""
        try:
            if page_num == 1:
                url = self.base_url
            else:
                url = f"{self.base_url}&page={page_num}"
                
            print(f"\ní˜ì´ì§€ {page_num} ìŠ¤í¬ë˜í•‘ ì‹œì‘: {url}")
            
            response = self.session.get(url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ
            posts = []
            tbody = soup.find('tbody')
            if tbody:
                for tr in tbody.find_all('tr'):
                    # ê³µì§€ì‚¬í•­ ìŠ¤í‚µ (bo_notice í´ë˜ìŠ¤)
                    if 'bo_notice' in tr.get('class', []):
                        continue
                    
                    # ì œëª© ë§í¬ ì°¾ê¸°
                    title_td = tr.find('td', class_='td_subject')
                    if title_td:
                        title_link = title_td.find('a', href=True)
                        if title_link and 'wr_id=' in title_link['href']:
                            post_url = urljoin(self.base_url, title_link['href'])
                            post_id = re.search(r'wr_id=(\d+)', title_link['href']).group(1)
                            title = title_link.text.strip()
                            
                            posts.append({
                                'id': post_id,
                                'title': title,
                                'url': post_url
                            })
            
            print(f"í˜ì´ì§€ {page_num}ì—ì„œ {len(posts)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")
            
            # ê° ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘
            for post in posts:
                print(f"\n[{post['id']}] {post['title']}")
                result = self.scrape_post_detail(post['url'], post['id'])
                if result:
                    self.stats['total_posts'] += 1
                
                # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                time.sleep(1)
            
            self.stats['pages_processed'] += 1
            return len(posts)
            
        except Exception as e:
            self.stats['errors'].append(f"í˜ì´ì§€ {page_num} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {str(e)}")
            print(f"í˜ì´ì§€ {page_num} ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {str(e)}")
            return 0
    
    def run(self, max_pages=3):
        """ìŠ¤í¬ë˜í•‘ ì‹¤í–‰"""
        print(f"CNGEC ìŠ¤í¬ë˜í•‘ ì‹œì‘ - ìµœëŒ€ {max_pages}í˜ì´ì§€")
        print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.output_dir}")
        print(f"ì‚¬ì´íŠ¸ ì½”ë“œ: {self.site_code}")
        
        start_time = time.time()
        
        for page in range(1, max_pages + 1):
            posts_count = self.scrape_page(page)
            
            if posts_count == 0:
                print(f"í˜ì´ì§€ {page}ì—ì„œ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            print(f"í˜ì´ì§€ {page} ì™„ë£Œ - {posts_count}ê°œ ê²Œì‹œê¸€ ì²˜ë¦¬")
            
            # í˜ì´ì§€ ê°„ ê°„ê²©
            if page < max_pages:
                time.sleep(2)
        
        end_time = time.time()
        
        # í†µê³„ ì¶œë ¥
        print(f"\n{'='*50}")
        print("ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
        print(f"{'='*50}")
        print(f"ì²˜ë¦¬ëœ í˜ì´ì§€: {self.stats['pages_processed']}")
        print(f"ìˆ˜ì§‘ëœ ê²Œì‹œê¸€: {self.stats['total_posts']}")
        print(f"ë‹¤ìš´ë¡œë“œëœ ì²¨ë¶€íŒŒì¼: {self.stats['attachments_downloaded']}")
        print(f"ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
        
        if self.stats['errors']:
            print(f"\nì˜¤ë¥˜ {len(self.stats['errors'])}ê°œ:")
            for error in self.stats['errors']:
                print(f"  - {error}")
        
        # í†µê³„ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        stats_file = os.path.join(self.output_dir, f"{self.site_code}_stats.json")
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, ensure_ascii=False, indent=2)
        
        return self.stats

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    base_url = "http://www.cngec.or.kr/bbs/board.php?bo_table=notice"
    site_code = "cngec"
    
    scraper = CNGECScraper(base_url, site_code)
    stats = scraper.run(max_pages=3)
    
    # íŒŒì¼ í¬ê¸° ê²€ì¦
    print(f"\n{'='*50}")
    print("íŒŒì¼ í¬ê¸° ê²€ì¦")
    print(f"{'='*50}")
    
    attachment_sizes = {}
    for root, dirs, files in os.walk(scraper.output_dir):
        for file in files:
            if not file.endswith('.md') and not file.endswith('.json'):
                filepath = os.path.join(root, file)
                size = os.path.getsize(filepath)
                
                if size in attachment_sizes:
                    attachment_sizes[size].append(filepath)
                else:
                    attachment_sizes[size] = [filepath]
    
    # ê°™ì€ í¬ê¸°ì˜ íŒŒì¼ë“¤ ì°¾ê¸°
    duplicate_sizes = {size: files for size, files in attachment_sizes.items() if len(files) > 1}
    
    if duplicate_sizes:
        print("âš ï¸  ê°™ì€ í¬ê¸°ì˜ íŒŒì¼ë“¤ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤ (ì˜¤ë¥˜ ê°€ëŠ¥ì„±):")
        for size, files in duplicate_sizes.items():
            print(f"  í¬ê¸° {size} bytes:")
            for file in files:
                print(f"    - {file}")
    else:
        print("âœ… ëª¨ë“  ì²¨ë¶€íŒŒì¼ì´ ì„œë¡œ ë‹¤ë¥¸ í¬ê¸°ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.")
    
    # í•œê¸€ íŒŒì¼ëª… ê²€ì¦
    print(f"\n{'='*50}")
    print("í•œê¸€ íŒŒì¼ëª… ê²€ì¦")
    print(f"{'='*50}")
    
    korean_files = []
    for root, dirs, files in os.walk(scraper.output_dir):
        for file in files:
            if re.search(r'[ê°€-í£]', file):
                korean_files.append(os.path.join(root, file))
    
    if korean_files:
        print(f"âœ… í•œê¸€ íŒŒì¼ëª… {len(korean_files)}ê°œê°€ ì •ìƒì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤:")
        for file in korean_files[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
            print(f"  - {os.path.basename(file)}")
        if len(korean_files) > 5:
            print(f"  ... ì´ {len(korean_files)}ê°œ")
    else:
        print("í•œê¸€ íŒŒì¼ëª…ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # ì™„ë£Œ ì‚¬ìš´ë“œ ì¬ìƒ
    try:
        import subprocess
        subprocess.run(["mpg123", "./ding.mp3"], capture_output=True)
        print("\nğŸ”” ì™„ë£Œ ì‚¬ìš´ë“œ ì¬ìƒ")
    except Exception as e:
        print(f"\nâš ï¸  ì™„ë£Œ ì‚¬ìš´ë“œ ì¬ìƒ ì‹¤íŒ¨: {str(e)}")

if __name__ == "__main__":
    main()