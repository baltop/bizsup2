#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
KEPCO(í•œêµ­ì „ë ¥ê³µì‚¬) ê³µì§€ì‚¬í•­ ìŠ¤í¬ë˜í¼
URL: https://home.kepco.co.kr/kepco/CY/K/ntcob/list.do?boardSeq=21069447&parnScrpSeq=21069447&depth=0&boardNo=0&boardCd=BRD_000039&replyRole=&pageIndex=1&searchKeyword=&searchCondition=&menuCd=FN02070501
"""

import re
import json
import logging
from typing import Dict, List, Any, Optional
from urllib.parse import urljoin, urlparse, parse_qs
from bs4 import BeautifulSoup
from enhanced_base_scraper import EnhancedBaseScraper

logger = logging.getLogger(__name__)

class EnhancedKepcoScraper(EnhancedBaseScraper):
    """KEPCO(í•œêµ­ì „ë ¥ê³µì‚¬) ê³µì§€ì‚¬í•­ ìŠ¤í¬ë˜í¼"""
    
    def __init__(self):
        super().__init__()
        
        # ì‚¬ì´íŠ¸ ê¸°ë³¸ ì„¤ì •
        self.base_url = "https://home.kepco.co.kr"
        self.list_url = "https://home.kepco.co.kr/kepco/CY/K/ntcob/list.do"
        self.start_url = self.list_url
        
        # URL íŒŒë¼ë¯¸í„° (ê³ ì •ê°’)
        self.base_params = {
            'boardSeq': '21069447',
            'parnScrpSeq': '21069447',
            'depth': '0',
            'boardNo': '0',
            'boardCd': 'BRD_000039',
            'replyRole': '',
            'searchKeyword': '',
            'searchCondition': 'total',
            'menuCd': 'FN02070501'
        }
        
        # í—¤ë” ì„¤ì •
        self.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        self.session.headers.update(self.headers)
        
        # í˜„ì¬ ìƒì„¸ í˜ì´ì§€ URL ì €ì¥ (Referer ìš©)
        self.current_detail_url = None
        
    def get_list_url(self, page_num: int) -> str:
        """í˜ì´ì§€ ë²ˆí˜¸ì— ë”°ë¥¸ ëª©ë¡ URL ìƒì„±"""
        params = self.base_params.copy()
        params['pageIndex'] = str(page_num)
        
        # URL íŒŒë¼ë¯¸í„° ë¬¸ìì—´ ìƒì„±
        param_str = '&'.join([f"{k}={v}" for k, v in params.items()])
        return f"{self.list_url}?{param_str}"
    
    def parse_list_page(self, html_content: str) -> List[Dict[str, Any]]:
        """ëª©ë¡ í˜ì´ì§€ íŒŒì‹±"""
        soup = BeautifulSoup(html_content, 'html.parser')
        announcements = []
        
        # ê³µì§€ì‚¬í•­ í…Œì´ë¸” ì°¾ê¸° (table.list)
        table = soup.find('table', class_='list')
        if not table:
            logger.warning("ê³µì§€ì‚¬í•­ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return announcements
        
        # tbody ì°¾ê¸°
        tbody = table.find('tbody')
        if not tbody:
            logger.warning("í…Œì´ë¸” tbodyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return announcements
        
        # ê° í–‰ ì²˜ë¦¬
        rows = tbody.find_all('tr')
        logger.info(f"ì´ {len(rows)}ê°œì˜ ê³µê³  í–‰ ë°œê²¬")
        
        for i, row in enumerate(rows):
            try:
                # ë²ˆí˜¸ ì…€ (th)
                number_cell = row.find('th')
                if not number_cell:
                    continue
                
                # ë‚˜ë¨¸ì§€ ì…€ë“¤ (td)
                cells = row.find_all('td')
                if len(cells) < 4:  # ìµœì†Œ 4ê°œ td í™•ì¸
                    continue
                
                # ì»¬ëŸ¼ êµ¬ì¡°: ë²ˆí˜¸(th), ì œëª©(td.tit), ì²¨ë¶€(td.down), ì‘ì„±ì¼(td), ì¡°íšŒìˆ˜(td)
                title_cell = cells[0]  # td.tit
                attachment_cell = cells[1]  # td.down
                date_cell = cells[2]
                views_cell = cells[3]
                
                # ë²ˆí˜¸ ì²˜ë¦¬
                number = number_cell.get_text(strip=True)
                if not number:
                    continue
                
                # ì œëª© ë° ë§í¬ ì¶”ì¶œ
                title_link = title_cell.find('a')
                if not title_link:
                    continue
                
                title = title_link.get_text(strip=True)
                if not title:
                    continue
                
                # JavaScript ë§í¬ì—ì„œ boardSeq ì¶”ì¶œ
                onclick = title_link.get('onclick', '')
                if not onclick or 'fncGoView' not in onclick:
                    continue
                
                # fncGoView('21069731') í˜•íƒœì—ì„œ boardSeq ì¶”ì¶œ
                match = re.search(r"fncGoView\('(\d+)'\)", onclick)
                if not match:
                    continue
                
                board_seq = match.group(1)
                
                # ìƒì„¸ í˜ì´ì§€ URL êµ¬ì„±
                detail_params = self.base_params.copy()
                detail_params['pageIndex'] = '1'
                detail_params['boardSeq'] = board_seq
                
                param_str = '&'.join([f"{k}={v}" for k, v in detail_params.items()])
                detail_url = f"{self.base_url}/kepco/CY/K/ntcob/ntcobView.do?{param_str}"
                
                # ì²¨ë¶€íŒŒì¼ ì—¬ë¶€ í™•ì¸
                has_attachment = bool(attachment_cell.find('a'))
                
                # ê³µê³  ì •ë³´ êµ¬ì„±
                announcement = {
                    'number': number,
                    'title': title,
                    'url': detail_url,
                    'board_seq': board_seq,
                    'date': date_cell.get_text(strip=True) if date_cell else '',
                    'views': views_cell.get_text(strip=True) if views_cell else '',
                    'has_attachment': has_attachment
                }
                
                announcements.append(announcement)
                logger.debug(f"ê³µê³  íŒŒì‹± ì™„ë£Œ: {title[:50]}...")
                
            except Exception as e:
                logger.error(f"ê³µê³  {i+1} íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        logger.info(f"ì´ {len(announcements)}ê°œ ê³µê³  íŒŒì‹± ì™„ë£Œ")
        return announcements
    
    def parse_detail_page(self, html_content: str, detail_url: str = None) -> Dict[str, Any]:
        """ìƒì„¸ í˜ì´ì§€ íŒŒì‹±"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # í˜„ì¬ ìƒì„¸ í˜ì´ì§€ URL ì €ì¥
        if detail_url:
            self.current_detail_url = detail_url
        
        # ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ (ë³¸ë¬¸ ì¶”ì¶œ ì „ì— ë¨¼ì € ì‹¤í–‰)
        attachments = self._extract_attachments(soup)
        
        # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
        content = self._extract_content(soup)
        
        return {
            'content': content,
            'attachments': attachments
        }
    
    def _extract_content(self, soup: BeautifulSoup) -> str:
        """ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ"""
        content_parts = []
        
        # ë°©ë²• 1: ê²Œì‹œê¸€ ì œëª© ì¶”ì¶œ (dt íƒœê·¸, ë‘ ë²ˆì§¸ dt ìš”ì†Œ)
        dt_elements = soup.find_all('dt')
        if len(dt_elements) >= 2:
            title_elem = dt_elements[1]
            title_text = title_elem.get_text(strip=True)
            if title_text:
                content_parts.append(f"# {title_text}")
        
        # ë°©ë²• 2: ë©”íƒ€ë°ì´í„° í…Œì´ë¸” ì¶”ì¶œ
        meta_table = soup.find('table', attrs={'caption': lambda x: x and 'ê²Œì‹œíŒ' in x})
        if meta_table:
            rows = meta_table.find_all('tr')
            for row in rows:
                cells = row.find_all(['th', 'td'])
                if len(cells) >= 2:
                    key = cells[0].get_text(strip=True)
                    value = cells[1].get_text(strip=True)
                    if key and value:
                        content_parts.append(f"**{key}**: {value}")
        
        # ë°©ë²• 3: ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ (dd.view_cont div.cont)
        content_container = soup.find('dd', class_='view_cont')
        if content_container:
            content_div = content_container.find('div', class_='cont')
            if content_div:
                # ì²¨ë¶€íŒŒì¼ ë§í¬ ì œê±°
                for file_link in content_div.find_all('a', href=re.compile(r'FileDownSecure\.do')):
                    file_link.decompose()
                
                # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                paragraphs = content_div.find_all('p')
                if paragraphs:
                    for p in paragraphs:
                        text = p.get_text(strip=True)
                        if text and len(text) > 10:
                            content_parts.append(text)
                else:
                    # p íƒœê·¸ê°€ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    text = content_div.get_text(strip=True)
                    if text and len(text) > 10:
                        content_parts.append(text)
        
        # ë°©ë²• 4: dd.view_cont ì „ì²´ì—ì„œ ì¶”ì¶œ
        if not content_parts or len(content_parts) <= 2:
            view_cont = soup.find('dd', class_='view_cont')
            if view_cont:
                # ì²¨ë¶€íŒŒì¼ ì„¹ì…˜ ì œê±°
                for file_section in view_cont.find_all('dd', class_='file'):
                    file_section.decompose()
                
                text = view_cont.get_text(strip=True)
                if text and len(text) > 20:
                    content_parts.append(text)
        
        # ë°©ë²• 5: ë‹¨ë½ë³„ ì¶”ì¶œ (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
        if not content_parts or len(content_parts) <= 2:
            paragraphs = soup.find_all('p')
            for p in paragraphs:
                text = p.get_text(strip=True)
                if text and len(text) > 20:
                    content_parts.append(text)
        
        # ìµœì¢… ë³¸ë¬¸ êµ¬ì„±
        if content_parts:
            return "\n\n".join(content_parts)
        else:
            return "ë³¸ë¬¸ ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def _extract_attachments(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ"""
        attachments = []
        
        # ë°©ë²• 1: dd.file ì„¹ì…˜ì—ì„œ ì²¨ë¶€íŒŒì¼ ì°¾ê¸°
        file_section = soup.find('dd', class_='file')
        if file_section:
            file_links = file_section.find_all('a', href=re.compile(r'FileDownSecure\.do'))
            logger.debug(f"dd.file ì„¹ì…˜ì—ì„œ {len(file_links)}ê°œ ë§í¬ ë°œê²¬")
            
            for link in file_links:
                href = link.get('href', '')
                filename = link.get_text(strip=True)
                
                if href and filename:
                    # ì ˆëŒ€ URL ìƒì„±
                    download_url = urljoin(self.base_url, href)
                    
                    # íŒŒì¼ í™•ì¥ì ì¶”ì¶œ
                    file_ext = filename.split('.')[-1].upper() if '.' in filename else 'UNKNOWN'
                    
                    attachments.append({
                        'filename': filename,
                        'url': download_url,
                        'size': '',
                        'type': file_ext
                    })
                    logger.debug(f"ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ ì„±ê³µ: {filename} - {download_url}")
        
        # ë°©ë²• 2: ì „ì²´ í˜ì´ì§€ì—ì„œ FileDownSecure.do ë§í¬ ì°¾ê¸°
        if not attachments:
            file_links = soup.find_all('a', href=re.compile(r'FileDownSecure\.do'))
            logger.debug(f"ì „ì²´ í˜ì´ì§€ì—ì„œ FileDownSecure.do ë§í¬ {len(file_links)}ê°œ ë°œê²¬")
            
            for link in file_links:
                href = link.get('href', '')
                filename = link.get_text(strip=True)
                
                if href and filename:
                    # ì ˆëŒ€ URL ìƒì„±
                    download_url = urljoin(self.base_url, href)
                    
                    # íŒŒì¼ í™•ì¥ì ì¶”ì¶œ
                    file_ext = filename.split('.')[-1].upper() if '.' in filename else 'UNKNOWN'
                    
                    attachments.append({
                        'filename': filename,
                        'url': download_url,
                        'size': '',
                        'type': file_ext
                    })
                    logger.debug(f"ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ ì„±ê³µ: {filename} - {download_url}")
        
        # ë°©ë²• 3: JavaScript onclick íŒ¨í„´ ì°¾ê¸°
        if not attachments:
            onclick_links = soup.find_all('a', onclick=re.compile(r'FileDownSecure\.do'))
            logger.debug(f"JavaScript onclick ë§í¬ {len(onclick_links)}ê°œ ë°œê²¬")
            
            for link in onclick_links:
                onclick = link.get('onclick', '')
                filename = link.get_text(strip=True)
                
                # location.href='/kepco/cmmn/fms/FileDownSecure.do?...' íŒ¨í„´ ì¶”ì¶œ
                match = re.search(r"location\.href='([^']*FileDownSecure\.do[^']*)'", onclick)
                if match:
                    download_url = urljoin(self.base_url, match.group(1))
                    
                    # íŒŒì¼ëª…ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì´ë¦„ ì‚¬ìš©
                    if not filename:
                        filename = f"attachment_{len(attachments) + 1}.file"
                    
                    # íŒŒì¼ í™•ì¥ì ì¶”ì¶œ
                    file_ext = filename.split('.')[-1].upper() if '.' in filename else 'UNKNOWN'
                    
                    attachments.append({
                        'filename': filename,
                        'url': download_url,
                        'size': '',
                        'type': file_ext
                    })
                    logger.debug(f"JavaScript ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ: {filename} - {download_url}")
        
        logger.info(f"ì²¨ë¶€íŒŒì¼ {len(attachments)}ê°œ ì¶”ì¶œ")
        return attachments
    
    # KEPCO ìŠ¤í¬ë˜í¼ëŠ” base scraperì˜ download_fileì„ ì‚¬ìš©í•©ë‹ˆë‹¤
    # í•„ìš”ì‹œ current_detail_urlì´ Refererë¡œ ìë™ ì„¤ì •ë©ë‹ˆë‹¤


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - 3í˜ì´ì§€ ìˆ˜ì§‘"""
    import sys
    import os
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler('kepco_scraper.log', encoding='utf-8')
        ]
    )
    
    logger.info("="*60)
    logger.info("ğŸš€ KEPCO(í•œêµ­ì „ë ¥ê³µì‚¬) ê³µì§€ì‚¬í•­ ìŠ¤í¬ë˜í¼ ì‹œì‘")
    logger.info("="*60)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    output_dir = "output/kepco"
    
    # ê¸°ì¡´ ì¶œë ¥ ë””ë ‰í† ë¦¬ ì •ë¦¬ (íŒŒì¼ë§Œ ì‚­ì œ, ë””ë ‰í† ë¦¬ êµ¬ì¡° ìœ ì§€)
    if os.path.exists(output_dir):
        import shutil
        logger.info(f"ê¸°ì¡´ ì¶œë ¥ ë””ë ‰í† ë¦¬ ì •ë¦¬: {output_dir}")
        shutil.rmtree(output_dir)
    
    # ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”
    scraper = EnhancedKepcoScraper()
    
    try:
        # 3í˜ì´ì§€ ìˆ˜ì§‘ ì‹¤í–‰
        success = scraper.scrape_pages(max_pages=3, output_base="output/kepco")
        
        if success:
            logger.info("âœ… ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
            
            # í†µê³„ ì¶œë ¥
            stats = scraper.get_stats()
            logger.info(f"ğŸ“Š ì²˜ë¦¬ í†µê³„: {stats}")
            
        else:
            logger.error("âŒ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨")
            return 1
            
    except KeyboardInterrupt:
        logger.info("â¹ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        return 1
    except Exception as e:
        logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    import sys
    sys.exit(main())