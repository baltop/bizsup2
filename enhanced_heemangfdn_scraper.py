# -*- coding: utf-8 -*-
"""
í¬ë§ë‚˜ëˆ”ì¬ë‹¨ ìë£Œì‹¤ ìŠ¤í¬ë˜í¼
URL: https://www.heemangfdn.or.kr/layout/res/home.php?go=pds.list&pds_type=1
"""

import os
import time
import logging
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from enhanced_base_scraper import EnhancedBaseScraper
from typing import List, Dict, Any
import re

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('heemangfdn_scraper.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

class EnhancedHeemangfdnScraper(EnhancedBaseScraper):
    """í¬ë§ë‚˜ëˆ”ì¬ë‹¨ ìë£Œì‹¤ ìŠ¤í¬ë˜í¼"""
    
    def __init__(self):
        super().__init__()
        self.base_url = "https://www.heemangfdn.or.kr"
        self.list_url = "https://www.heemangfdn.or.kr/layout/res/home.php?go=pds.list&pds_type=1"
        self.site_code = "heemangfdn"
        
        # í—¤ë” ì„¤ì •
        self.headers.update({
            'Referer': self.base_url,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        self.session.headers.update(self.headers)
        
        # ì‚¬ì´íŠ¸ë³„ ì„¤ì •
        self.verify_ssl = True
        self.timeout = 30
        self.delay_between_requests = 1
        
    def get_list_url(self, page_num: int) -> str:
        """í˜ì´ì§€ ë²ˆí˜¸ì— ë”°ë¥¸ ëª©ë¡ URL ë°˜í™˜"""
        if page_num == 1:
            return self.list_url
        else:
            # í—¤ë§Œì¬ë‹¨ì€ start= íŒŒë¼ë¯¸í„° ì‚¬ìš© (10ê°œì”©)
            start = (page_num - 1) * 10
            return f"{self.base_url}/layout/res/home.php?go=pds.list&pds_type=1&start={start}"
    
    def parse_list_page(self, html_content: str) -> List[Dict[str, Any]]:
        """ëª©ë¡ í˜ì´ì§€ì—ì„œ ê³µê³  ëª©ë¡ ì¶”ì¶œ"""
        soup = BeautifulSoup(html_content, 'html.parser')
        announcements = []
        
        # ê³µê³  ëª©ë¡ì´ ë‹´ê¸´ í…Œì´ë¸” ì°¾ê¸° - heemangfdn ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„ í•„ìš”
        table = soup.find('table')
        if not table:
            logger.warning("ê³µê³  ëª©ë¡ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return announcements
        
        # í…Œì´ë¸”ì˜ í–‰ë“¤ ì°¾ê¸° (í—¤ë” ì œì™¸)
        rows = table.select('tr')
        
        if not rows:
            logger.warning("í…Œì´ë¸”ì— ë°ì´í„° í–‰ì´ ì—†ìŠµë‹ˆë‹¤")
            return announcements
        
        logger.info(f"ì´ {len(rows)}ê°œì˜ í–‰ ë°œê²¬")
        
        # ì²« ë²ˆì§¸ í–‰ì€ í—¤ë”ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë¯€ë¡œ ê±´ë„ˆë›°ê¸°
        data_rows = rows[1:] if len(rows) > 1 else rows
        
        for i, row in enumerate(data_rows):
            try:
                cells = row.find_all(['td', 'th'])
                if len(cells) < 3:
                    logger.debug(f"í–‰ {i}: ì…€ ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ ({len(cells)}ê°œ)")
                    continue
                
                # ì œëª©ê³¼ ë§í¬ ì°¾ê¸° - heemangfdnì€ href ì†ì„± ì‚¬ìš©
                title_link = None
                title = ""
                announcement_id = None
                
                # ë°©ë²• 1: href ì†ì„±ì— num= íŒŒë¼ë¯¸í„°ê°€ ìˆëŠ” a íƒœê·¸ ì°¾ê¸°
                for cell in cells:
                    links = cell.find_all('a', href=True)
                    for link in links:
                        href = link.get('href', '')
                        text = link.get_text(strip=True)
                        
                        # href="home.php?go=pds.list&pds_type=1&num=1195&..." íŒ¨í„´
                        if href and 'num=' in href and text:
                            # num íŒŒë¼ë¯¸í„° ì¶”ì¶œ
                            num_match = re.search(r'num=(\d+)', href)
                            if num_match:
                                title_link = link
                                title = text
                                announcement_id = num_match.group(1)
                                break
                    
                    if title_link:
                        break
                
                if not title_link or not announcement_id:
                    logger.debug(f"í–‰ {i}: ì œëª© ë§í¬ ë˜ëŠ” IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    continue
                
                # heemangfdn ì‚¬ì´íŠ¸ì˜ ìƒì„¸ URL êµ¬ì„±
                # ì‹¤ì œë¡œëŠ” ê°™ì€ í˜ì´ì§€ì—ì„œ í™•ì¥ë˜ì§€ë§Œ, ì§ì ‘ ì ‘ê·¼ ê°€ëŠ¥í•œ URL êµ¬ì„±
                detail_url = f"{self.base_url}/layout/res/home.php?go=pds.list&pds_type=1&num={announcement_id}"
                
                # ê¸°ë³¸ ê³µê³  ì •ë³´
                announcement = {
                    'title': title.strip(),
                    'url': detail_url,
                    'announcement_id': announcement_id
                }
                
                # ì¶”ê°€ ì •ë³´ ì¶”ì¶œ (ë²ˆí˜¸, ë¶„ë¥˜, ì‘ì„±ì¼, ì¡°íšŒìˆ˜)
                try:
                    # ë²ˆí˜¸ (ì²«ë²ˆì§¸ ì…€)
                    if cells[0]:
                        number = cells[0].get_text(strip=True)
                        if number and not number.lower() in ['ë²ˆí˜¸', 'no']:
                            announcement['number'] = number
                    
                    # ì‘ì„±ì¼ (ëì—ì„œ ë‘ë²ˆì§¸ ì…€)
                    if len(cells) >= 4:
                        date_text = cells[-2].get_text(strip=True)
                        if date_text and not date_text.lower() in ['ë“±ë¡ì¼', 'ì‘ì„±ì¼', 'date']:
                            announcement['date'] = date_text
                    
                    # ì¡°íšŒìˆ˜ (ë§ˆì§€ë§‰ ì…€)
                    views_text = cells[-1].get_text(strip=True)
                    if views_text and views_text.isdigit():
                        announcement['views'] = views_text
                        
                except Exception as e:
                    logger.debug(f"í–‰ {i} ì¶”ê°€ ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                
                announcements.append(announcement)
                logger.debug(f"ê³µê³  ì¶”ê°€: {title[:50]}... (ID: {announcement_id})")
                
            except Exception as e:
                logger.error(f"í–‰ {i} íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        logger.info(f"ì´ {len(announcements)}ê°œ ê³µê³  ì¶”ì¶œ ì™„ë£Œ")
        return announcements
    
    def parse_detail_page(self, html_content: str) -> Dict[str, Any]:
        """ìƒì„¸ í˜ì´ì§€ì—ì„œ ë‚´ìš©ê³¼ ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ - heemangfdn ì‚¬ì´íŠ¸ íŠ¹í™”"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
        content_parts = []
        
        # heemangfdn ì‚¬ì´íŠ¸ì˜ ìƒì„¸ë³´ê¸°ëŠ” ê°™ì€ í˜ì´ì§€ì—ì„œ í™•ì¥ë˜ëŠ” í˜•íƒœ
        # ìƒì„¸ ë‚´ìš©ì€ ëª©ë¡ ì•„ë˜ì— í‘œì‹œë¨
        
        # ë°©ë²• 1: ì´ë¯¸ì§€ê°€ í¬í•¨ëœ ë³¸ë¬¸ ì°¾ê¸° (UI ì•„ì´ì½˜ ì œì™¸)
        # heemangfdnì€ ì´ë¯¸ì§€ë¥¼ ë§ì´ ì‚¬ìš©í•¨
        all_images = soup.find_all('img')
        for img in all_images:
            src = img.get('src', '')
            alt = img.get('alt', '')
            
            # UI ì•„ì´ì½˜ì´ë‚˜ ì‹œìŠ¤í…œ ì´ë¯¸ì§€ëŠ” ì œì™¸
            excluded_patterns = [
                '/images/ico/', '/images/icon/', '/ico/', '/icon/', '/images/board/',
                'btn_', 'button_', 'arrow_', 'arr_', 'bg_', 'header_', 'footer_',
                'nav_', 'menu_', 'quick', 'close', 'play', 'pause', 'logo',
                'hd_', 'go_', 'kakao', 'facebook', 'twitter', 'instagram',
                'icon_', 'blank.gif', 'spacer.gif', 'dot.gif'
            ]
            
            # UI íŒ¨í„´ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°
            if any(pattern in src.lower() for pattern in excluded_patterns):
                continue
            
            # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            if src and not src.startswith('http'):
                if src.startswith('/'):
                    img_url = self.base_url + src
                else:
                    img_url = urljoin(self.base_url, src)
                
                # ì´ë¯¸ì§€ íŒŒì¼ëª…ì—ì„œ í™•ì¥ì í™•ì¸
                if any(ext in src.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp']):
                    content_parts.append(f"![{alt}]({img_url})")
        
        # ë°©ë²• 2: í…Œì´ë¸” ì…€ì—ì„œ ê¸´ í…ìŠ¤íŠ¸ ì°¾ê¸°
        all_cells = soup.find_all('td')
        for cell in all_cells:
            cell_text = cell.get_text(strip=True)
            # ìƒë‹¹íˆ ê¸´ í…ìŠ¤íŠ¸ë§Œ ë³¸ë¬¸ìœ¼ë¡œ ê°„ì£¼
            if len(cell_text) > 100:
                # ë„ˆë¬´ ê¸´ ê²½ìš° ì¼ë¶€ë§Œ ì‚¬ìš©
                if len(cell_text) > 2000:
                    cell_text = cell_text[:2000] + "..."
                content_parts.append(cell_text)
        
        # ë°©ë²• 3: divë‚˜ p íƒœê·¸ì—ì„œ ë³¸ë¬¸ ì°¾ê¸°
        content_elements = soup.find_all(['div', 'p'])
        for element in content_elements:
            text = element.get_text(strip=True)
            if len(text) > 50 and len(text) <= 1000:
                content_parts.append(text)
        
        # ë³¸ë¬¸ ì¡°í•©
        content = '\n\n'.join(content_parts) if content_parts else ""
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
        if content:
            # ì¤‘ë³µëœ í…ìŠ¤íŠ¸ ì œê±°
            lines = content.split('\n')
            unique_lines = []
            seen_lines = set()
            for line in lines:
                line = line.strip()
                if line and line not in seen_lines and len(line) > 10:
                    unique_lines.append(line)
                    seen_lines.add(line)
            content = '\n\n'.join(unique_lines)
                
        # ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ - heemangfdn íŠ¹í™”
        attachments = []
        
        # ë°©ë²• 1: ì´ë¯¸ì§€ íŒŒì¼ë“¤ì„ ì²¨ë¶€íŒŒì¼ë¡œ ì²˜ë¦¬ (UI ì•„ì´ì½˜ ì œì™¸)
        for img in all_images:
            src = img.get('src', '')
            if src and any(ext in src.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp']):
                # UI ì•„ì´ì½˜ì´ë‚˜ ì‹œìŠ¤í…œ ì´ë¯¸ì§€ëŠ” ì œì™¸
                excluded_patterns = [
                    '/images/ico/', '/images/icon/', '/ico/', '/icon/', '/images/board/',
                    'btn_', 'button_', 'arrow_', 'arr_', 'bg_', 'header_', 'footer_',
                    'nav_', 'menu_', 'quick', 'close', 'play', 'pause', 'logo',
                    'hd_', 'go_', 'kakao', 'facebook', 'twitter', 'instagram',
                    'icon_', 'blank.gif', 'spacer.gif', 'dot.gif'
                ]
                
                # UI íŒ¨í„´ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                if any(pattern in src.lower() for pattern in excluded_patterns):
                    continue
                
                # íŒŒì¼ëª… ì¶”ì¶œ
                filename = os.path.basename(src)
                if not filename:
                    continue
                
                # ì ˆëŒ€ URL êµ¬ì„±
                if src.startswith('/'):
                    img_url = self.base_url + src
                else:
                    img_url = urljoin(self.base_url, src)
                
                # ì¤‘ë³µ í™•ì¸
                if not any(att['filename'] == filename for att in attachments):
                    attachments.append({
                        'filename': filename,
                        'url': img_url
                    })
                    logger.debug(f"ì´ë¯¸ì§€ ì²¨ë¶€íŒŒì¼ ë°œê²¬: {filename}")
        
        # ë°©ë²• 2: "ì²¨ë¶€íŒŒì¼" ë¼ë²¨ì´ ìˆëŠ” í–‰ì—ì„œ ë‹¤ìš´ë¡œë“œ ë§í¬ ì°¾ê¸°
        attach_cells = soup.find_all(['th', 'td'], string=lambda text: text and 'ì²¨ë¶€íŒŒì¼' in text)
        for cell in attach_cells:
            parent_row = cell.find_parent('tr')
            if parent_row:
                file_links = parent_row.find_all('a', href=True)
                for link in file_links:
                    href = link.get('href', '')
                    filename = link.get_text(strip=True)
                    
                    if filename and href and not href.startswith('#'):
                        download_url = urljoin(self.base_url, href)
                        # ì¤‘ë³µ í™•ì¸
                        if not any(att['filename'] == filename for att in attachments):
                            attachments.append({
                                'filename': filename,
                                'url': download_url
                            })
                            logger.debug(f"ì²¨ë¶€íŒŒì¼ ë°œê²¬: {filename} -> {download_url}")
        
        # ë°©ë²• 3: ëª¨ë“  íŒŒì¼ í™•ì¥ì ë§í¬ ì°¾ê¸°
        file_extensions = ['.pdf', '.hwp', '.doc', '.docx', '.xls', '.xlsx', '.zip', '.jpg', '.jpeg', '.png', '.gif', '.bmp']
        all_links = soup.find_all('a', href=True)
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # hrefë‚˜ í…ìŠ¤íŠ¸ì— íŒŒì¼ í™•ì¥ìê°€ í¬í•¨ëœ ê²½ìš°
            if any(ext in href.lower() for ext in file_extensions) or any(ext in text.lower() for ext in file_extensions):
                filename = text if text else os.path.basename(href)
                if filename and not filename.startswith('#'):
                    download_url = urljoin(self.base_url, href)
                    # ì¤‘ë³µ í™•ì¸
                    if not any(att['filename'] == filename for att in attachments):
                        attachments.append({
                            'filename': filename,
                            'url': download_url
                        })
                        logger.debug(f"íŒŒì¼ í™•ì¥ì ê¸°ë°˜ ì²¨ë¶€íŒŒì¼ ë°œê²¬: {filename}")
        
        logger.info(f"ë³¸ë¬¸ ê¸¸ì´: {len(content)}, ì²¨ë¶€íŒŒì¼: {len(attachments)}ê°œ")
        
        return {
            'content': content,
            'attachments': attachments
        }


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    scraper = EnhancedHeemangfdnScraper()
    
    # output/heemangfdn ë””ë ‰í† ë¦¬ ì„¤ì •
    output_dir = os.path.join('output', scraper.site_code)
    
    logger.info("="*60)
    logger.info("ğŸ›ï¸ í¬ë§ë‚˜ëˆ”ì¬ë‹¨ ìë£Œì‹¤ ìŠ¤í¬ë˜í¼ ì‹œì‘")
    logger.info(f"ğŸ“‚ ì €ì¥ ê²½ë¡œ: {output_dir}")
    logger.info(f"ğŸŒ ëŒ€ìƒ ì‚¬ì´íŠ¸: {scraper.base_url}")
    logger.info("="*60)
    
    try:
        # 3í˜ì´ì§€ê¹Œì§€ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
        success = scraper.scrape_pages(max_pages=3, output_base=output_dir)
        
        if success:
            logger.info("âœ… ìŠ¤í¬ë˜í•‘ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            logger.error("âŒ ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            
    except KeyboardInterrupt:
        logger.info("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        raise


if __name__ == "__main__":
    main()