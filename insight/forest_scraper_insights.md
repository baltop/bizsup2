# 산림청 공고 스크래퍼 개발 인사이트

## 개발 일시
2025-07-17

## 사이트 정보
- **사이트명**: 산림청 공고
- **URL**: https://www.forest.go.kr/kfsweb/cop/bbs/selectBoardList.do?bbsId=BBSMSTR_1032&mn=NKFS_04_01_02
- **사이트 코드**: forest

## 주요 기술적 특징

### 1. 테이블 구조
- **표준 테이블 구조**: 6개 컬럼 (번호, 제목, 담당부서, 작성일, 첨부, 조회)
- **Caption 기반 식별**: "공고 게시판입니다" 텍스트로 테이블 식별
- **명확한 셀 구조**: 각 컬럼이 명확히 분리되어 있음

### 2. HTML 구조 분석
```html
<table>
  <caption>
    <strong>공고 게시판입니다.</strong>
    <p>게시물의 번호, 제목, 담당부서, 작성일, 첨부, 조회수를 확인하실수 있습니다.</p>
  </caption>
  <thead>
    <tr>
      <th>번호</th>
      <th>제목</th>
      <th>담당부서</th>
      <th>작성일</th>
      <th>첨부</th>
      <th>조회</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>6997</td>
      <td><strong>[서부지방산림청]</strong><a href="...">제목...</a></td>
      <td>산림경영과</td>
      <td>2025-07-04</td>
      <td><a href="javascript:fn_egov_select_fileList('...')">첨부파일 내려받기</a></td>
      <td>406</td>
    </tr>
  </tbody>
</table>
```

### 3. 메타데이터 추출 방식
- **번호**: 첫 번째 td 셀에서 직접 추출
- **제목**: 두 번째 td 셀의 a 태그에서 추출
- **담당부서**: 세 번째 td 셀에서 직접 추출
- **작성일**: 네 번째 td 셀에서 YYYY-MM-DD 형식
- **첨부파일**: 다섯 번째 td 셀에 img 태그 또는 "첨부" 텍스트 존재 여부
- **조회수**: 여섯 번째 td 셀에서 직접 추출

### 4. 페이지네이션
- **쿼리 파라미터 기반**: `pageIndex` 파라미터 사용
- **표준 형식**: 1, 2, 3... 형태의 숫자 페이지네이션
- **URL 패턴**: `?bbsId=BBSMSTR_1032&mn=NKFS_04_01_02&pageIndex=2`

### 5. 첨부파일 시스템
- **다운로드 URL 패턴**: `/kfsweb/cmm/fms/FileDown.do?atchFileId=...&fileSn=...`
- **파일 정보**: 링크 텍스트에서 파일명과 크기 정보 추출
- **파일 타입**: pdf, hwp, hwpx, zip, xlsx 등 다양한 형식
- **파일 크기**: 대괄호 안에 크기 정보 포함 (예: `[330.3 KB]`)

### 6. 카테고리 시스템
- **기관 분류**: 제목 앞에 `[기관명]` 형태로 표시
- **주요 기관**: 본청, 서부지방산림청, 북부지방산림청, 동부지방산림청, 남부지방산림청, 중부지방산림청, 국립산림과학원, 국립산림품종관리센터 등

## 개발 과정에서 발견한 문제점과 해결책

### 1. 테이블 선택자 최적화
**문제**: 여러 테이블 중 정확한 공고 테이블 선택 필요
**해결**: Caption 텍스트를 이용한 테이블 식별

```python
# 해결 방법
table = soup.find('table', string=lambda text: text and '공고 게시판입니다' in text)
if not table:
    tables = soup.find_all('table')
    for t in tables:
        caption = t.find('caption')
        if caption and '공고 게시판입니다' in caption.get_text():
            table = t
            break
```

### 2. 제목에서 카테고리 추출
**문제**: 제목과 기관명이 혼재되어 있음
**해결**: 정규표현식을 이용한 카테고리 분리

```python
# 해결 방법
category_match = re.match(r'\[([^\]]+)\]\s*(.+)', title)
if category_match:
    announcement['category'] = category_match.group(1)
    announcement['title'] = category_match.group(2)
```

### 3. 첨부파일 정보 추출
**문제**: 파일명과 크기 정보가 혼재된 텍스트에서 정확한 추출 필요
**해결**: 정규표현식을 이용한 정보 분리

```python
# 해결 방법
size_match = re.search(r'\[([^\]]+)\]', text)
size = size_match.group(1) if size_match else ''

clean_filename = re.sub(r'\s*\[[^\]]+\]', '', filename)
clean_filename = re.sub(r'\s*자료받기.*$', '', clean_filename)
```

### 4. 상세 페이지 내용 파싱 최적화
**문제**: 일관성 없는 HTML 구조로 인한 내용 추출 어려움
**해결**: 다단계 접근 방식 적용

```python
# 해결 방법
# 1. 내용 후보 div 찾기
content_candidates = soup.find_all('div', class_=lambda x: x and 'content' in x.lower())

# 2. 텍스트 길이 기반 선택
for div in divs:
    text = div.get_text(strip=True)
    if len(text) > 100 and '공고' in text:
        content_div = div
        break

# 3. 키워드 기반 선택
for div in soup.find_all('div'):
    text = div.get_text(strip=True)
    if any(keyword in text for keyword in ['공고', '붙임', '첨부', '년', '월', '일']):
        if len(text) > 50:
            content_div = div
            break
```

## 성능 통계
- **처리된 공고**: 30개 (3페이지)
- **다운로드 파일**: 43개 (평균 1.4개/공고)
- **총 다운로드 크기**: 약 149MB
- **한국어 파일명**: 완전 지원
- **실행 시간**: 약 160초

## 파일 타입 분석
- **pdf**: 공고문 및 설명 자료 (가장 일반적)
- **hwpx**: 한글 워드프로세서 파일 (공고문)
- **hwp**: 한글 워드프로세서 파일 (구버전)
- **zip**: 압축 파일 (여러 서식 포함)
- **xlsx**: 엑셀 파일 (상세 내역, 목록)

## 한국어 파일명 지원 확인
✅ 완전 지원됨
- 예시: `2025년_2차_국유품종보호권_처분(통상실시권_허락)_공고(2025-258호).pdf`
- 특수문자: 괄호, 언더스코어, 한글, 쉼표 등 모두 정상 처리
- 매우 긴 파일명도 정상 처리 (82MB 파일 포함)

## 파일 크기 분석
- **소형 파일**: 30KB~100KB (일반 공고문)
- **중형 파일**: 100KB~1MB (상세 자료)
- **대형 파일**: 1MB~55MB (지도, 도면, 상세 매뉴얼)
- **초대형 파일**: 82MB (품종별 특성 상세 자료)

## 향후 개선사항

### 1. 내용 파싱 정확도 향상
- 더 정교한 본문 추출 로직
- 메타데이터와 본문 구분 강화

### 2. 첨부파일 타입별 처리
- 파일 타입별 적절한 처리 로직
- 압축 파일 내부 파일 목록 추출

### 3. 에러 처리 강화
- 대용량 파일 다운로드 최적화
- 네트워크 타임아웃 상황 대응

## 다른 개발자를 위한 팁

### 1. 사이트 분석
- Caption 텍스트를 활용한 테이블 식별이 매우 효과적
- 표준적인 테이블 구조로 파싱이 상대적으로 용이

### 2. 카테고리 처리
- 제목에 포함된 기관명 추출로 분류 가능
- 정규표현식을 이용한 정확한 분리 필요

### 3. 파일 다운로드
- 파일 크기가 매우 다양 (30KB~82MB)
- 대용량 파일에 대한 충분한 타임아웃 설정 필요
- 스트리밍 다운로드 방식 권장

### 4. 성능 최적화
- 페이지 로딩 시간이 상대적으로 길어 적절한 대기 시간 필요
- 대용량 파일 다운로드 시 메모리 효율적 처리 중요

## 코드 재사용성
- EnhancedBaseScraper 클래스 완벽 활용
- 표준 테이블 구조로 다른 정부 사이트에 응용 가능
- 한국어 파일명 처리 시스템 검증 완료
- 다양한 파일 형식 지원 확인

## 특별 사항

### 1. 대용량 파일 처리
- 82MB 파일 정상 다운로드 확인
- 메모리 효율적 스트리밍 다운로드 구현

### 2. 다양한 파일 형식
- 5가지 파일 형식 (.pdf, .hwp, .hwpx, .zip, .xlsx) 지원
- 각 형식별 정확한 크기 정보 추출

### 3. 기관별 분류
- 8개 주요 기관의 공고 분류 시스템
- 카테고리별 필터링 가능한 구조

## 결론
산림청 사이트는 표준적인 테이블 구조와 명확한 메타데이터를 제공하여 스크래핑에 매우 적합합니다. 특히 한국어 파일명 지원이 완벽하고, 대용량 파일 처리도 안정적입니다. 다양한 파일 형식을 지원하며, 기관별 분류 시스템도 잘 구축되어 있어 정부 사이트 스크래핑의 모범 사례로 활용할 수 있습니다.