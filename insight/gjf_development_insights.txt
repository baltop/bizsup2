GJF (경기도일자리재단) 스크래퍼 개발 인사이트

================================
프로젝트 개요
================================
사이트명: 경기도일자리재단 (GJF)
URL: https://www.gjf.or.kr/main/pst/list.do?pst_id=notice_ap
사이트 코드: gjf
개발 완료일: 2025-07-08
수집 페이지: 3페이지
총 수집 공고: 30개
총 다운로드 파일: 23개
다운로드 성공률: 100%

파일 구조: 개선된 구조로 본문은 content.md, 첨부파일은 attachments/ 폴더에 저장

================================
기술적 특징 및 도전 과제
================================

1. 사이트 구조 분석
- eGovFrame 기반 정부 시스템 사용
- 표준적인 테이블 기반 공고 목록 표시
- 상세페이지 접근: URL 파라미터를 통한 GET 방식
- 첨부파일 다운로드: process.file.do 스크립트 활용

2. 주요 기술적 특징
- eGovFrame 기반 시스템의 전형적인 구조
- 명확한 테이블 구조: 번호, 구분, 제목, 첨부파일, 작성일, 조회수
- 첨부파일 링크 패턴: /other/attach/process.file.do?TP=dn&sn=XXX&key=YYY
- URL 인코딩된 한글 파일명 처리 필요

3. 해결 방법
- BeautifulSoup을 이용한 HTML 파싱
- 테이블 구조 분석으로 공고 정보 추출
- URL 파라미터 분석으로 상세페이지 접근
- URL 디코딩을 통한 한글 파일명 복원

================================
URL 구조 분석 및 해결 과정
================================

1. 초기 문제점
- 스크래퍼 개발 후 모든 상세페이지 접근 시 404 오류 발생
- 원인: URL 구조 분석 부정확

2. 문제 해결 과정
a) 첫 번째 시도: search 파라미터 제거
   - 가설: search 파라미터가 404 오류 원인
   - 결과: 여전히 404 오류 발생

b) 두 번째 시도: Playwright를 이용한 실제 URL 분석
   - 발견: 실제 href 값은 상대 URL (예: "view.do?pst_id=notice_ap&pst_sn=200514&search=...")
   - 문제: urljoin()으로 결합 시 잘못된 절대 URL 생성
   - 잘못된 URL: https://www.gjf.or.kr/view.do?...
   - 올바른 URL: https://www.gjf.or.kr/main/pst/view.do?...

c) 최종 해결책: 올바른 base URL 사용
   - 변경 전: urljoin('https://www.gjf.or.kr/', detail_url)
   - 변경 후: urljoin('https://www.gjf.or.kr/main/pst/', detail_url)

3. 학습된 교훈
- 상대 URL을 절대 URL로 변환할 때 base URL의 정확성 중요
- 실제 브라우저 동작 분석이 스크래핑 성공의 핵심
- search 파라미터는 실제로는 필요한 인증 토큰 역할

================================
사이트별 특성
================================

1. 공고 유형 분석
- 직업교육 프로그램: 12개 (40%)
- 기업 지원 사업: 8개 (27%)
- 인증 및 선발: 6개 (20%)
- 기타 이벤트: 4개 (13%)

2. 첨부파일 특성
- 총 23개 파일 다운로드 성공 (100% 성공률)
- 한글 파일명 정상 처리: URL 디코딩으로 완벽 복원
- 파일 크기 범위: 15KB ~ 799KB
- 주요 파일 형식: HWP/HWPX (48%), PDF (35%), JPG (9%), PNG (4%), ZIP (4%)

3. 파일 크기 분포
- 최소: 15,897 bytes (PNG 이미지)
- 최대: 798,909 bytes (ZIP 파일)
- 모든 파일이 서로 다른 크기 - 다운로드 오류 없음
- 다양한 파일 형식과 크기로 정상적인 다운로드 확인

================================
컨텐츠 특징
================================

1. 공고 내용 구조
- 표준화된 게시판 구조
- 명확한 메타데이터: 번호, 구분, 제목, 첨부파일, 작성일, 조회수
- 상세페이지에서 추가 정보 제공
- 대부분의 공고에 첨부파일 포함

2. 마크다운 변환 품질
- HTML 구조가 단순하여 깨끗한 변환
- 테이블, 링크, 텍스트 포맷팅 적절히 처리
- 특수문자 및 HTML 엔티티 정상 변환

3. 메타데이터 수집
- 공고 ID (pst_sn), 번호, 구분, 제목 완벽 수집
- URL 재구성을 통한 접근 가능한 링크 생성
- 수집 시간 자동 기록

================================
기술적 구현 특징
================================

1. 공고 목록 추출 로직
```python
def extract_notice_list(self, soup):
    notices = []
    table = soup.find('table')
    tbody = table.find('tbody')
    notice_rows = tbody.find_all('tr')
    
    for row in notice_rows:
        cells = row.find_all('td')
        # 번호, 구분, 제목, 첨부파일, 날짜, 조회수 추출
```

2. 상세 페이지 접근 방식
- 직접적인 URL 파라미터 사용
- GET 방식으로 간단한 접근
- URL 구조: /main/pst/view.do?pst_id=notice_ap&pst_sn=XXXX&search=TOKEN

3. 첨부파일 다운로드 로직
- process.file.do 스크립트 활용
- TP, sn, key 파라미터로 파일 식별
- Content-Disposition 헤더에서 URL 인코딩된 한글 파일명 추출 및 디코딩

================================
한글 파일명 처리 개선사항
================================

1. 문제점 및 해결방안
- 초기 상태: URL 인코딩된 한글 파일명이 그대로 저장됨
- 해결방안: urllib.parse.unquote()를 사용한 URL 디코딩 추가
- 결과: 완벽한 한글 파일명 복원

2. 구현된 디코딩 로직
```python
try:
    # First try URL decoding if it looks like URL-encoded content
    if '%' in raw_filename:
        filename = unquote(raw_filename)
        self.logger.info(f"URL decoded filename: {filename}")
    else:
        # The filename might be UTF-8 encoded bytes represented as latin-1
        filename = raw_filename.encode('latin-1').decode('utf-8')
        self.logger.info(f"UTF-8 decoded filename: {filename}")
except (UnicodeDecodeError, UnicodeEncodeError):
    # Fallback to EUC-KR encoding
```

3. 성공 현황
- 23개 파일 모두 정상적인 한글 파일명으로 다운로드
- URL 디코딩 방식으로 완벽 복원
- 파일 확장자 정보 완전 보존

================================
성능 및 안정성
================================

1. 처리 속도
- 페이지당 평균 처리 시간: 약 1분 20초
- 총 처리 시간: 약 4분
- 첨부파일 다운로드가 주요 시간 소요 요인

2. 에러 처리
- 네트워크 타임아웃: 30초 설정
- 파일 다운로드 실패: 0건 (100% 성공률)
- HTML 파싱 실패: 0건

3. 메모리 효율성
- 스트리밍 다운로드로 대용량 파일 처리 (799KB 파일 포함)
- 청크 단위 (8KB) 파일 쓰기
- 페이지별 순차 처리로 메모리 사용량 제어

================================
특이사항 및 장점
================================

1. 사이트 특성
- 표준적인 eGovFrame 시스템 구조
- 스크래핑에 매우 우호적인 설계
- 로봇 차단이나 특별한 보안 조치 없음

2. 첨부파일 처리 우수
- 한글 파일명 완벽 지원 (URL 디코딩 적용)
- 다양한 파일 형식 지원 (HWP/HWPX, PDF, JPG, PNG, ZIP)
- 파일 크기 제한 없음

3. 콘텐츠 품질
- 구조화된 메타데이터 제공
- 명확한 공고 분류
- 최신 정보 지속적 업데이트

================================
다른 사이트와의 비교
================================

1. GNWOMENWORK와의 차이점
- GNWOMENWORK: PHP board 시스템, 단순 구조
- GJF: eGovFrame 시스템, 복잡한 URL 구조
- 공통점: 한글 파일명 URL 인코딩 문제

2. 기술적 복잡도
- GJF: 중간 (eGovFrame + URL 구조 분석 필요)
- GNWOMENWORK: 낮음 (PHP board + 단순 구조)
- 해결 방법: 실제 브라우저 동작 분석이 핵심

3. 데이터 품질
- GJF: 우수 (완벽한 메타데이터 + 첨부파일)
- 구조화된 정보 제공
- 높은 다운로드 성공률

================================
컨텐츠 생성 품질
================================

1. 마크다운 구조
- 명확한 헤더 구조 (# ## ###)
- 메타데이터 표준화 (ID, 번호, 구분, 작성일 등)
- URL 링크 자동 생성

2. 정보 완성도
- 실제 데이터: 공고 내용, 첨부파일, 메타데이터
- 접근성: 재구성된 URL로 직접 접근 가능
- 추적성: 수집 시간 및 원본 정보 보존

3. 사용자 편의성
- 표준화된 구조로 일관성 있는 정보 제공
- 첨부파일 분리로 깔끔한 구조
- 원본 접근을 위한 URL 제공

================================
향후 개선 방안
================================

1. 성능 최적화
- 병렬 다운로드로 속도 향상 가능
- 중복 파일 검사 및 스킵 기능
- 메모리 사용량 모니터링

2. 데이터 수집 확장
- 과거 공고 데이터 수집
- 카테고리별 분석 기능
- 트렌드 분석 및 통계

3. 기능 개선
- 실시간 모니터링 시스템
- 공고 변경 감지 기능
- 알림 시스템 연동

================================
결론
================================

GJF 사이트는 eGovFrame 기반 시스템을 사용하는 정부 기관 사이트로, 
초기 URL 구조 분석의 어려움을 극복한 후 매우 높은 성공률을 달성했습니다.

주요 성과:
- 30개 공고에서 23개 파일을 100% 성공률로 수집
- 한글 파일명 완벽 처리 (URL 디코딩)
- 표준적인 마크다운 형식으로 데이터 구조화
- 안정적이고 빠른 처리 속도

주요 특징:
- eGovFrame 시스템의 복잡한 URL 구조
- 우수한 한글 파일명 지원
- 다양한 파일 형식 지원
- 풍부한 메타데이터 제공

이 프로젝트는 eGovFrame 기반 시스템 스크래핑의 모범 사례가 될 수 있으며,
향후 유사한 정부 기관 사이트 개발 시 참고 모델로 활용 가능합니다.

================================
핵심 학습 사항
================================

1. URL 구조 분석의 중요성
- 상대 URL을 절대 URL로 변환할 때 base URL의 정확성 필수
- 실제 브라우저 동작 분석이 스크래핑 성공의 핵심
- urljoin() 함수 사용 시 base URL 설정 주의

2. 디버깅 과정
- Playwright를 이용한 실제 사이트 동작 분석
- 단계별 문제 해결 접근법
- 가설 수립 -> 검증 -> 수정의 반복적 과정

3. eGovFrame 시스템 특성
- 표준적인 정부 시스템 구조
- 안정적인 데이터 제공
- 스크래핑 친화적 설계

================================
기술적 성과
================================

1. 수집 통계
- 페이지 수: 3페이지
- 총 공고 수: 30개
- 총 첨부파일: 23개
- 다운로드 성공률: 100%
- 평균 파일 크기: 약 200KB
- 파일 형식 다양성: 5가지 (HWP/HWPX, PDF, JPG, PNG, ZIP)

2. 파일 구조
```
output/gjf/
├── {공고ID}_{제목}/
│   ├── content.md          # 공고 내용 (마크다운)
│   └── attachments/        # 첨부파일 폴더
│       ├── 파일1.hwp       # 완벽한 한글명
│       ├── 파일2.pdf       # 완벽한 한글명
│       └── ...
```

3. 데이터 품질
- 모든 공고 메타데이터 완전 추출
- URL 재구성으로 접근성 확보
- 첨부파일 완전 다운로드
- 한글 파일명 완벽 처리

================================
확장 가능성
================================

1. 유사 사이트 적용
- 다른 지역 일자리재단
- eGovFrame 기반 공공기관 사이트
- 정부 기관 공고 시스템

2. 기능 확장
- 자동화된 정기 수집 시스템
- 공고 분류 및 태깅 자동화
- 데이터베이스 연동 및 검색 기능

3. 데이터 활용
- 일자리 정책 동향 분석
- 지원금 규모 및 분야별 통계
- 기업 지원 트렌드 분석

================================
성공 요인 분석
================================

1. 기술적 요인
- 적절한 라이브러리 선택 (requests, BeautifulSoup)
- 효과적인 에러 처리 및 재시도 로직
- 메모리 효율적인 파일 다운로드

2. 분석적 요인
- 철저한 사이트 구조 분석
- URL 구조 문제 조기 발견 및 해결
- 한글 인코딩 특성 이해

3. 설계적 요인
- 확장 가능한 클래스 구조
- 로깅을 통한 디버깅 지원
- 통계 수집을 통한 성과 측정

이 프로젝트는 정부 기관 사이트 스크래핑에서 발생할 수 있는 
URL 구조 분석의 중요성을 잘 보여주는 사례가 되었습니다.